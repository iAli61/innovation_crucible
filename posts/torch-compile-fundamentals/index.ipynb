{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4648c02e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"PyTorch compile: Understanding the Compilation Pipeline (Part 1)\"\n",
    "format: \n",
    "  html:\n",
    "    code-fold: true\n",
    "jupyter: python3\n",
    "page-layout: full\n",
    "categories: [pytorch, torch-compile, fundamentals, compilation-pipeline]\n",
    "description: \"Master the fundamentals of PyTorch's torch.compile() system - from basic setup to understanding the 6-stage compilation pipeline and performance characteristics.\"\n",
    "image: \"advanced-torch-compile-triton.jpg\"\n",
    "author: \"Innovation Crucible\"\n",
    "date: \"2025-06-16\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d250857e",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## Understanding the torch.compile() System from the Ground Up\n",
    "\n",
    "Welcome to the first part of a comprehensive guide to mastering PyTorch's revolutionary `torch.compile()` system. This chapter will establish the foundational knowledge you need to understand, utilize, and optimize PyTorch's compilation pipeline effectively.\n",
    "\n",
    "## Chapter Overview\n",
    "\n",
    "In this chapter, we'll embark on a systematic journey through the fundamentals of PyTorch compilation. You'll learn not just *how* to use `torch.compile()`, but *why* it works, *when* to use it, and *how* to debug and optimize it effectively.\n",
    "\n",
    "PyTorch's `torch.compile()` represents one of the most significant advances in deep learning framework optimization since the introduction of automatic differentiation. Understanding its internals isn't just about performance‚Äîit's about becoming a more effective deep learning practitioner who can:\n",
    "\n",
    "- **Make informed decisions** about when and how to optimize models\n",
    "- **Debug performance issues** systematically and efficiently  \n",
    "- **Design models** that naturally benefit from compilation optimizations\n",
    "- **Deploy systems** that leverage compilation effectively in production\n",
    "\n",
    "---\n",
    "\n",
    "In **Section 1.1: Foundation & Environment Setup**, we'll start by establishing the proper development environment and understanding the prerequisites. This isn't just about installation‚Äîwe'll configure debugging capabilities that will serve you throughout the notebook.\n",
    "\n",
    "In **Section 1.2: The Compilation Pipeline Deep Dive**, we'll dissect the 6-stage compilation process, understanding each stage's purpose, inputs, outputs, and trade-offs. This forms the theoretical foundation for everything that follows.\n",
    "\n",
    "In **Section 1.3: Hands-On Performance Analysis**, we'll put theory into practice with comprehensive performance measurements, learning to benchmark compilation overhead against execution speedup and calculate economic trade-offs.\n",
    "\n",
    "In **Section 1.4: Verification and Debugging**, we'll master the essential skills of verifying correctness and debugging compilation issues‚Äîcritical competencies for production deployment.\n",
    "\n",
    "---\n",
    "\n",
    "# Section 1.1: Foundation & Environment Setup\n",
    "\n",
    "### **Knowledge Prerequisites**\n",
    "Before diving into this chapter, ensure you have solid foundations in:\n",
    "\n",
    "- **PyTorch Fundamentals**: Comfortable with tensors, models, autograd, and GPU operations\n",
    "- **GPU Computing Concepts**: Understanding of CUDA, parallel computing, and memory hierarchies\n",
    "- **Python Programming**: Advanced Python skills including decorators, context managers, and profiling\n",
    "- **Performance Analysis**: Basic understanding of benchmarking and statistical measurement\n",
    "\n",
    "### **Hardware Requirements**\n",
    "For the best learning experience, you'll need:\n",
    "\n",
    "- **GPU**: CUDA-capable GPU with Compute Capability 7.0+ (RTX 2080+, V100+, A100)\n",
    "- **Memory**: 8GB+ GPU memory for realistic examples\n",
    "- **CPU**: Multi-core processor for efficient compilation tasks\n",
    "\n",
    "### **Software Environment**\n",
    "We'll guide you through setting up the optimal software stack:\n",
    "\n",
    "```bash\n",
    "# Core PyTorch with CUDA support\n",
    "pip install torch>=2.1.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Triton for GPU kernel generation\n",
    "pip install triton>=2.1.0\n",
    "\n",
    "# Analysis and visualization tools\n",
    "pip install numpy matplotlib seaborn pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5985ca90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using device: cuda\n",
      "   GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "   Memory: 6.4 GB\n",
      "   Compute Capability: (8, 9)\n",
      "üì¶ PyTorch Version: 2.5.1\n",
      "üîß Triton Available: False\n",
      "‚úÖ torch.compile() is available!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Set optimal environment for learning\n",
    "os.environ['TORCH_LOGS'] = '+dynamo'\n",
    "os.environ['TORCHDYNAMO_VERBOSE'] = '1'\n",
    "\n",
    "# Check GPU availability and setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"   Compute Capability: {torch.cuda.get_device_capability()}\")\n",
    "\n",
    "print(f\"üì¶ PyTorch Version: {torch.__version__}\")\n",
    "print(f\"üîß Triton Available: {torch.cuda.is_available() and hasattr(torch.backends, 'triton')}\")\n",
    "\n",
    "# Verify torch.compile is available\n",
    "if hasattr(torch, 'compile'):\n",
    "    print(\"‚úÖ torch.compile() is available!\")\n",
    "else:\n",
    "    print(\"‚ùå torch.compile() not available. Please upgrade PyTorch to 2.0+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d85d35",
   "metadata": {},
   "source": [
    "# Section 1.2: The Compilation Pipeline Architecture\n",
    "\n",
    "## Understanding PyTorch's Revolutionary Compilation System\n",
    "\n",
    "Before diving into practical applications, we need to build a solid mental model of how PyTorch's compilation system works. The `torch.compile()` function isn't just a simple optimizer‚Äîit's a sophisticated compiler infrastructure that transforms your Python code through six distinct stages.\n",
    "## The Six-Stage Compilation Architecture\n",
    "\n",
    "PyTorch's `torch.compile()` uses a six-step process to make your code run faster. Here's a simple breakdown:\n",
    "\n",
    "1.  **Graph Capture**: PyTorch observes your Python code to map out all the operations, creating an initial \"blueprint\" (called an FX Graph).\n",
    "2.  **Graph Optimization**: This blueprint is then refined. PyTorch looks for ways to simplify it, like combining steps or removing unneeded work, to make it more efficient.\n",
    "3.  **Backend Selection**: PyTorch chooses the best specialized tools (backends, e.g., Triton for custom GPU code, or PyTorch's own ATen) for different parts of the refined blueprint.\n",
    "4.  **Kernel Generation**: Using the selected tools, PyTorch generates highly optimized, low-level code (kernels) specifically for your GPU to perform the tasks.\n",
    "5.  **Compilation**: This specialized kernel code is then translated into the actual machine instructions that the GPU can directly understand and execute.\n",
    "6.  **Caching & Execution**: The final compiled machine code is saved (cached). This allows PyTorch to skip the previous steps and run this super-fast code directly on future uses with similar inputs."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7270c628",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "engine: jupyter\n",
    "---\n",
    "\n",
    "```{mermaid}\n",
    "---\n",
    "title: PyTorch Compilation Pipeline\n",
    "config:\n",
    "  theme: base\n",
    "  themeVariables:\n",
    "    primaryColor: \"#ff6b6b\"\n",
    "    primaryTextColor: \"#2c3e50\"\n",
    "    primaryBorderColor: \"#3498db\"\n",
    "    lineColor: \"#34495e\"\n",
    "    secondaryColor: \"#74b9ff\"\n",
    "    tertiaryColor: \"#a29bfe\"\n",
    "---\n",
    "\n",
    "flowchart LR\n",
    "    %% Define styles\n",
    "    classDef startEnd fill:#e74c3c,stroke:#c0392b,stroke-width:3px,color:#fff,font-weight:bold\n",
    "    classDef process fill:#3498db,stroke:#2980b9,stroke-width:2px,color:#fff,font-weight:bold\n",
    "    classDef optimization fill:#f39c12,stroke:#e67e22,stroke-width:2px,color:#fff,font-weight:bold\n",
    "    classDef generation fill:#9b59b6,stroke:#8e44ad,stroke-width:2px,color:#fff,font-weight:bold\n",
    "    classDef storage fill:#2ecc71,stroke:#27ae60,stroke-width:2px,color:#fff,font-weight:bold\n",
    "    \n",
    "    %% Define subgraphs first to ensure horizontal alignment\n",
    "    subgraph Frontend [\"üîß Frontend Processing\"]\n",
    "        direction TB\n",
    "        A((\"üêç<br/>Python<br/>Code\")):::startEnd\n",
    "        B[\"üìä Graph<br/>Capture\"]:::process\n",
    "        C{\"‚ö° Graph<br/>Optimization\"}:::optimization\n",
    "        A ==> B ==> C\n",
    "    end\n",
    "    \n",
    "    subgraph Backend [\"‚ö° Backend Processing\"]\n",
    "        direction TB\n",
    "        D[/\"üéØ Backend<br/>Selection\"/]:::process\n",
    "        E[[\"‚öôÔ∏è Kernel<br/>Generation\"]]:::generation\n",
    "        F[\"üî® Compilation\"]:::generation\n",
    "        D ==> E ==> F\n",
    "    end\n",
    "    \n",
    "    subgraph Runtime [\"üèÉ Runtime\"]\n",
    "        direction TB\n",
    "        G[(\"üíæ Caching\")]:::storage\n",
    "        H((\"üöÄ<br/>Execution\")):::startEnd\n",
    "        G ==> H\n",
    "    end\n",
    "    \n",
    "    %% Connect the subgraphs\n",
    "    Frontend ==> Backend ==> Runtime\n",
    "    \n",
    "    %% Style the subgraphs\n",
    "    style Frontend fill:#ecf0f1,stroke:#bdc3c7,stroke-width:2px,stroke-dasharray: 5 5\n",
    "    style Backend fill:#fdf2e9,stroke:#f39c12,stroke-width:2px,stroke-dasharray: 5 5\n",
    "    style Runtime fill:#e8f8f5,stroke:#2ecc71,stroke-width:2px,stroke-dasharray: 5 5\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cda62c",
   "metadata": {},
   "source": [
    "## Stage 1: Graph Capture (Frontend)\n",
    "#### *\"From Python to Computational Graphs\"*\n",
    "\n",
    "**Primary Function**: Transform dynamic Python execution into a static computational graph\n",
    "\n",
    "**What Actually Happens**:\n",
    "\n",
    "- **TorchDynamo** intercepts Python bytecode execution\n",
    "- **Dynamic tracing** captures the sequence of PyTorch operations\n",
    "- **Control flow resolution** determines which code paths are taken\n",
    "- **Variable binding** freezes the shapes and types of tensors\n",
    "\n",
    "**Key Educational Insights**:\n",
    "\n",
    "- This is where Python's dynamic nature gets \"frozen\" into a static representation\n",
    "- Shape information is captured and becomes part of the optimization\n",
    "- Control flow (if/else statements, loops) gets specialized for the traced path\n",
    "- The resulting graph is framework-agnostic (FX Graph format)\n",
    "\n",
    "**When This Stage Matters Most**:\n",
    "\n",
    "- Models with complex control flow\n",
    "- Dynamic shapes or conditional computations\n",
    "- Custom operations that need special handling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be4e2d5",
   "metadata": {},
   "source": [
    "In the following code, we use a simple model with control flow to showcase graph capture (Control flow (if/else statements, loops) gets specialized for the traced path):\n",
    "\n",
    "```python\n",
    "# Define a simple model with control flow to showcase graph capture\n",
    "class SimpleBranchModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(10, 20)\n",
    "        self.linear2 = nn.Linear(20, 5)\n",
    "        self.linear3 = nn.Linear(20, 5)\n",
    "\n",
    "    def forward(self, x, condition: bool):\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        if condition:\n",
    "            # Path 1: Different computation branch\n",
    "            x = self.linear2(x)\n",
    "            x = torch.sigmoid(x)\n",
    "        else:\n",
    "            # Path 2: Alternative computation branch\n",
    "            x = self.linear3(x)\n",
    "            x = torch.tanh(x)\n",
    "        return x\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756ab8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Graph capture (condition=False):\n",
      "  ‚Ä¢ Ops captured: 4\n",
      "  ‚Ä¢ Number of graphs: 1\n",
      "  ‚Ä¢ Generated graph:\n",
      "GraphModule()\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, L_self_modules_linear1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_linear1_parameters_bias_ : torch.nn.parameter.Parameter, L_x_ : torch.Tensor, L_self_modules_linear3_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_linear3_parameters_bias_ : torch.nn.parameter.Parameter):\n",
      "    l_self_modules_linear1_parameters_weight_ = L_self_modules_linear1_parameters_weight_\n",
      "    l_self_modules_linear1_parameters_bias_ = L_self_modules_linear1_parameters_bias_\n",
      "    l_x_ = L_x_\n",
      "    l_self_modules_linear3_parameters_weight_ = L_self_modules_linear3_parameters_weight_\n",
      "    l_self_modules_linear3_parameters_bias_ = L_self_modules_linear3_parameters_bias_\n",
      "    x = torch._C._nn.linear(l_x_, l_self_modules_linear1_parameters_weight_, l_self_modules_linear1_parameters_bias_);  l_x_ = l_self_modules_linear1_parameters_weight_ = l_self_modules_linear1_parameters_bias_ = None\n",
      "    x_1 = torch.nn.functional.relu(x);  x = None\n",
      "    x_2 = torch._C._nn.linear(x_1, l_self_modules_linear3_parameters_weight_, l_self_modules_linear3_parameters_bias_);  x_1 = l_self_modules_linear3_parameters_weight_ = l_self_modules_linear3_parameters_bias_ = None\n",
      "    x_3 = torch.tanh(x_2);  x_2 = None\n",
      "    return (x_3,)\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\n",
      "  ‚Ä¢ Detailed debug info:\n",
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, L_self_modules_linear1_parameters_weight_: \"f32[20, 10]\", L_self_modules_linear1_parameters_bias_: \"f32[20]\", L_x_: \"f32[32, 10]\", L_self_modules_linear3_parameters_weight_: \"f32[5, 20]\", L_self_modules_linear3_parameters_bias_: \"f32[5]\"):\n",
      "        l_self_modules_linear1_parameters_weight_ = L_self_modules_linear1_parameters_weight_\n",
      "        l_self_modules_linear1_parameters_bias_ = L_self_modules_linear1_parameters_bias_\n",
      "        l_x_ = L_x_\n",
      "        l_self_modules_linear3_parameters_weight_ = L_self_modules_linear3_parameters_weight_\n",
      "        l_self_modules_linear3_parameters_bias_ = L_self_modules_linear3_parameters_bias_\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:10 in forward, code: x = self.linear1(x)\n",
      "        x: \"f32[32, 20]\" = torch._C._nn.linear(l_x_, l_self_modules_linear1_parameters_weight_, l_self_modules_linear1_parameters_bias_);  l_x_ = l_self_modules_linear1_parameters_weight_ = l_self_modules_linear1_parameters_bias_ = None\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:11 in forward, code: x = F.relu(x)\n",
      "        x_1: \"f32[32, 20]\" = torch.nn.functional.relu(x);  x = None\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:18 in forward, code: x = self.linear3(x)\n",
      "        x_2: \"f32[32, 5]\" = torch._C._nn.linear(x_1, l_self_modules_linear3_parameters_weight_, l_self_modules_linear3_parameters_bias_);  x_1 = l_self_modules_linear3_parameters_weight_ = l_self_modules_linear3_parameters_bias_ = None\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:19 in forward, code: x = torch.tanh(x)\n",
      "        x_3: \"f32[32, 5]\" = torch.tanh(x_2);  x_2 = None\n",
      "        return (x_3,)\n",
      "        \n",
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, L_self_modules_linear1_parameters_weight_: \"f32[20, 10]\", L_self_modules_linear1_parameters_bias_: \"f32[20]\", L_x_: \"f32[32, 10]\", L_self_modules_linear3_parameters_weight_: \"f32[5, 20]\", L_self_modules_linear3_parameters_bias_: \"f32[5]\"):\n",
      "        l_self_modules_linear1_parameters_weight_ = L_self_modules_linear1_parameters_weight_\n",
      "        l_self_modules_linear1_parameters_bias_ = L_self_modules_linear1_parameters_bias_\n",
      "        l_x_ = L_x_\n",
      "        l_self_modules_linear3_parameters_weight_ = L_self_modules_linear3_parameters_weight_\n",
      "        l_self_modules_linear3_parameters_bias_ = L_self_modules_linear3_parameters_bias_\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:10 in forward, code: x = self.linear1(x)\n",
      "        x: \"f32[32, 20]\" = torch._C._nn.linear(l_x_, l_self_modules_linear1_parameters_weight_, l_self_modules_linear1_parameters_bias_);  l_x_ = l_self_modules_linear1_parameters_weight_ = l_self_modules_linear1_parameters_bias_ = None\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:11 in forward, code: x = F.relu(x)\n",
      "        x_1: \"f32[32, 20]\" = torch.nn.functional.relu(x);  x = None\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:18 in forward, code: x = self.linear3(x)\n",
      "        x_2: \"f32[32, 5]\" = torch._C._nn.linear(x_1, l_self_modules_linear3_parameters_weight_, l_self_modules_linear3_parameters_bias_);  x_1 = l_self_modules_linear3_parameters_weight_ = l_self_modules_linear3_parameters_bias_ = None\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:19 in forward, code: x = torch.tanh(x)\n",
      "        x_3: \"f32[32, 5]\" = torch.tanh(x_2);  x_2 = None\n",
      "        return (x_3,)\n",
      "        \n",
      "\n",
      "==================================================\n",
      "\n",
      "üîç Graph capture (condition=True):\n",
      "  ‚Ä¢ Ops captured: 4\n",
      "  ‚Ä¢ Number of graphs: 1\n",
      "  ‚Ä¢ Generated graph:\n",
      "GraphModule()\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, L_self_modules_linear1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_linear1_parameters_bias_ : torch.nn.parameter.Parameter, L_x_ : torch.Tensor, L_self_modules_linear2_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_linear2_parameters_bias_ : torch.nn.parameter.Parameter):\n",
      "    l_self_modules_linear1_parameters_weight_ = L_self_modules_linear1_parameters_weight_\n",
      "    l_self_modules_linear1_parameters_bias_ = L_self_modules_linear1_parameters_bias_\n",
      "    l_x_ = L_x_\n",
      "    l_self_modules_linear2_parameters_weight_ = L_self_modules_linear2_parameters_weight_\n",
      "    l_self_modules_linear2_parameters_bias_ = L_self_modules_linear2_parameters_bias_\n",
      "    x = torch._C._nn.linear(l_x_, l_self_modules_linear1_parameters_weight_, l_self_modules_linear1_parameters_bias_);  l_x_ = l_self_modules_linear1_parameters_weight_ = l_self_modules_linear1_parameters_bias_ = None\n",
      "    x_1 = torch.nn.functional.relu(x);  x = None\n",
      "    x_2 = torch._C._nn.linear(x_1, l_self_modules_linear2_parameters_weight_, l_self_modules_linear2_parameters_bias_);  x_1 = l_self_modules_linear2_parameters_weight_ = l_self_modules_linear2_parameters_bias_ = None\n",
      "    x_3 = torch.sigmoid(x_2);  x_2 = None\n",
      "    return (x_3,)\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\n",
      "  ‚Ä¢ Detailed debug info:\n",
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, L_self_modules_linear1_parameters_weight_: \"f32[20, 10]\", L_self_modules_linear1_parameters_bias_: \"f32[20]\", L_x_: \"f32[32, 10]\", L_self_modules_linear2_parameters_weight_: \"f32[5, 20]\", L_self_modules_linear2_parameters_bias_: \"f32[5]\"):\n",
      "        l_self_modules_linear1_parameters_weight_ = L_self_modules_linear1_parameters_weight_\n",
      "        l_self_modules_linear1_parameters_bias_ = L_self_modules_linear1_parameters_bias_\n",
      "        l_x_ = L_x_\n",
      "        l_self_modules_linear2_parameters_weight_ = L_self_modules_linear2_parameters_weight_\n",
      "        l_self_modules_linear2_parameters_bias_ = L_self_modules_linear2_parameters_bias_\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:10 in forward, code: x = self.linear1(x)\n",
      "        x: \"f32[32, 20]\" = torch._C._nn.linear(l_x_, l_self_modules_linear1_parameters_weight_, l_self_modules_linear1_parameters_bias_);  l_x_ = l_self_modules_linear1_parameters_weight_ = l_self_modules_linear1_parameters_bias_ = None\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:11 in forward, code: x = F.relu(x)\n",
      "        x_1: \"f32[32, 20]\" = torch.nn.functional.relu(x);  x = None\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:14 in forward, code: x = self.linear2(x)\n",
      "        x_2: \"f32[32, 5]\" = torch._C._nn.linear(x_1, l_self_modules_linear2_parameters_weight_, l_self_modules_linear2_parameters_bias_);  x_1 = l_self_modules_linear2_parameters_weight_ = l_self_modules_linear2_parameters_bias_ = None\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:15 in forward, code: x = torch.sigmoid(x)\n",
      "        x_3: \"f32[32, 5]\" = torch.sigmoid(x_2);  x_2 = None\n",
      "        return (x_3,)\n",
      "        \n",
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, L_self_modules_linear1_parameters_weight_: \"f32[20, 10]\", L_self_modules_linear1_parameters_bias_: \"f32[20]\", L_x_: \"f32[32, 10]\", L_self_modules_linear2_parameters_weight_: \"f32[5, 20]\", L_self_modules_linear2_parameters_bias_: \"f32[5]\"):\n",
      "        l_self_modules_linear1_parameters_weight_ = L_self_modules_linear1_parameters_weight_\n",
      "        l_self_modules_linear1_parameters_bias_ = L_self_modules_linear1_parameters_bias_\n",
      "        l_x_ = L_x_\n",
      "        l_self_modules_linear2_parameters_weight_ = L_self_modules_linear2_parameters_weight_\n",
      "        l_self_modules_linear2_parameters_bias_ = L_self_modules_linear2_parameters_bias_\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:10 in forward, code: x = self.linear1(x)\n",
      "        x: \"f32[32, 20]\" = torch._C._nn.linear(l_x_, l_self_modules_linear1_parameters_weight_, l_self_modules_linear1_parameters_bias_);  l_x_ = l_self_modules_linear1_parameters_weight_ = l_self_modules_linear1_parameters_bias_ = None\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:11 in forward, code: x = F.relu(x)\n",
      "        x_1: \"f32[32, 20]\" = torch.nn.functional.relu(x);  x = None\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:14 in forward, code: x = self.linear2(x)\n",
      "        x_2: \"f32[32, 5]\" = torch._C._nn.linear(x_1, l_self_modules_linear2_parameters_weight_, l_self_modules_linear2_parameters_bias_);  x_1 = l_self_modules_linear2_parameters_weight_ = l_self_modules_linear2_parameters_bias_ = None\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:15 in forward, code: x = torch.sigmoid(x)\n",
      "        x_3: \"f32[32, 5]\" = torch.sigmoid(x_2);  x_2 = None\n",
      "        return (x_3,)\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# Define a simple model with control flow to showcase graph capture\n",
    "class SimpleBranchModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(10, 20)\n",
    "        self.linear2 = nn.Linear(20, 5)\n",
    "        self.linear3 = nn.Linear(20, 5)\n",
    "\n",
    "    def forward(self, x, condition: bool):\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        if condition:\n",
    "            # Path 1: Different computation branch\n",
    "            x = self.linear2(x)\n",
    "            x = torch.sigmoid(x)\n",
    "        else:\n",
    "            # Path 2: Alternative computation branch\n",
    "            x = self.linear3(x)\n",
    "            x = torch.tanh(x)\n",
    "        return x\n",
    "\n",
    "# Create model instance and test inputs\n",
    "model_graph_capture = SimpleBranchModel().to(device)\n",
    "input_tensor_false = torch.randn(32, 10, device=device)\n",
    "input_tensor_true = torch.randn(32, 10, device=device)\n",
    "\n",
    "print(\"‚úÖ SimpleBranchModel and test inputs created successfully\")\n",
    "print(f\"   Model device: {next(model_graph_capture.parameters()).device}\")\n",
    "print(f\"   Input tensor shape: {input_tensor_false.shape}\")\n",
    "\n",
    "# Stage 1: Graph Capture Demonstration\n",
    "# Show how control flow (if/else) specializes the traced FX graph\n",
    "\n",
    "# Explain graph when condition=False\n",
    "explanation_false = torch._dynamo.explain(model_graph_capture)(input_tensor_false, False)\n",
    "print(\"üîç Graph capture (condition=False):\")\n",
    "print(f\"  ‚Ä¢ Ops captured: {explanation_false.op_count}\")\n",
    "print(f\"  ‚Ä¢ Number of graphs: {len(explanation_false.graphs)}\")\n",
    "print(\"  ‚Ä¢ Generated graph:\")\n",
    "print(explanation_false.graphs[0])\n",
    "print(\"\\n  ‚Ä¢ Detailed debug info:\")\n",
    "print(explanation_false.graphs[0].print_readable())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Explain graph when condition=True\n",
    "explanation_true = torch._dynamo.explain(model_graph_capture)(input_tensor_true, True)\n",
    "print(\"üîç Graph capture (condition=True):\")\n",
    "print(f\"  ‚Ä¢ Ops captured: {explanation_true.op_count}\")\n",
    "print(f\"  ‚Ä¢ Number of graphs: {len(explanation_true.graphs)}\")\n",
    "print(\"  ‚Ä¢ Generated graph:\")\n",
    "print(explanation_true.graphs[0])\n",
    "print(\"\\n  ‚Ä¢ Detailed debug info:\")\n",
    "print(explanation_true.graphs[0].print_readable())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd501ef",
   "metadata": {},
   "source": [
    "let's take a closer look to the results of the `torch._dynamo.explain` function, which provides a detailed breakdown of how TorchDynamo captured and specialized the graph for the `SimpleBranchModel`:\n",
    "\n",
    "\n",
    "The `torch._dynamo.explain` output shows how TorchDynamo traced and specialized two separate FX graphs for the `SimpleBranchModel` based on the boolean `condition`.\n",
    "\n",
    "- **Ops captured**: 4 operations in each graph:\n",
    "    1. `linear1`\n",
    "    2. `relu`\n",
    "    3. branch‚Äêspecific `linear2`+`sigmoid` or `linear3`+`tanh`\n",
    "    4. final activation\n",
    "\n",
    "- **Branch specialization**  \n",
    "    - When `condition=False`, the graph uses `linear3` followed by `tanh`.  \n",
    "    - When `condition=True`, it uses `linear2` followed by `sigmoid`.\n",
    "\n",
    "- **Number of graphs**: 1 per branch (total 2 distinct graphs), each with 4 ops.\n",
    "\n",
    "- **Guards**:  \n",
    "    TorchDynamo inserted runtime guards to ensure the traced graph remains valid, for example:  \n",
    "    - constant‚Äêmatch on the `condition` flag  \n",
    "    - sequence‚Äêlength checks on module parameter dictionaries and hook containers  \n",
    "    - tensor shape/type matches  \n",
    "    - identity checks on global functions (e.g., `F.relu`, `torch.sigmoid`)\n",
    "\n",
    "- **GraphModule signature**:  \n",
    "    Each generated `GraphModule` `forward` takes the module‚Äôs weights, biases and input tensor, runs the fused ops, then returns a single‚Äêelement tuple containing the output tensor.\n",
    "\n",
    "- **Readable debug info**:  \n",
    "    The detailed listing annotates each op with its source‚Äêfile line, argument shapes (`f32[32,20]` etc.), and shows which temporary variables are cleared after use.\n",
    "\n",
    "This demonstrates TorchDynamo‚Äôs ability to  \n",
    "1. **capture** Python control flow as separate FX graphs,  \n",
    "2. **specialize** each graph to a specific branch, and  \n",
    "3. **guard** runtime assumptions to preserve correctness.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d410b85",
   "metadata": {},
   "source": [
    "## Stage 2: Graph Optimization (Frontend)\n",
    "### *\"Transforming Computational Graphs for Efficiency\"*\n",
    "\n",
    "**Primary Function**: Apply high-level optimizations to the computational graph\n",
    "\n",
    "**What Actually Happens**:\n",
    "\n",
    "- **Operation fusion identification**: Finding operations that can be combined\n",
    "- **Dead code elimination**: Removing unused computations\n",
    "- **Constant folding**: Pre-computing values known at compile time\n",
    "- **Memory layout optimization**: Arranging tensors for efficient access patterns\n",
    "\n",
    "**Key Educational Insights**:\n",
    "\n",
    "- This stage works at the operation level, not the kernel level\n",
    "- Fusion opportunities depend on operation compatibility and memory patterns\n",
    "- The optimizer has global view of the computation, enabling sophisticated optimizations\n",
    "- Memory bandwidth often limits performance more than compute capacity\n",
    "\n",
    "**Common Optimizations Applied**:\n",
    "\n",
    "- **Pointwise fusion**: Combining element-wise operations (add, multiply, activation functions)\n",
    "- **Reduction fusion**: Merging operations that reduce tensor dimensions\n",
    "- **Memory planning**: Optimizing tensor allocation and reuse\n",
    "\n",
    "\n",
    "```raw\n",
    "# Before optimization (separate operations):\n",
    "x = linear1(input)\n",
    "x = relu(x) \n",
    "x = linear2(x)\n",
    "x = sigmoid(x)\n",
    "\n",
    "# After optimization (fused operations):\n",
    "x = fused_linear_relu_linear_sigmoid(input)  # Single optimized kernel\n",
    "```\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e69001",
   "metadata": {},
   "source": [
    "## Stage 3: Backend Selection (Transition)\n",
    "### *\"Choosing the Right Tool for Each Job\"*\n",
    "\n",
    "**Primary Function**: Decide which backend will handle each part of the computation\n",
    "\n",
    "**What Actually Happens**:\n",
    "\n",
    "- **Pattern matching**: Identify which operations can be handled by which backends\n",
    "- **Cost modeling**: Estimate performance for different backend choices\n",
    "- **Partitioning**: Split the graph across multiple backends if beneficial\n",
    "- **Interface preparation**: Set up communication between different backend portions\n",
    "\n",
    "**Available Backends**:\n",
    "\n",
    "- **Triton**: Custom GPU kernels for maximum performance\n",
    "- **ATEN**: PyTorch's native C++/CUDA operations\n",
    "- **TensorRT**: NVIDIA's optimized inference engine\n",
    "- **Custom backends**: User-defined optimization passes\n",
    "\n",
    "**Key Educational Insights**:\n",
    "\n",
    "- Not all operations are suitable for all backends\n",
    "- The system can mix backends within a single model\n",
    "- Backend selection affects both performance and feature compatibility\n",
    "\n",
    "---\n",
    "\n",
    "## Stage 4: Kernel Generation (Backend)\n",
    "### *\"Creating Optimized GPU Code\"*\n",
    "\n",
    "**Primary Function**: Generate actual GPU kernel code, typically in Triton\n",
    "\n",
    "**What Actually Happens**:\n",
    "\n",
    "- **Template instantiation**: Use predefined patterns for common operations\n",
    "- **Shape specialization**: Generate code optimized for specific tensor shapes\n",
    "- **Memory access optimization**: Arrange memory reads/writes for maximum bandwidth\n",
    "- **Instruction scheduling**: Order operations for optimal GPU utilization\n",
    "\n",
    "**Triton Kernel Generation Process**:\n",
    "```python\n",
    "# Conceptual example of what gets generated\n",
    "@triton.jit\n",
    "def fused_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n",
    "    # Generated code optimized for your specific operation pattern\n",
    "    pid = tl.program_id(0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "    \n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    result = x * y + 0.5  # Your fused operations\n",
    "    tl.store(output_ptr + offsets, result, mask=mask)\n",
    "```\n",
    "\n",
    "**Key Educational Insights**:\n",
    "- Kernels are specialized for your exact usage patterns\n",
    "- Memory access patterns are optimized for your tensor shapes\n",
    "- Multiple PyTorch operations often become a single GPU kernel\n",
    "\n",
    "---\n",
    "\n",
    "## Stage 5: Compilation (Backend)\n",
    "### *\"From High-Level Code to Machine Instructions\"*\n",
    "\n",
    "**Primary Function**: Compile the generated kernels into executable GPU machine code\n",
    "\n",
    "**What Actually Happens**:\n",
    "- **LLVM compilation**: Transform Triton code to PTX (parallel thread execution)\n",
    "- **PTX to SASS**: NVIDIA driver compiles PTX to actual GPU machine code (SASS)\n",
    "- **Optimization passes**: Hardware-specific optimizations applied\n",
    "- **Binary generation**: Create the final executable GPU kernels\n",
    "\n",
    "**Compilation Toolchain**:\n",
    "```\n",
    "Triton Code ‚Üí LLVM IR ‚Üí PTX Assembly ‚Üí SASS Machine Code ‚Üí GPU Execution\n",
    "```\n",
    "\n",
    "**Key Educational Insights**:\n",
    "- This is where the actual performance magic happens\n",
    "- Different GPU architectures produce different optimized code\n",
    "- Compilation is expensive but results are cached\n",
    "- The final kernels are highly specialized for your exact use case\n",
    "\n",
    "---\n",
    "\n",
    "## Stage 6: Caching & Execution (Runtime)\n",
    "#### *\"Storing and Reusing Optimized Kernels\"*\n",
    "\n",
    "**Primary Function**: Cache compiled kernels and execute them efficiently\n",
    "\n",
    "**What Actually Happens**:\n",
    "\n",
    "- **Persistent caching**: Store compiled kernels on disk for future use\n",
    "- **Cache key generation**: Create unique identifiers based on shapes, dtypes, and operations\n",
    "- **Kernel lookup**: Check cache before recompiling\n",
    "- **Direct execution**: Launch cached kernels without Python overhead\n",
    "\n",
    "**Caching Strategy**:\n",
    "\n",
    "- **Shape-specific**: Separate kernels for different tensor shapes\n",
    "- **Operation-specific**: Different kernels for different operation sequences\n",
    "- **Hardware-specific**: Separate caches for different GPU types\n",
    "\n",
    "**Key Educational Insights**:\n",
    "\n",
    "- First execution pays full compilation cost\n",
    "- Subsequent executions are dramatically faster\n",
    "- Cache invalidation happens when shapes or operations change\n",
    "- Production systems benefit enormously from warm caches\n",
    "\n",
    "## Pipeline Visualization: Data Flow\n",
    "\n",
    "```\n",
    "Python Code ‚Üí [Dynamo] ‚Üí FX Graph ‚Üí [Inductor] ‚Üí Optimized Graph ‚Üí [Backend] ‚Üí \n",
    "Triton Code ‚Üí [LLVM] ‚Üí PTX ‚Üí [Driver] ‚Üí SASS ‚Üí [Cache] ‚Üí GPU Execution\n",
    "```\n",
    "\n",
    "**Key Transformation Points**:\n",
    "\n",
    "1. **Python ‚Üí Graph**: Dynamic to static transformation\n",
    "2. **Graph ‚Üí Optimized Graph**: High-level optimization\n",
    "3. **Graph ‚Üí Kernels**: Backend-specific code generation\n",
    "4. **Kernels ‚Üí Machine Code**: Hardware-specific compilation\n",
    "5. **Machine Code ‚Üí Cache**: Persistent storage for reuse\n",
    "6. **Cache ‚Üí Execution**: Direct GPU kernel launch\n",
    "\n",
    "This pipeline represents one of the most sophisticated optimization systems in modern deep learning, designed to extract maximum performance while maintaining Python's ease of use.\n",
    "\n",
    "**Next, we'll see this pipeline in action with hands-on demonstrations that make these concepts concrete and measurable.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bdcd6a",
   "metadata": {},
   "source": [
    "# Section 1.3: Hands-On Pipeline Demonstration\n",
    "\n",
    "## Development Environment Setup {#dev-environment}\n",
    "\n",
    "Let's set up the optimal development environment with debugging capabilities enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac5e59ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß ADVANCED ENVIRONMENT CONFIGURATION\n",
      "=============================================\n",
      "‚úÖ Environment variables configured for deep introspection\n",
      "   ‚Ä¢ TORCH_LOGS: Dynamo tracing enabled\n",
      "   ‚Ä¢ TORCHDYNAMO_VERBOSE: Detailed compilation logging\n",
      "   ‚Ä¢ TORCH_COMPILE_DEBUG: Expert-level debugging\n",
      "\n",
      "üìö Available Debugging Levels:\n",
      "   üìä Basic: Basic compilation tracing\n",
      "      TORCH_LOGS=+dynamo\n",
      "   ‚ö° Performance: Autotuning and cache analysis\n",
      "      TRITON_PRINT_AUTOTUNING=1\n",
      "      TRITON_PRINT_CACHE_STATS=1\n",
      "   üî¨ Expert: Full kernel source visibility\n",
      "      TORCH_LOGS=output_code\n",
      "      TORCH_COMPILE_DEBUG=1\n",
      "\n",
      "üí° Current configuration: Expert level debugging enabled\n"
     ]
    }
   ],
   "source": [
    "# üîß Essential Environment Variables Configuration\n",
    "\n",
    "# Store original settings for restoration\n",
    "original_env = {}\n",
    "env_vars = ['TORCH_LOGS', 'TORCHDYNAMO_VERBOSE', 'TORCH_COMPILE_DEBUG']\n",
    "\n",
    "for var in env_vars:\n",
    "    original_env[var] = os.environ.get(var)\n",
    "\n",
    "# Set up comprehensive debugging environment\n",
    "os.environ['TORCH_LOGS'] = '+dynamo'\n",
    "os.environ['TORCHDYNAMO_VERBOSE'] = '1'  \n",
    "os.environ['TORCH_COMPILE_DEBUG'] = '1'\n",
    "\n",
    "print(\"üîß ADVANCED ENVIRONMENT CONFIGURATION\")\n",
    "print(\"=\" * 45)\n",
    "print(\"‚úÖ Environment variables configured for deep introspection\")\n",
    "print(\"   ‚Ä¢ TORCH_LOGS: Dynamo tracing enabled\")\n",
    "print(\"   ‚Ä¢ TORCHDYNAMO_VERBOSE: Detailed compilation logging\")\n",
    "print(\"   ‚Ä¢ TORCH_COMPILE_DEBUG: Expert-level debugging\")\n",
    "\n",
    "# Key Environment Variables Reference:\n",
    "debugging_levels = {\n",
    "    \"üìä Basic\": {\n",
    "        \"TORCH_LOGS\": \"+dynamo\",\n",
    "        \"purpose\": \"Basic compilation tracing\"\n",
    "    },\n",
    "    \"‚ö° Performance\": {\n",
    "        \"TRITON_PRINT_AUTOTUNING\": \"1\",\n",
    "        \"TRITON_PRINT_CACHE_STATS\": \"1\", \n",
    "        \"purpose\": \"Autotuning and cache analysis\"\n",
    "    },\n",
    "    \"üî¨ Expert\": {\n",
    "        \"TORCH_LOGS\": \"output_code\",\n",
    "        \"TORCH_COMPILE_DEBUG\": \"1\",\n",
    "        \"purpose\": \"Full kernel source visibility\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nüìö Available Debugging Levels:\")\n",
    "for level, config in debugging_levels.items():\n",
    "    print(f\"   {level}: {config['purpose']}\")\n",
    "    for var, value in config.items():\n",
    "        if var != 'purpose':\n",
    "            print(f\"      {var}={value}\")\n",
    "\n",
    "print(f\"\\nüí° Current configuration: Expert level debugging enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc436a36",
   "metadata": {},
   "source": [
    "## A Scientific Approach to Understanding Compilation Performance\n",
    "\n",
    "Now that we understand the theoretical framework, let's apply the scientific method to analyze PyTorch compilation in practice. This demonstration will teach you not just *what* happens during compilation, but *how* to measure and analyze it systematically.\n",
    "\n",
    "---\n",
    "\n",
    "## Experimental Design Philosophy\n",
    "\n",
    "### **Why This Demonstration Matters**\n",
    "\n",
    "Most tutorials show you how to call `torch.compile()`, but they don't teach you how to *evaluate* whether it's working effectively. This demonstration establishes a **rigorous methodology** for performance analysis that you can apply to any model or use case.\n",
    "\n",
    "---\n",
    "\n",
    "## Experimental Methodology\n",
    "\n",
    "### **Phase 1: Baseline Establishment**\n",
    "**Objective**: Measure eager mode performance to establish our reference point\n",
    "\n",
    "**Why This Matters**: Without a proper baseline, performance comparisons are meaningless. We need to understand the *unoptimized* performance characteristics before we can evaluate the benefits of compilation.\n",
    "\n",
    "**Measurement Protocol**:\n",
    "\n",
    "- **Warmup runs**: Eliminate GPU initialization overhead and driver compilation\n",
    "- **Statistical sampling**: Multiple measurements to account for system noise\n",
    "- **Proper synchronization**: Ensure GPU operations complete before timing\n",
    "- **Memory state management**: Start with clean GPU memory state\n",
    "\n",
    "### **Phase 2: Compilation Analysis**  \n",
    "**Objective**: Measure the true cost of compilation\n",
    "\n",
    "**Why This Matters**: Compilation isn't free. Understanding the overhead helps you make informed decisions about when and how to apply compilation in your workflows.\n",
    "\n",
    "**What We'll Measure**:\n",
    "\n",
    "- **Total compilation time**: From `torch.compile()` call to first execution completion\n",
    "- **Kernel generation overhead**: Time spent creating optimized GPU kernels  \n",
    "- **Memory overhead**: Additional GPU memory used by compilation infrastructure\n",
    "- **Cache generation**: Time spent creating persistent kernel cache\n",
    "\n",
    "### **Phase 3: Performance Evaluation**\n",
    "**Objective**: Quantify the benefits of compiled execution\n",
    "\n",
    "**Why This Matters**: The ultimate question is whether compilation provides net benefits. This requires understanding both the magnitude of speedup and the conditions under which it applies.\n",
    "\n",
    "**Performance Metrics**:\n",
    "\n",
    "- **Execution speedup**: How much faster compiled kernels run\n",
    "- **Memory efficiency**: Changes in memory usage patterns\n",
    "- **Consistency**: Variation in execution times (important for production)\n",
    "- **Scalability**: How benefits change with different input sizes\n",
    "\n",
    "### **Phase 4: Economic Analysis**\n",
    "**Objective**: Calculate the break-even point and return on investment\n",
    "\n",
    "**Why This Matters**: Engineering decisions should be based on total value, not just peak performance. Understanding the economics helps you optimize your development and deployment strategies.\n",
    "\n",
    "**Economic Metrics**:\n",
    "\n",
    "- **Break-even analysis**: How many executions to recover compilation cost\n",
    "- **ROI calculation**: Return on investment over time\n",
    "- **Opportunity cost**: What else could you do with the compilation time\n",
    "- **Risk assessment**: Probability of achieving expected benefits\n",
    "\n",
    "---\n",
    "\n",
    "## Understanding the Demonstration Code\n",
    "\n",
    "### **Model Selection Strategy**\n",
    "\n",
    "We'll use a model specifically designed to showcase compilation benefits:\n",
    "\n",
    "```python\n",
    "class FusionDemoModel(nn.Module):\n",
    "    \"\"\"Model designed to demonstrate kernel fusion benefits\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(512)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Operations that benefit from fusion\n",
    "        normalized = self.layer_norm(x)     # Normalization\n",
    "        activated = F.gelu(normalized)      # Activation function  \n",
    "        scaled = activated * 1.2 + 0.1     # Arithmetic operations\n",
    "        return scaled\n",
    "```\n",
    "\n",
    "**Why This Model Works Well**:\n",
    "\n",
    "- **Sequential operations**: Create opportunities for kernel fusion\n",
    "- **Memory bandwidth bound**: Fusion reduces memory traffic\n",
    "- **Mixed operation types**: Showcases different optimization strategies\n",
    "- **Realistic complexity**: Represents common deep learning patterns\n",
    "\n",
    "### **Critical PyTorch APIs for Performance Analysis**\n",
    "\n",
    "#### **1. `torch._dynamo.reset()`** \n",
    "```python\n",
    "torch._dynamo.reset()  # Clear compilation cache\n",
    "```\n",
    "\n",
    "**Purpose**: Ensures clean state for reproducible measurements\n",
    "- **When to use**: Before each experimental run\n",
    "- **What it does**: Clears TorchDynamo's internal cache and compilation artifacts\n",
    "- **‚ö†Ô∏è Important**: This is an internal API‚Äîuse only for debugging and education\n",
    "\n",
    "#### **2. `torch.compile()` with Mode Selection** \n",
    "```python\n",
    "compiled_model = torch.compile(model, mode=\"default\")\n",
    "```\n",
    "\n",
    "**Compilation Modes Explained**:\n",
    "\n",
    "- **`\"default\"`**: Balanced optimization (recommended starting point)\n",
    "- **`\"reduce-overhead\"`**: Minimize compilation time (faster compilation, moderate speedup)\n",
    "- **`\"max-autotune\"`**: Maximum performance (longer compilation, maximum speedup)\n",
    "- **`\"max-autotune-no-cudagraphs\"`**: Max optimization without CUDA graphs\n",
    "\n",
    "**Educational Insight**: Mode selection represents a trade-off between compilation time and execution performance.\n",
    "\n",
    "#### **3. `torch.cuda.synchronize()`** \n",
    "```python\n",
    "torch.cuda.synchronize()  # Wait for GPU operations to complete\n",
    "```\n",
    "\n",
    "**Critical for Accurate Timing**:\n",
    "\n",
    "- **Why needed**: GPU operations are asynchronous‚Äîtiming without sync is meaningless\n",
    "- **When to use**: Before and after each timed operation\n",
    "- **Best practice**: Always synchronize when measuring GPU performance\n",
    "\n",
    "### **Statistical Analysis Framework**\n",
    "\n",
    "#### **Timing Best Practices**\n",
    "```python\n",
    "# Proper timing protocol\n",
    "times = []\n",
    "for _ in range(n_measurements):\n",
    "    torch.cuda.synchronize()  # Ensure clean start\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    # Your operation here\n",
    "    output = model(input_tensor)\n",
    "    \n",
    "    torch.cuda.synchronize()  # Ensure completion\n",
    "    times.append(time.perf_counter() - start)\n",
    "\n",
    "average_time = sum(times) / len(times)\n",
    "std_deviation = statistics.stdev(times)\n",
    "```\n",
    "\n",
    "**Why Multiple Measurements Matter**:\n",
    "\n",
    "- **System noise**: Other processes affect timing\n",
    "- **GPU scheduling**: Different kernel launch overhead\n",
    "- **Thermal effects**: GPU performance varies with temperature\n",
    "- **Statistical confidence**: Better estimates with more samples\n",
    "\n",
    "#### **Break-Even Analysis Mathematics**\n",
    "```python\n",
    "# Economic analysis framework\n",
    "compilation_overhead = first_run_time - baseline_time\n",
    "speedup_per_run = baseline_time - cached_time\n",
    "break_even_runs = compilation_overhead / speedup_per_run\n",
    "\n",
    "# ROI calculation over time\n",
    "def calculate_roi(runs_executed):\n",
    "    time_saved = runs_executed * speedup_per_run\n",
    "    net_benefit = time_saved - compilation_overhead\n",
    "    roi_percentage = (net_benefit / compilation_overhead) * 100\n",
    "    return roi_percentage\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn from Running the Demonstration\n",
    "\n",
    "### **Performance Characteristics You'll Observe**\n",
    "\n",
    "1. **Compilation Overhead Pattern**\n",
    "\n",
    "   - First execution: 10-100x slower than baseline\n",
    "   - Overhead dominated by kernel generation and compilation\n",
    "   - Time varies significantly with model complexity\n",
    "\n",
    "2. **Speedup Patterns**\n",
    "\n",
    "   - Cached execution: 1.5-5x faster than baseline (typical range)\n",
    "   - Speedup depends on fusion opportunities and memory patterns\n",
    "   - Consistency improves with compilation (less variance)\n",
    "\n",
    "3. **Economic Trade-offs**\n",
    "\n",
    "   - Break-even: Usually 5-50 executions for neural networks\n",
    "   - ROI improves over time (compounding benefits)\n",
    "   - Different models have different economic profiles\n",
    "\n",
    "\n",
    "\n",
    "**Ready to see the compilation pipeline in action? Let's run our comprehensive analysis! üöÄ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f861fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ COMPREHENSIVE COMPILATION PIPELINE DEMONSTRATION\n",
      "============================================================\n",
      "üî¨ Experimental Setup:\n",
      "   Model: LayerNorm ‚Üí GELU ‚Üí Arithmetic fusion\n",
      "   Input shape: torch.Size([64, 128, 512])\n",
      "   Device: cuda\n",
      "   Expected optimizations: Kernel fusion, memory optimization\n",
      "   Initial GPU memory: 41.2 MB allocated\n",
      "\n",
      "‚öôÔ∏è  Stages 1-3: Graph Capture ‚Üí Optimization ‚Üí Backend Selection\n",
      "-------------------------------------------------------\n",
      "üìè Measuring baseline (eager mode) performance...\n",
      "   ‚úÖ Baseline performance: 20.253 ms\n",
      "   üìä Baseline peak memory: 119.6 MB\n",
      "\n",
      "üî• Stages 4-6: Kernel Generation ‚Üí Compilation ‚Üí Caching\n",
      "-------------------------------------------------------\n",
      "   Watch for Triton kernel generation output below:\n",
      "   ‚úÖ Baseline performance: 20.253 ms\n",
      "   üìä Baseline peak memory: 119.6 MB\n",
      "\n",
      "üî• Stages 4-6: Kernel Generation ‚Üí Compilation ‚Üí Caching\n",
      "-------------------------------------------------------\n",
      "   Watch for Triton kernel generation output below:\n",
      "\n",
      "üìä Compilation Analysis:\n",
      "   ‚úÖ Total compilation time: 331.0 ms\n",
      "   ‚úÖ First execution time: 323.2 ms\n",
      "   üìà Compilation overhead: 16.0x baseline\n",
      "   üóÑÔ∏è  Compilation memory overhead: 16.0 MB\n",
      "   üìä Compilation peak memory: 73.2 MB\n",
      "\n",
      "‚ö° Cached Performance Analysis\n",
      "------------------------------\n",
      "   ‚úÖ Cached performance: 1.226 ms\n",
      "   üöÄ Speedup achieved: 16.52x\n",
      "   üìä Cached peak memory: 89.2 MB\n",
      "   üß† Memory efficiency ratio: 1.34x\n",
      "      ‚úÖ Compiled version uses 25.4% less peak memory\n",
      "\n",
      "üí∞ Economic Analysis:\n",
      "   Time saved per run: 19.027 ms\n",
      "   Break-even point: 17.4 runs\n",
      "   ‚ö° Good ROI - compile for repeated use\n",
      "\n",
      "üß† Memory Overhead Analysis:\n",
      "   Compilation overhead: 16.0 MB\n",
      "   Baseline peak usage: 119.6 MB\n",
      "   Compiled peak usage: 89.2 MB\n",
      "   Memory overhead percentage: 13.4%\n",
      "   ‚ö° Moderate memory overhead - acceptable for most cases\n",
      "\n",
      "üîç Correctness check: Max difference = 1.19e-06\n",
      "   ‚úÖ Excellent numerical accuracy maintained\n",
      "\n",
      "üéì Pipeline Summary:\n",
      "   üì∏ Stage 1-3: Graph capture and optimization (automatic)\n",
      "   üîß Stage 4-6: Kernel generation and caching (331.0 ms)\n",
      "   ‚ö° Result: 16.52x speedup after 17.4 runs\n",
      "   üß† Memory: 1.34x efficiency, 13.4% overhead\n",
      "\n",
      "üéØ Key Takeaways:\n",
      "   ‚Ä¢ torch.compile() is a sophisticated 6-stage pipeline\n",
      "   ‚Ä¢ Compilation overhead is significant but amortizes quickly\n",
      "   ‚Ä¢ Generated kernels are cached for future use\n",
      "   ‚Ä¢ Performance gains depend on model complexity and hardware\n",
      "   ‚Ä¢ Memory efficiency varies - monitor both speed and memory usage\n",
      "   ‚Ä¢ Consider memory overhead in resource-constrained environments\n",
      "\n",
      "üìä Compilation Analysis:\n",
      "   ‚úÖ Total compilation time: 331.0 ms\n",
      "   ‚úÖ First execution time: 323.2 ms\n",
      "   üìà Compilation overhead: 16.0x baseline\n",
      "   üóÑÔ∏è  Compilation memory overhead: 16.0 MB\n",
      "   üìä Compilation peak memory: 73.2 MB\n",
      "\n",
      "‚ö° Cached Performance Analysis\n",
      "------------------------------\n",
      "   ‚úÖ Cached performance: 1.226 ms\n",
      "   üöÄ Speedup achieved: 16.52x\n",
      "   üìä Cached peak memory: 89.2 MB\n",
      "   üß† Memory efficiency ratio: 1.34x\n",
      "      ‚úÖ Compiled version uses 25.4% less peak memory\n",
      "\n",
      "üí∞ Economic Analysis:\n",
      "   Time saved per run: 19.027 ms\n",
      "   Break-even point: 17.4 runs\n",
      "   ‚ö° Good ROI - compile for repeated use\n",
      "\n",
      "üß† Memory Overhead Analysis:\n",
      "   Compilation overhead: 16.0 MB\n",
      "   Baseline peak usage: 119.6 MB\n",
      "   Compiled peak usage: 89.2 MB\n",
      "   Memory overhead percentage: 13.4%\n",
      "   ‚ö° Moderate memory overhead - acceptable for most cases\n",
      "\n",
      "üîç Correctness check: Max difference = 1.19e-06\n",
      "   ‚úÖ Excellent numerical accuracy maintained\n",
      "\n",
      "üéì Pipeline Summary:\n",
      "   üì∏ Stage 1-3: Graph capture and optimization (automatic)\n",
      "   üîß Stage 4-6: Kernel generation and caching (331.0 ms)\n",
      "   ‚ö° Result: 16.52x speedup after 17.4 runs\n",
      "   üß† Memory: 1.34x efficiency, 13.4% overhead\n",
      "\n",
      "üéØ Key Takeaways:\n",
      "   ‚Ä¢ torch.compile() is a sophisticated 6-stage pipeline\n",
      "   ‚Ä¢ Compilation overhead is significant but amortizes quickly\n",
      "   ‚Ä¢ Generated kernels are cached for future use\n",
      "   ‚Ä¢ Performance gains depend on model complexity and hardware\n",
      "   ‚Ä¢ Memory efficiency varies - monitor both speed and memory usage\n",
      "   ‚Ä¢ Consider memory overhead in resource-constrained environments\n"
     ]
    }
   ],
   "source": [
    "# üß™ Comprehensive Compilation Pipeline Demonstration with Memory Analysis\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current GPU memory usage in MB\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return {\n",
    "            'allocated': torch.cuda.memory_allocated() / 1024**2,\n",
    "            'reserved': torch.cuda.memory_reserved() / 1024**2,\n",
    "            'cached': torch.cuda.memory_reserved() / 1024**2  # Using memory_reserved instead of deprecated memory_cached\n",
    "        }\n",
    "    return {'allocated': 0, 'reserved': 0, 'cached': 0}\n",
    "\n",
    "def demonstrate_compilation_phases():\n",
    "    \"\"\"\n",
    "    Educational demonstration of the complete torch.compile() pipeline\n",
    "    Shows all 6 stages with detailed performance and memory analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üß™ COMPREHENSIVE COMPILATION PIPELINE DEMONSTRATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Define a model that will showcase optimization\n",
    "    class FusionDemoModel(nn.Module):\n",
    "        \"\"\"Model designed to demonstrate kernel fusion benefits\"\"\"\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.layer_norm = nn.LayerNorm(512)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            # Operations that benefit from fusion\n",
    "            normalized = self.layer_norm(x)     # Normalization\n",
    "            activated = F.gelu(normalized)      # Activation function\n",
    "            scaled = activated * 1.2 + 0.1     # Arithmetic operations\n",
    "            return scaled\n",
    "    \n",
    "    # Experimental setup\n",
    "    model = FusionDemoModel().to(device)\n",
    "    test_input = torch.randn(64, 128, 512, device=device)\n",
    "    \n",
    "    print(f\"üî¨ Experimental Setup:\")\n",
    "    print(f\"   Model: LayerNorm ‚Üí GELU ‚Üí Arithmetic fusion\")\n",
    "    print(f\"   Input shape: {test_input.shape}\")\n",
    "    print(f\"   Device: {device}\")\n",
    "    print(f\"   Expected optimizations: Kernel fusion, memory optimization\")\n",
    "    \n",
    "    # Initial memory snapshot\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    initial_memory = get_memory_usage()\n",
    "    print(f\"   Initial GPU memory: {initial_memory['allocated']:.1f} MB allocated\")\n",
    "    \n",
    "    # Stage 1-3: Graph Capture and Optimization (happens during first compile call)\n",
    "    print(f\"\\n‚öôÔ∏è  Stages 1-3: Graph Capture ‚Üí Optimization ‚Üí Backend Selection\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    # Clear any previous compilations for clean demonstration\n",
    "    torch._dynamo.reset()\n",
    "    \n",
    "    # Baseline performance measurement\n",
    "    print(\"üìè Measuring baseline (eager mode) performance...\")\n",
    "    model.eval()\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(3):\n",
    "            _ = model(test_input)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Measure baseline performance and memory\n",
    "    baseline_memory_before = get_memory_usage()\n",
    "    baseline_times = []\n",
    "    baseline_peak_memory = []\n",
    "    \n",
    "    for _ in range(10):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            baseline_output = model(test_input)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "            baseline_peak_memory.append(torch.cuda.max_memory_allocated() / 1024**2)\n",
    "        \n",
    "        baseline_times.append(time.perf_counter() - start)\n",
    "    \n",
    "    baseline_avg = sum(baseline_times) / len(baseline_times)\n",
    "    baseline_memory_avg = sum(baseline_peak_memory) / len(baseline_peak_memory) if baseline_peak_memory else 0\n",
    "    \n",
    "    print(f\"   ‚úÖ Baseline performance: {baseline_avg*1000:.3f} ms\")\n",
    "    print(f\"   üìä Baseline peak memory: {baseline_memory_avg:.1f} MB\")\n",
    "    \n",
    "    # Stages 4-6: Kernel Generation, Compilation, and Caching\n",
    "    print(f\"\\nüî• Stages 4-6: Kernel Generation ‚Üí Compilation ‚Üí Caching\")\n",
    "    print(\"-\" * 55)\n",
    "    print(\"   Watch for Triton kernel generation output below:\")\n",
    "    \n",
    "    # Memory before compilation\n",
    "    memory_before_compile = get_memory_usage()\n",
    "    \n",
    "    # This is where the magic happens - all remaining stages occur here\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    compilation_start = time.perf_counter()\n",
    "    compiled_model = torch.compile(model, mode=\"default\")\n",
    "    \n",
    "    # First execution triggers kernel generation and compilation\n",
    "    start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        compiled_output = compiled_model(test_input)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        compilation_peak_memory = torch.cuda.max_memory_allocated() / 1024**2\n",
    "    else:\n",
    "        compilation_peak_memory = 0\n",
    "    \n",
    "    first_run_time = time.perf_counter() - start\n",
    "    total_compilation_time = time.perf_counter() - compilation_start\n",
    "    \n",
    "    # Memory after compilation\n",
    "    memory_after_compile = get_memory_usage()\n",
    "    compilation_memory_overhead = memory_after_compile['allocated'] - memory_before_compile['allocated']\n",
    "    \n",
    "    print(f\"\\nüìä Compilation Analysis:\")\n",
    "    print(f\"   ‚úÖ Total compilation time: {total_compilation_time*1000:.1f} ms\")\n",
    "    print(f\"   ‚úÖ First execution time: {first_run_time*1000:.1f} ms\")\n",
    "    print(f\"   üìà Compilation overhead: {first_run_time/baseline_avg:.1f}x baseline\")\n",
    "    print(f\"   üóÑÔ∏è  Compilation memory overhead: {compilation_memory_overhead:.1f} MB\")\n",
    "    print(f\"   üìä Compilation peak memory: {compilation_peak_memory:.1f} MB\")\n",
    "    \n",
    "    # Test cached performance (Stage 6: Execution from cache)\n",
    "    print(f\"\\n‚ö° Cached Performance Analysis\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    cached_times = []\n",
    "    cached_peak_memory = []\n",
    "    \n",
    "    for _ in range(10):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            _ = compiled_model(test_input)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "            cached_peak_memory.append(torch.cuda.max_memory_allocated() / 1024**2)\n",
    "        \n",
    "        cached_times.append(time.perf_counter() - start)\n",
    "    \n",
    "    cached_avg = sum(cached_times) / len(cached_times)\n",
    "    cached_memory_avg = sum(cached_peak_memory) / len(cached_peak_memory) if cached_peak_memory else 0\n",
    "    speedup = baseline_avg / cached_avg if cached_avg > 0 else 0\n",
    "    \n",
    "    print(f\"   ‚úÖ Cached performance: {cached_avg*1000:.3f} ms\")\n",
    "    print(f\"   üöÄ Speedup achieved: {speedup:.2f}x\")\n",
    "    print(f\"   üìä Cached peak memory: {cached_memory_avg:.1f} MB\")\n",
    "    \n",
    "    # Memory efficiency analysis\n",
    "    memory_efficiency = baseline_memory_avg / cached_memory_avg if cached_memory_avg > 0 else 1\n",
    "    print(f\"   üß† Memory efficiency ratio: {memory_efficiency:.2f}x\")\n",
    "    \n",
    "    if memory_efficiency > 1:\n",
    "        print(f\"      ‚úÖ Compiled version uses {((1 - 1/memory_efficiency) * 100):.1f}% less peak memory\")\n",
    "    elif memory_efficiency < 1:\n",
    "        print(f\"      ‚ö†Ô∏è  Compiled version uses {((1/memory_efficiency - 1) * 100):.1f}% more peak memory\")\n",
    "    else:\n",
    "        print(f\"      ‚û°Ô∏è  Similar memory usage between versions\")\n",
    "    \n",
    "    # Economic analysis\n",
    "    if speedup > 1:\n",
    "        time_saved_per_run = baseline_avg - cached_avg\n",
    "        break_even_runs = total_compilation_time / time_saved_per_run\n",
    "        \n",
    "        print(f\"\\nüí∞ Economic Analysis:\")\n",
    "        print(f\"   Time saved per run: {time_saved_per_run*1000:.3f} ms\")\n",
    "        print(f\"   Break-even point: {break_even_runs:.1f} runs\")\n",
    "        \n",
    "        if break_even_runs < 10:\n",
    "            print(f\"   ‚úÖ Excellent ROI - compile immediately\")\n",
    "        elif break_even_runs < 50:\n",
    "            print(f\"   ‚ö° Good ROI - compile for repeated use\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  High break-even - evaluate use case\")\n",
    "    \n",
    "    # Memory overhead analysis\n",
    "    print(f\"\\nüß† Memory Overhead Analysis:\")\n",
    "    print(f\"   Compilation overhead: {compilation_memory_overhead:.1f} MB\")\n",
    "    print(f\"   Baseline peak usage: {baseline_memory_avg:.1f} MB\")\n",
    "    print(f\"   Compiled peak usage: {cached_memory_avg:.1f} MB\")\n",
    "    \n",
    "    overhead_percentage = (compilation_memory_overhead / baseline_memory_avg) * 100 if baseline_memory_avg > 0 else 0\n",
    "    print(f\"   Memory overhead percentage: {overhead_percentage:.1f}%\")\n",
    "    \n",
    "    if overhead_percentage < 10:\n",
    "        print(f\"   ‚úÖ Low memory overhead - negligible impact\")\n",
    "    elif overhead_percentage < 25:\n",
    "        print(f\"   ‚ö° Moderate memory overhead - acceptable for most cases\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  High memory overhead - consider memory constraints\")\n",
    "    \n",
    "    # Correctness verification\n",
    "    max_diff = (baseline_output - compiled_output).abs().max().item()\n",
    "    print(f\"\\nüîç Correctness check: Max difference = {max_diff:.2e}\")\n",
    "    if max_diff < 1e-5:\n",
    "        print(f\"   ‚úÖ Excellent numerical accuracy maintained\")\n",
    "    \n",
    "    print(f\"\\nüéì Pipeline Summary:\")\n",
    "    print(f\"   üì∏ Stage 1-3: Graph capture and optimization (automatic)\")\n",
    "    print(f\"   üîß Stage 4-6: Kernel generation and caching ({total_compilation_time*1000:.1f} ms)\")\n",
    "    print(f\"   ‚ö° Result: {speedup:.2f}x speedup after {break_even_runs:.1f} runs\")\n",
    "    print(f\"   üß† Memory: {memory_efficiency:.2f}x efficiency, {overhead_percentage:.1f}% overhead\")\n",
    "    \n",
    "    return {\n",
    "        'baseline_ms': baseline_avg * 1000,\n",
    "        'compiled_ms': cached_avg * 1000,\n",
    "        'compilation_ms': total_compilation_time * 1000,\n",
    "        'speedup': speedup,\n",
    "        'break_even': break_even_runs if speedup > 1 else float('inf'),\n",
    "        'baseline_memory_mb': baseline_memory_avg,\n",
    "        'compiled_memory_mb': cached_memory_avg,\n",
    "        'memory_overhead_mb': compilation_memory_overhead,\n",
    "        'memory_efficiency': memory_efficiency,\n",
    "        'memory_overhead_percent': overhead_percentage\n",
    "    }\n",
    "\n",
    "# Execute the comprehensive demonstration\n",
    "compilation_results = demonstrate_compilation_phases()\n",
    "\n",
    "print(f\"\\nüéØ Key Takeaways:\")\n",
    "print(f\"   ‚Ä¢ torch.compile() is a sophisticated 6-stage pipeline\")\n",
    "print(f\"   ‚Ä¢ Compilation overhead is significant but amortizes quickly\") \n",
    "print(f\"   ‚Ä¢ Generated kernels are cached for future use\")\n",
    "print(f\"   ‚Ä¢ Performance gains depend on model complexity and hardware\")\n",
    "print(f\"   ‚Ä¢ Memory efficiency varies - monitor both speed and memory usage\")\n",
    "print(f\"   ‚Ä¢ Consider memory overhead in resource-constrained environments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67de08e",
   "metadata": {},
   "source": [
    "## üß† Deep Dive: Memory Analysis in torch.compile()\n",
    "\n",
    "### **Understanding Memory Overhead and Efficiency**\n",
    "\n",
    "The enhanced demonstration above now includes comprehensive memory analysis that reveals crucial insights about how `torch.compile()` affects GPU memory usage. Let's break down what each memory metric tells us:\n",
    "\n",
    "#### **Key Memory Metrics Explained**\n",
    "\n",
    "1. **Compilation Memory Overhead**\n",
    "   - The additional memory required to store compiled kernels and metadata\n",
    "   - In our example: 16.0 MB overhead (13.5% of baseline)\n",
    "   - This is a one-time cost that persists while the compiled model is in memory\n",
    "\n",
    "2. **Peak Memory Usage Comparison**\n",
    "   - **Baseline**: 118.5 MB - memory used by eager mode execution\n",
    "   - **Compiled**: 88.1 MB - memory used by optimized kernels\n",
    "   - **Efficiency Ratio**: 1.34x - compiled version uses 25.6% less peak memory\n",
    "\n",
    "3. **Memory Efficiency Factors**\n",
    "   - **Kernel Fusion**: Reduces intermediate tensor allocations\n",
    "   - **Optimized Memory Layout**: Better access patterns reduce memory fragmentation\n",
    "   - **Reduced Temporary Storage**: Fused operations need fewer intermediate results\n",
    "\n",
    "#### **When Memory Efficiency Matters Most**\n",
    "\n",
    "- **Large Batch Processing**: Memory savings compound with larger inputs\n",
    "- **Limited GPU Memory**: Every MB counts on smaller GPUs (like our 6.4GB RTX 4050)\n",
    "- **Multi-Model Deployment**: Running multiple models simultaneously\n",
    "- **Long-Running Processes**: Sustained memory efficiency over time\n",
    "\n",
    "#### **Memory vs. Performance Trade-offs**\n",
    "\n",
    "Our results show an interesting pattern:\n",
    "- **Performance**: 16.53x speedup üöÄ\n",
    "- **Memory**: 1.34x efficiency (25.6% reduction) üß†  \n",
    "- **Overhead**: 13.5% compilation memory cost ‚ö†Ô∏è\n",
    "\n",
    "This demonstrates that `torch.compile()` can simultaneously improve both speed AND memory efficiency, making it valuable even in memory-constrained environments.\n",
    "\n",
    "### **Production Memory Considerations**\n",
    "\n",
    "#### **Planning for Memory Overhead**\n",
    "```python\n",
    "# Example memory planning calculation\n",
    "baseline_memory = 118.5  # MB\n",
    "compilation_overhead = 16.0  # MB  \n",
    "total_memory_needed = baseline_memory + compilation_overhead  # 134.5 MB\n",
    "\n",
    "# Factor this into your deployment planning\n",
    "safety_margin = 1.2  # 20% safety margin\n",
    "planned_memory = total_memory_needed * safety_margin  # 161.4 MB\n",
    "```\n",
    "\n",
    "#### **Memory Monitoring Best Practices**\n",
    "- Monitor both peak memory during execution AND persistent overhead\n",
    "- Track memory efficiency trends across different model architectures\n",
    "- Plan for worst-case memory scenarios in production deployments\n",
    "- Consider memory pressure when deciding between compilation modes\n",
    "\n",
    "The addition of memory analysis to our toolkit provides a complete picture of compilation trade-offs, enabling data-driven decisions about when and how to deploy `torch.compile()` in production systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb89308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéØ COMPREHENSIVE COMPILATION ANALYSIS SUMMARY\n",
      "============================================================\n",
      "\n",
      "‚ö° PERFORMANCE METRICS:\n",
      "   Baseline execution time:     20.253 ms\n",
      "   Compiled execution time:     1.226 ms\n",
      "   Compilation overhead:        331.0 ms\n",
      "   Speedup achieved:            16.52x\n",
      "   Break-even point:            17.4 runs\n",
      "\n",
      "üß† MEMORY METRICS:\n",
      "   Baseline peak memory:        119.6 MB\n",
      "   Compiled peak memory:        89.2 MB\n",
      "   Memory overhead:             16.0 MB\n",
      "   Memory efficiency ratio:     1.34x\n",
      "   Memory overhead percentage:  13.4%\n",
      "\n",
      "üí∞ ECONOMIC ANALYSIS:\n",
      "   Time saved per run:          19.027 ms\n",
      "   Total cost (compilation):    331.0 ms\n",
      "   Benefit after 100 runs:      1902.7 ms\n",
      "   Net benefit (100 runs):      1571.7 ms\n",
      "\n",
      "üéØ RECOMMENDATIONS:\n",
      "   ‚úÖ EXCELLENT - Compile immediately for production use\n",
      "   üß† MEMORY: Excellent memory efficiency gained\n",
      "\n",
      "============================================================\n",
      "\n",
      "üéì CONGRATULATIONS!\n",
      "You now have comprehensive memory and performance analysis capabilities!\n",
      "üìä The notebook measures:\n",
      "   ‚Ä¢ Execution time (baseline vs compiled)\n",
      "   ‚Ä¢ Memory overhead (compilation cost)\n",
      "   ‚Ä¢ Memory efficiency (peak usage comparison)\n",
      "   ‚Ä¢ Economic analysis (break-even calculations)\n",
      "   ‚Ä¢ Practical recommendations for production use\n"
     ]
    }
   ],
   "source": [
    "# üìà Comprehensive Results Summary\n",
    "\n",
    "def display_compilation_summary(results: dict):\n",
    "    \"\"\"\n",
    "    Display a comprehensive summary of compilation results including memory analysis\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéØ COMPREHENSIVE COMPILATION ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Performance Metrics\n",
    "    print(\"\\n‚ö° PERFORMANCE METRICS:\")\n",
    "    print(f\"   Baseline execution time:     {results['baseline_ms']:.3f} ms\")\n",
    "    print(f\"   Compiled execution time:     {results['compiled_ms']:.3f} ms\")\n",
    "    print(f\"   Compilation overhead:        {results['compilation_ms']:.1f} ms\")\n",
    "    print(f\"   Speedup achieved:            {results['speedup']:.2f}x\")\n",
    "    print(f\"   Break-even point:            {results['break_even']:.1f} runs\")\n",
    "    \n",
    "    # Memory Metrics\n",
    "    print(\"\\nüß† MEMORY METRICS:\")\n",
    "    print(f\"   Baseline peak memory:        {results['baseline_memory_mb']:.1f} MB\")\n",
    "    print(f\"   Compiled peak memory:        {results['compiled_memory_mb']:.1f} MB\")\n",
    "    print(f\"   Memory overhead:             {results['memory_overhead_mb']:.1f} MB\")\n",
    "    print(f\"   Memory efficiency ratio:     {results['memory_efficiency']:.2f}x\")\n",
    "    print(f\"   Memory overhead percentage:  {results['memory_overhead_percent']:.1f}%\")\n",
    "    \n",
    "    # Economic Analysis\n",
    "    print(\"\\nüí∞ ECONOMIC ANALYSIS:\")\n",
    "    time_saved_per_run = results['baseline_ms'] - results['compiled_ms']\n",
    "    total_benefit_100_runs = time_saved_per_run * 100\n",
    "    total_cost = results['compilation_ms']\n",
    "    net_benefit_100_runs = total_benefit_100_runs - total_cost\n",
    "    \n",
    "    print(f\"   Time saved per run:          {time_saved_per_run:.3f} ms\")\n",
    "    print(f\"   Total cost (compilation):    {total_cost:.1f} ms\")\n",
    "    print(f\"   Benefit after 100 runs:      {total_benefit_100_runs:.1f} ms\")\n",
    "    print(f\"   Net benefit (100 runs):      {net_benefit_100_runs:.1f} ms\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\nüéØ RECOMMENDATIONS:\")\n",
    "    if results['speedup'] > 5 and results['break_even'] < 50:\n",
    "        print(\"   ‚úÖ EXCELLENT - Compile immediately for production use\")\n",
    "    elif results['speedup'] > 2 and results['break_even'] < 100:\n",
    "        print(\"   ‚ö° GOOD - Compile for repeated execution scenarios\")\n",
    "    elif results['speedup'] > 1 and results['break_even'] < 500:\n",
    "        print(\"   ‚ö†Ô∏è  MODERATE - Evaluate based on specific use case\")\n",
    "    else:\n",
    "        print(\"   ‚ùå POOR - Consider alternative optimization strategies\")\n",
    "        \n",
    "    if results['memory_efficiency'] > 1.2:\n",
    "        print(\"   üß† MEMORY: Excellent memory efficiency gained\")\n",
    "    elif results['memory_efficiency'] > 1.0:\n",
    "        print(\"   üß† MEMORY: Modest memory efficiency improvement\")\n",
    "    elif results['memory_overhead_percent'] < 20:\n",
    "        print(\"   üß† MEMORY: Acceptable memory overhead\")\n",
    "    else:\n",
    "        print(\"   üß† MEMORY: High memory overhead - monitor carefully\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Display comprehensive summary of our compilation results\n",
    "display_compilation_summary(compilation_results)\n",
    "\n",
    "print(\"\\nüéì CONGRATULATIONS!\")\n",
    "print(\"You now have comprehensive memory and performance analysis capabilities!\")\n",
    "print(\"üìä The notebook measures:\")\n",
    "print(\"   ‚Ä¢ Execution time (baseline vs compiled)\")\n",
    "print(\"   ‚Ä¢ Memory overhead (compilation cost)\")\n",
    "print(\"   ‚Ä¢ Memory efficiency (peak usage comparison)\")\n",
    "print(\"   ‚Ä¢ Economic analysis (break-even calculations)\")\n",
    "print(\"   ‚Ä¢ Practical recommendations for production use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8d4d49",
   "metadata": {},
   "source": [
    "\n",
    "### **Production-Ready Insights**\n",
    "\n",
    "1. **When to Compile**: Compilation is beneficial for models with repeated execution patterns, batch processing workflows, and inference serving scenarios.  \n",
    "2. **When NOT to Compile**: Avoid compilation for single-shot execution scenarios, rapidly changing model architectures, and during development or debugging phases.  \n",
    "3. **Optimization Strategy**: Begin with baseline measurements, apply compilation systematically, measure and verify improvements, and plan for cache warming in production environments.\n",
    "\n",
    "## Important Considerations here\n",
    "\n",
    "### **Environment Dependencies**\n",
    "\n",
    "- **GPU Architecture**: Results vary significantly between GPU generations\n",
    "- **PyTorch Version**: Compilation features evolve rapidly\n",
    "- **Driver Version**: CUDA capabilities affect optimization opportunities\n",
    "- **System Load**: Other processes can affect measurements\n",
    "\n",
    "### **Model Complexity Effects**\n",
    "\n",
    "- **Simple operations**: May not show significant speedup\n",
    "- **Complex models**: Generally benefit more from compilation\n",
    "- **Batch size**: Larger batches typically show better compilation benefits\n",
    "- **Operation types**: Some operations optimize better than others\n",
    "\n",
    "### **Production Considerations**\n",
    "\n",
    "- **Cache warming**: Plan for first-run overhead in production\n",
    "- **Memory usage**: Compilation can increase memory requirements\n",
    "- **Debugging complexity**: Compiled models are harder to debug\n",
    "- **Version compatibility**: Cached kernels may not transfer between environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f083741",
   "metadata": {},
   "source": [
    "# Summary: PyTorch Compilation\n",
    "\n",
    "## Congratulations on Completing Chapter 1!\n",
    "\n",
    "We have successfully completed the foundational chapter of our PyTorch compilation mastery series. This chapter has equipped us with both the theoretical understanding and practical skills necessary to leverage PyTorch's compilation system effectively.\n",
    "\n",
    "---\n",
    "\n",
    "## Knowledge Gained: A Comprehensive Review\n",
    "\n",
    "### **Architectural Understanding**\n",
    "\n",
    "#### **The Six-Stage Compilation Pipeline**\n",
    "We now understand PyTorch compilation as a sophisticated transformation process:\n",
    "\n",
    "1. **Graph Capture** ‚Üí Converting dynamic Python to static computational graphs\n",
    "2. **Graph Optimization** ‚Üí High-level transformations for efficiency\n",
    "3. **Backend Selection** ‚Üí Choosing optimal execution strategies  \n",
    "4. **Kernel Generation** ‚Üí Creating specialized GPU code\n",
    "5. **Compilation** ‚Üí Transforming to machine-executable code\n",
    "6. **Caching & Execution** ‚Üí Persistent storage and efficient execution\n",
    "\n",
    "**Key Insight**: Compilation is an investment strategy with high fixed costs and low marginal costs.\n",
    "\n",
    "#### **Mental Models Developed**\n",
    "- **Economic Perspective**: Compilation as an optimization investment with measurable ROI\n",
    "- **Performance Trade-offs**: Understanding when compilation helps vs. hurts\n",
    "- **System Thinking**: Recognizing compilation as part of a larger optimization ecosystem\n",
    "\n",
    "### ** Technical Competencies Acquired**\n",
    "\n",
    "#### **Environment Mastery**\n",
    "```bash\n",
    "# Essential environment variables you now understand\n",
    "TORCH_LOGS=\"+dynamo\"                    # Basic compilation tracing\n",
    "TORCHDYNAMO_VERBOSE=\"1\"                 # Detailed compilation logging  \n",
    "TORCH_COMPILE_DEBUG=\"1\"                 # Expert-level debugging\n",
    "TRITON_PRINT_AUTOTUNING=\"1\"            # Kernel optimization insights\n",
    "```\n",
    "\n",
    "#### **Performance Analysis Framework**\n",
    "You've mastered a complete methodology for compilation analysis:\n",
    "\n",
    "**1. Baseline Establishment**\n",
    "- Proper warmup procedures\n",
    "- Statistical measurement techniques\n",
    "- GPU synchronization protocols\n",
    "\n",
    "**2. Compilation Cost Analysis**\n",
    "- Overhead measurement\n",
    "- Break-even calculations\n",
    "- Economic impact assessment\n",
    "\n",
    "**3. Benefit Quantification**\n",
    "- Speedup measurement\n",
    "- Consistency analysis\n",
    "- Scalability evaluation\n",
    "\n",
    "### **Strategic Thinking Skills**\n",
    "\n",
    "#### **Decision-Making Framework**\n",
    "You can now systematically evaluate compilation decisions:\n",
    "\n",
    "**When to Compile**:\n",
    "- ‚úÖ Repeated execution patterns (batch processing, inference serving)\n",
    "- ‚úÖ Models with fusion opportunities (sequential operations)\n",
    "- ‚úÖ Performance-critical applications (production inference)\n",
    "- ‚úÖ Stable model architectures (post-development phase)\n",
    "\n",
    "**When NOT to Compile**:\n",
    "- ‚ùå Single-shot execution scenarios (one-time analysis), e.g., Quantization\n",
    "- ‚ùå Rapid prototyping phases (frequent model changes)\n",
    "- ‚ùå Development and debugging (need Python-level debugging)\n",
    "- ‚ùå Simple operations (insufficient optimization opportunities)\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Skills Mastered\n",
    "\n",
    "### **Development Workflow Integration**\n",
    "You can now integrate compilation analysis into your development process:\n",
    "\n",
    "```python\n",
    "# Standard compilation analysis workflow\n",
    "def analyze_model_compilation(model, test_input):\n",
    "    # 1. Establish baseline\n",
    "    baseline_time = measure_baseline_performance(model, test_input)\n",
    "    \n",
    "    # 2. Measure compilation overhead  \n",
    "    compilation_time, first_run_time = measure_compilation_cost(model, test_input)\n",
    "    \n",
    "    # 3. Evaluate cached performance\n",
    "    cached_time = measure_cached_performance(model, test_input)\n",
    "    \n",
    "    # 4. Economic analysis\n",
    "    break_even_point = calculate_break_even(compilation_time, baseline_time, cached_time)\n",
    "    \n",
    "    # 5. Correctness verification\n",
    "    verify_numerical_accuracy(model, test_input)\n",
    "    \n",
    "    return CompilationAnalysis(baseline_time, cached_time, break_even_point)\n",
    "```\n",
    "\n",
    "### **Debugging and Troubleshooting**\n",
    "\n",
    "- **Environment configuration** for comprehensive debugging\n",
    "- **Log interpretation** for compilation issues\n",
    "- **Performance regression** detection and analysis\n",
    "- **Systematic troubleshooting** methodologies\n",
    "\n",
    "### **Production Planning**\n",
    "\n",
    "- **Cache warming strategies** for deployment\n",
    "- **Memory overhead planning** for resource allocation\n",
    "- **Performance monitoring** approaches for production systems\n",
    "- **Version compatibility** considerations for deployments\n",
    "\n",
    "---\n",
    "\n",
    "## üåü Advanced Insights for Expert Practice\n",
    "\n",
    "### **Performance Optimization Principles**\n",
    "1. **Measure First**: Always establish baselines before optimizing\n",
    "2. **Think Economically**: Consider total cost of ownership, not just peak performance\n",
    "3. **Plan for Production**: Account for cache warming and memory overhead\n",
    "4. **Verify Continuously**: Ensure optimizations maintain correctness\n",
    "\n",
    "### **Common Pitfalls to Avoid**\n",
    "- **Premature Compilation**: Applying compilation before stabilizing model architecture\n",
    "- **Ignoring Overhead**: Focusing only on speedup without considering compilation cost\n",
    "- **Environment Inconsistency**: Assuming results transfer across different hardware/software configurations\n",
    "- **Incomplete Verification**: Optimizing without thorough correctness checking\n",
    "\n",
    "### **Professional Best Practices**\n",
    "- **Documentation**: Always document compilation decisions and their rationale\n",
    "- **Monitoring**: Establish metrics for compilation effectiveness in production\n",
    "- **Version Control**: Track compilation configurations alongside code changes\n",
    "- **Team Communication**: Share compilation insights and best practices with team members\n",
    "\n",
    "---\n",
    "\n",
    "# Your Next Learning Journey\n",
    "\n",
    "## **Immediate Application Opportunities**\n",
    "\n",
    "**Put Your Knowledge to the Test**: \n",
    "\n",
    "Take one of your own PyTorch models and perform a complete compilation analysis using the methodology you've learned:\n",
    "\n",
    "1. **Establish Environment**: Configure debugging and measurement setup\n",
    "2. **Baseline Analysis**: Measure eager mode performance with proper statistics\n",
    "3. **Compilation Evaluation**: Measure overhead and cached performance\n",
    "4. **Economic Assessment**: Calculate break-even point and ROI projections\n",
    "5. **Decision Making**: Determine whether compilation is beneficial for your use case\n",
    "6. **Documentation**: Write a brief report summarizing your findings and recommendations\n",
    "\n",
    "**Success Criteria**: You should be able to make a data-driven recommendation about whether to use compilation for your specific model and use case.\n",
    "\n",
    "## **Preparing for Advanced Topics**\n",
    "\n",
    "### **Chapter 2: Advanced Debugging & Optimization**\n",
    "Coming next, you'll learn:\n",
    "- **Expert Debugging Techniques**: Deep dive into TorchDynamo and Triton internals\n",
    "- **Kernel Analysis**: Understanding and optimizing generated GPU kernels\n",
    "- **Advanced Benchmarking**: Sophisticated performance measurement techniques\n",
    "- **Custom Backend Development**: Creating specialized optimization passes\n",
    "\n",
    "### **Chapter 3: Production Deployment & Best Practices**\n",
    "In the final chapter, you'll master:\n",
    "- **Enterprise Deployment Patterns**: Production-grade compilation strategies\n",
    "- **Monitoring and Alerting**: Systematic performance tracking in production\n",
    "- **Troubleshooting Methodologies**: Diagnosing and resolving compilation issues at scale\n",
    "- **Expert Recommendations**: Battle-tested optimization patterns from industry leaders\n",
    "\n",
    "---\n",
    "\n",
    "**Ready for the next level? Continue with Chapter 2: Advanced Debugging & Optimization to master the expert-level techniques that will set you apart as a performance optimization specialist! üöÄ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeb73b7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-qat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
