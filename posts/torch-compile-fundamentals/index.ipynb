{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4648c02e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"PyTorch compile: Understanding the Compilation Pipeline (Part 1)\"\n",
    "format: \n",
    "  html:\n",
    "    code-fold: true\n",
    "draft: false\n",
    "jupyter: python3\n",
    "page-layout: full\n",
    "categories: [pytorch, torch-compile, fundamentals, compilation-pipeline]\n",
    "description: \"Master the fundamentals of PyTorch's torch.compile() system - from basic setup to understanding the 6-stage compilation pipeline and performance characteristics.\"\n",
    "image: \"torch-compile-fundamentals.jpg\"\n",
    "author: \"Innovation Crucible\"\n",
    "date: \"2025-06-16\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d250857e",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## Deconstructing torch.compile()\n",
    "\n",
    "PyTorch's `torch.compile()` transforms Python-based neural networks into specialized GPU kernels through a deterministic six-stage compilation pipeline. This transformation process‚Äîfirst introduced in PyTorch 2.0‚Äîrepresents the most significant performance advancement since automatic differentiation, delivering speedups ranging from 1.5x to 10x for production workloads.\n",
    "\n",
    "Understanding this pipeline empowers you to:\n",
    "\n",
    "- **Diagnose performance bottlenecks** by analyzing compilation logs and kernel generation patterns\n",
    "- **Design models that naturally exploit** kernel fusion, memory coalescing, and instruction-level parallelism\n",
    "- **Deploy production systems** that leverage cached kernels and handle compilation overhead strategically\n",
    "- **Debug compilation failures** through systematic analysis of graph capture and optimization stages\n",
    "\n",
    "The six stages‚Äîgraph capture via TorchDynamo, frontend optimization, backend selection, Triton kernel generation, LLVM compilation, and persistent caching‚Äîeach serve specific optimization goals while maintaining numerical accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "# Section 1.1: Foundation & Environment Setup\n",
    "\n",
    "### Knowledge Prerequisites\n",
    "\n",
    "This tutorial assumes proficiency with:\n",
    "\n",
    "- **PyTorch internals**: autograd mechanics, device management, tensor memory layout (contiguous vs. strided)\n",
    "- **CUDA programming**: kernel launch parameters, memory hierarchies (global, shared, registers), warp-level primitives\n",
    "- **Performance analysis**: statistical significance in benchmarking, identifying bottlenecks through profiling tools\n",
    "- **Python metaprogramming**: decorators, context managers, bytecode inspection fundamentals\n",
    "\n",
    "### Hardware Requirements\n",
    "\n",
    "Effective compilation learning requires:\n",
    "\n",
    "- **CUDA GPU**: Compute Capability ‚â• 7.0 (Tensor Cores available on Volta+, RTX 2080 or newer recommended)\n",
    "- **GPU Memory**: ‚â•8 GB VRAM for compilation overhead and kernel caching\n",
    "- **CPU**: Multi-core processor for parallel compilation workloads (Triton autotuning is CPU-intensive)\n",
    "\n",
    "### Software Environment\n",
    "\n",
    "Install the complete toolkit:\n",
    "\n",
    "```bash\n",
    "# PyTorch 2.1+ with CUDA 11.8 support\n",
    "pip install torch>=2.1.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Triton compiler for GPU kernel generation\n",
    "pip install triton>=2.1.0\n",
    "\n",
    "# Performance analysis ecosystem\n",
    "pip install numpy matplotlib seaborn pandas psutil\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5985ca90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using device: cuda\n",
      "   GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "   Memory: 6.4 GB\n",
      "   Compute Capability: (8, 9)\n",
      "üì¶ PyTorch Version: 2.5.1\n",
      "üîß Triton Available: False\n",
      "‚úÖ torch.compile() is available!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Set optimal environment for learning\n",
    "os.environ['TORCH_LOGS'] = '+dynamo'\n",
    "os.environ['TORCHDYNAMO_VERBOSE'] = '1'\n",
    "\n",
    "# Check GPU availability and setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"   Compute Capability: {torch.cuda.get_device_capability()}\")\n",
    "\n",
    "print(f\"üì¶ PyTorch Version: {torch.__version__}\")\n",
    "print(f\"üîß Triton Available: {torch.cuda.is_available() and hasattr(torch.backends, 'triton')}\")\n",
    "\n",
    "# Verify torch.compile is available\n",
    "if hasattr(torch, 'compile'):\n",
    "    print(\"‚úÖ torch.compile() is available!\")\n",
    "else:\n",
    "    print(\"‚ùå torch.compile() not available. Please upgrade PyTorch to 2.0+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d85d35",
   "metadata": {},
   "source": [
    "# Section 1.2: The Architecture of the Compilation Pipeline\n",
    "\n",
    "## A Six-Stage Transformation: From Python to Optimized Kernels\n",
    "\n",
    "PyTorch 2.x introduces a powerful just-in-time (JIT) compilation pipeline that transforms dynamic Python code into highly optimized kernels. This process unfolds across six principal stages:\n",
    "\n",
    "1. **Graph Capture with TorchDynamo**\n",
    "\n",
    "The process begins with TorchDynamo, which safely captures your PyTorch model's computational graph directly from Python bytecode. As your model runs, Dynamo intercepts each operation without requiring static tracing or example inputs. By analyzing low-level bytecode instructions like CALL_FUNCTION, it records a faithful, graph-based representation of your model's logic, known as an FX Graph. This method gracefully handles Python's dynamic features, such as control flow, by creating graph \"breaks\" only when necessary, keeping the majority of the code in an optimizable format.\n",
    "\n",
    "2. **Frontend Optimizations**\n",
    "\n",
    "Once captured, the FX Graph enters a frontend optimization phase. Here, a series of graph-to-graph transformations refine the model's structure. Pattern-matching algorithms identify and fuse sequential pointwise operations (like activations and additions) into a single, more efficient operation. Redundant computations are eliminated through dead code elimination, while constant folding pre-computes any parts of the graph that have static inputs. Furthermore, memory planners analyze tensor usage, optimizing allocation to reduce fragmentation and safely enable in-place memory operations.\n",
    "\n",
    "3. **Backend Selection (Partitioning)**\n",
    "\n",
    "The optimized graph is then intelligently partitioned. Using a sophisticated cost model, PyTorch decides how to handle different segments of the graph. Fusable sections composed of pointwise and reduction operations are typically delegated to the Triton backend for custom kernel generation. More complex or pre-optimized PyTorch operations might fall back to the standard ATen backend to leverage its highly tuned library functions. For specific hardware and model patterns (like conv-batchnorm-relu), partitions may be sent to specialized backends like NVIDIA's TensorRT.\n",
    "\n",
    "4. **Kernel Generation via Triton**\n",
    "\n",
    "For the graph partitions sent to the Triton backend, the next step is generating custom GPU kernels. Triton acts as a Python-based DSL (Domain-Specific Language) for creating high-performance parallel code. It translates the high-level graph patterns into efficient parallel algorithms. For example, fused pointwise operations become a single element-wise CUDA kernel. To ensure peak performance, Triton‚Äôs integrated autotuner automatically benchmarks different configurations, such as varying thread block sizes, to find the optimal setup that maximizes GPU hardware occupancy.\n",
    "\n",
    "5. **The LLVM Compilation Pipeline**\n",
    "\n",
    "The high-level Triton code now undergoes a multi-stage compilation process using the LLVM framework. The Triton IR (Intermediate Representation) is first lowered to LLVM IR. From there, it is compiled into PTX (Parallel Thread Execution), a GPU-agnostic assembly language. Finally, the NVIDIA ptxas compiler performs the last step, translating the PTX into SASS (Shader Assembly), the native machine code for the target GPU architecture (e.g., Ampere or Ada Lovelace). This final stage applies critical hardware-specific optimizations, including precise instruction scheduling and register allocation.\n",
    "\n",
    "6. **Caching and Runtime Execution**\n",
    "\n",
    "The final SASS machine code is the executable kernel. To eliminate redundant work, this binary is cached on disk. The cache key is derived from a signature of the operation, the properties of the input tensors (like shape and data type), and the GPU hardware details. When the model is run again with the same configuration, this cache is hit, and the compilation pipeline is bypassed entirely, allowing the kernel to be launched directly via the CUDA API for maximum speed. A cache miss, which occurs on the first run or when input shapes change, triggers recompilation, explaining why the initial execution is notably slower than all subsequent runs."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7270c628",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "engine: jupyter\n",
    "---\n",
    "\n",
    "```{mermaid}\n",
    "---\n",
    "title: PyTorch Compilation Pipeline\n",
    "config:\n",
    "  theme: base\n",
    "  themeVariables:\n",
    "    primaryColor: \"#ff6b6b\"\n",
    "    primaryTextColor: \"#2c3e50\"\n",
    "    primaryBorderColor: \"#3498db\"\n",
    "    lineColor: \"#34495e\"\n",
    "    secondaryColor: \"#74b9ff\"\n",
    "    tertiaryColor: \"#a29bfe\"\n",
    "---\n",
    "\n",
    "flowchart LR\n",
    "    %% Define styles\n",
    "    classDef startEnd fill:#e74c3c,stroke:#c0392b,stroke-width:3px,color:#fff,font-weight:bold\n",
    "    classDef process fill:#3498db,stroke:#2980b9,stroke-width:2px,color:#fff,font-weight:bold\n",
    "    classDef optimization fill:#f39c12,stroke:#e67e22,stroke-width:2px,color:#fff,font-weight:bold\n",
    "    classDef generation fill:#9b59b6,stroke:#8e44ad,stroke-width:2px,color:#fff,font-weight:bold\n",
    "    classDef storage fill:#2ecc71,stroke:#27ae60,stroke-width:2px,color:#fff,font-weight:bold\n",
    "    \n",
    "    %% Define subgraphs first to ensure horizontal alignment\n",
    "    subgraph Frontend [\"üîß Frontend Processing\"]\n",
    "        direction TB\n",
    "        A((\"üêç<br/>Python<br/>Code\")):::startEnd\n",
    "        B[\"üìä Graph<br/>Capture\"]:::process\n",
    "        C{\"‚ö° Graph<br/>Optimization\"}:::optimization\n",
    "        A ==> B ==> C\n",
    "    end\n",
    "    \n",
    "    subgraph Backend [\"‚ö° Backend Processing\"]\n",
    "        direction TB\n",
    "        D[/\"üéØ Backend<br/>Selection\"/]:::process\n",
    "        E[[\"‚öôÔ∏è Kernel<br/>Generation\"]]:::generation\n",
    "        F[\"üî® Compilation\"]:::generation\n",
    "        D ==> E ==> F\n",
    "    end\n",
    "    \n",
    "    subgraph Runtime [\"üèÉ Runtime\"]\n",
    "        direction TB\n",
    "        G[(\"üíæ Caching\")]:::storage\n",
    "        H((\"üöÄ<br/>Execution\")):::startEnd\n",
    "        G ==> H\n",
    "    end\n",
    "    \n",
    "    %% Connect the subgraphs\n",
    "    Frontend ==> Backend ==> Runtime\n",
    "    \n",
    "    %% Style the subgraphs\n",
    "    style Frontend fill:#ecf0f1,stroke:#bdc3c7,stroke-width:2px,stroke-dasharray: 5 5\n",
    "    style Backend fill:#fdf2e9,stroke:#f39c12,stroke-width:2px,stroke-dasharray: 5 5\n",
    "    style Runtime fill:#e8f8f5,stroke:#2ecc71,stroke-width:2px,stroke-dasharray: 5 5\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cda62c",
   "metadata": {},
   "source": [
    "## Stage 1: Graph Capture\n",
    "### From Python to FX Graphs\n",
    "\n",
    "TorchDynamo employs **Python frame evaluation hooks** to intercept bytecode execution at the CPython interpreter level. Rather than requiring traced execution with example inputs, it analyzes `CALL_FUNCTION`, `LOAD_GLOBAL`, and `BINARY_OP` bytecodes to reconstruct the computational graph during actual model execution.\n",
    "\n",
    "**Concrete Technical Mechanisms**:\n",
    "\n",
    "- **Bytecode interception**: Hooks into CPython's `_PyEval_EvalFrameDefault` to capture function calls as they execute\n",
    "- **Symbolic execution**: Records tensor operations without executing them, building graph nodes from PyTorch function calls\n",
    "- **Control flow specialization**: When encountering `if` statements or loops, TorchDynamo captures the taken branch and inserts runtime guards\n",
    "- **Shape and dtype binding**: Records tensor metadata (`torch.Size([32, 128, 512])`, `torch.float32`) as graph constraints\n",
    "\n",
    "**Performance Implications**:\n",
    "\n",
    "- **Graph construction overhead**: Bytecode analysis adds 10-50Œºs per captured operation\n",
    "- **Memory overhead**: FX Graph nodes consume ~200 bytes per operation\n",
    "- **Specialization cost**: Each unique control flow path generates a separate cached graph\n",
    "\n",
    "**Critical Success Factors**:\n",
    "\n",
    "- Models with stable control flow paths (minimal dynamic branching) optimize effectively\n",
    "- Architectures using standard PyTorch operations (nn.Linear, F.relu) capture completely\n",
    "- Custom Python functions may trigger graph breaks, forcing fallbacks to eager execution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be4e2d5",
   "metadata": {},
   "source": [
    "The following demonstration uses a branching model to showcase TorchDynamo's specialization behavior. When the `condition` parameter changes, TorchDynamo generates separate optimized graphs‚Äîone for each branch‚Äîrather than creating a single graph with conditional logic.\n",
    "\n",
    "```python\n",
    "# Define a simple model with control flow to showcase graph capture\n",
    "class SimpleBranchModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(10, 20)\n",
    "        self.linear2 = nn.Linear(20, 5)\n",
    "        self.linear3 = nn.Linear(20, 5)\n",
    "\n",
    "    def forward(self, x, condition: bool):\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        if condition:\n",
    "            # Path 1: Different computation branch\n",
    "            x = self.linear2(x)\n",
    "            x = torch.sigmoid(x)\n",
    "        else:\n",
    "            # Path 2: Alternative computation branch\n",
    "            x = self.linear3(x)\n",
    "            x = torch.tanh(x)\n",
    "        return x\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "756ab8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Graph capture (condition=False):\n",
      "  ‚Ä¢ Ops captured: 4\n",
      "  ‚Ä¢ Number of graphs: 1\n",
      "  ‚Ä¢ Generated graph:\n",
      "GraphModule()\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, L_self_modules_linear1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_linear1_parameters_bias_ : torch.nn.parameter.Parameter, L_x_ : torch.Tensor, L_self_modules_linear3_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_linear3_parameters_bias_ : torch.nn.parameter.Parameter):\n",
      "    l_self_modules_linear1_parameters_weight_ = L_self_modules_linear1_parameters_weight_\n",
      "    l_self_modules_linear1_parameters_bias_ = L_self_modules_linear1_parameters_bias_\n",
      "    l_x_ = L_x_\n",
      "    l_self_modules_linear3_parameters_weight_ = L_self_modules_linear3_parameters_weight_\n",
      "    l_self_modules_linear3_parameters_bias_ = L_self_modules_linear3_parameters_bias_\n",
      "    x = torch._C._nn.linear(l_x_, l_self_modules_linear1_parameters_weight_, l_self_modules_linear1_parameters_bias_);  l_x_ = l_self_modules_linear1_parameters_weight_ = l_self_modules_linear1_parameters_bias_ = None\n",
      "    x_1 = torch.nn.functional.relu(x);  x = None\n",
      "    x_2 = torch._C._nn.linear(x_1, l_self_modules_linear3_parameters_weight_, l_self_modules_linear3_parameters_bias_);  x_1 = l_self_modules_linear3_parameters_weight_ = l_self_modules_linear3_parameters_bias_ = None\n",
      "    x_3 = torch.tanh(x_2);  x_2 = None\n",
      "    return (x_3,)\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\n",
      "  ‚Ä¢ Detailed debug info:\n",
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, L_self_modules_linear1_parameters_weight_: \"f32[20, 10]\", L_self_modules_linear1_parameters_bias_: \"f32[20]\", L_x_: \"f32[32, 10]\", L_self_modules_linear3_parameters_weight_: \"f32[5, 20]\", L_self_modules_linear3_parameters_bias_: \"f32[5]\"):\n",
      "        l_self_modules_linear1_parameters_weight_ = L_self_modules_linear1_parameters_weight_\n",
      "        l_self_modules_linear1_parameters_bias_ = L_self_modules_linear1_parameters_bias_\n",
      "        l_x_ = L_x_\n",
      "        l_self_modules_linear3_parameters_weight_ = L_self_modules_linear3_parameters_weight_\n",
      "        l_self_modules_linear3_parameters_bias_ = L_self_modules_linear3_parameters_bias_\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:10 in forward, code: x = self.linear1(x)\n",
      "        x: \"f32[32, 20]\" = torch._C._nn.linear(l_x_, l_self_modules_linear1_parameters_weight_, l_self_modules_linear1_parameters_bias_);  l_x_ = l_self_modules_linear1_parameters_weight_ = l_self_modules_linear1_parameters_bias_ = None\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:11 in forward, code: x = F.relu(x)\n",
      "        x_1: \"f32[32, 20]\" = torch.nn.functional.relu(x);  x = None\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:18 in forward, code: x = self.linear3(x)\n",
      "        x_2: \"f32[32, 5]\" = torch._C._nn.linear(x_1, l_self_modules_linear3_parameters_weight_, l_self_modules_linear3_parameters_bias_);  x_1 = l_self_modules_linear3_parameters_weight_ = l_self_modules_linear3_parameters_bias_ = None\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:19 in forward, code: x = torch.tanh(x)\n",
      "        x_3: \"f32[32, 5]\" = torch.tanh(x_2);  x_2 = None\n",
      "        return (x_3,)\n",
      "        \n",
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, L_self_modules_linear1_parameters_weight_: \"f32[20, 10]\", L_self_modules_linear1_parameters_bias_: \"f32[20]\", L_x_: \"f32[32, 10]\", L_self_modules_linear3_parameters_weight_: \"f32[5, 20]\", L_self_modules_linear3_parameters_bias_: \"f32[5]\"):\n",
      "        l_self_modules_linear1_parameters_weight_ = L_self_modules_linear1_parameters_weight_\n",
      "        l_self_modules_linear1_parameters_bias_ = L_self_modules_linear1_parameters_bias_\n",
      "        l_x_ = L_x_\n",
      "        l_self_modules_linear3_parameters_weight_ = L_self_modules_linear3_parameters_weight_\n",
      "        l_self_modules_linear3_parameters_bias_ = L_self_modules_linear3_parameters_bias_\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:10 in forward, code: x = self.linear1(x)\n",
      "        x: \"f32[32, 20]\" = torch._C._nn.linear(l_x_, l_self_modules_linear1_parameters_weight_, l_self_modules_linear1_parameters_bias_);  l_x_ = l_self_modules_linear1_parameters_weight_ = l_self_modules_linear1_parameters_bias_ = None\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:11 in forward, code: x = F.relu(x)\n",
      "        x_1: \"f32[32, 20]\" = torch.nn.functional.relu(x);  x = None\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:18 in forward, code: x = self.linear3(x)\n",
      "        x_2: \"f32[32, 5]\" = torch._C._nn.linear(x_1, l_self_modules_linear3_parameters_weight_, l_self_modules_linear3_parameters_bias_);  x_1 = l_self_modules_linear3_parameters_weight_ = l_self_modules_linear3_parameters_bias_ = None\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:19 in forward, code: x = torch.tanh(x)\n",
      "        x_3: \"f32[32, 5]\" = torch.tanh(x_2);  x_2 = None\n",
      "        return (x_3,)\n",
      "        \n",
      "\n",
      "==================================================\n",
      "\n",
      "üîç Graph capture (condition=True):\n",
      "  ‚Ä¢ Ops captured: 4\n",
      "  ‚Ä¢ Number of graphs: 1\n",
      "  ‚Ä¢ Generated graph:\n",
      "GraphModule()\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, L_self_modules_linear1_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_linear1_parameters_bias_ : torch.nn.parameter.Parameter, L_x_ : torch.Tensor, L_self_modules_linear2_parameters_weight_ : torch.nn.parameter.Parameter, L_self_modules_linear2_parameters_bias_ : torch.nn.parameter.Parameter):\n",
      "    l_self_modules_linear1_parameters_weight_ = L_self_modules_linear1_parameters_weight_\n",
      "    l_self_modules_linear1_parameters_bias_ = L_self_modules_linear1_parameters_bias_\n",
      "    l_x_ = L_x_\n",
      "    l_self_modules_linear2_parameters_weight_ = L_self_modules_linear2_parameters_weight_\n",
      "    l_self_modules_linear2_parameters_bias_ = L_self_modules_linear2_parameters_bias_\n",
      "    x = torch._C._nn.linear(l_x_, l_self_modules_linear1_parameters_weight_, l_self_modules_linear1_parameters_bias_);  l_x_ = l_self_modules_linear1_parameters_weight_ = l_self_modules_linear1_parameters_bias_ = None\n",
      "    x_1 = torch.nn.functional.relu(x);  x = None\n",
      "    x_2 = torch._C._nn.linear(x_1, l_self_modules_linear2_parameters_weight_, l_self_modules_linear2_parameters_bias_);  x_1 = l_self_modules_linear2_parameters_weight_ = l_self_modules_linear2_parameters_bias_ = None\n",
      "    x_3 = torch.sigmoid(x_2);  x_2 = None\n",
      "    return (x_3,)\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\n",
      "  ‚Ä¢ Detailed debug info:\n",
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, L_self_modules_linear1_parameters_weight_: \"f32[20, 10]\", L_self_modules_linear1_parameters_bias_: \"f32[20]\", L_x_: \"f32[32, 10]\", L_self_modules_linear2_parameters_weight_: \"f32[5, 20]\", L_self_modules_linear2_parameters_bias_: \"f32[5]\"):\n",
      "        l_self_modules_linear1_parameters_weight_ = L_self_modules_linear1_parameters_weight_\n",
      "        l_self_modules_linear1_parameters_bias_ = L_self_modules_linear1_parameters_bias_\n",
      "        l_x_ = L_x_\n",
      "        l_self_modules_linear2_parameters_weight_ = L_self_modules_linear2_parameters_weight_\n",
      "        l_self_modules_linear2_parameters_bias_ = L_self_modules_linear2_parameters_bias_\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:10 in forward, code: x = self.linear1(x)\n",
      "        x: \"f32[32, 20]\" = torch._C._nn.linear(l_x_, l_self_modules_linear1_parameters_weight_, l_self_modules_linear1_parameters_bias_);  l_x_ = l_self_modules_linear1_parameters_weight_ = l_self_modules_linear1_parameters_bias_ = None\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:11 in forward, code: x = F.relu(x)\n",
      "        x_1: \"f32[32, 20]\" = torch.nn.functional.relu(x);  x = None\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:14 in forward, code: x = self.linear2(x)\n",
      "        x_2: \"f32[32, 5]\" = torch._C._nn.linear(x_1, l_self_modules_linear2_parameters_weight_, l_self_modules_linear2_parameters_bias_);  x_1 = l_self_modules_linear2_parameters_weight_ = l_self_modules_linear2_parameters_bias_ = None\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:15 in forward, code: x = torch.sigmoid(x)\n",
      "        x_3: \"f32[32, 5]\" = torch.sigmoid(x_2);  x_2 = None\n",
      "        return (x_3,)\n",
      "        \n",
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, L_self_modules_linear1_parameters_weight_: \"f32[20, 10]\", L_self_modules_linear1_parameters_bias_: \"f32[20]\", L_x_: \"f32[32, 10]\", L_self_modules_linear2_parameters_weight_: \"f32[5, 20]\", L_self_modules_linear2_parameters_bias_: \"f32[5]\"):\n",
      "        l_self_modules_linear1_parameters_weight_ = L_self_modules_linear1_parameters_weight_\n",
      "        l_self_modules_linear1_parameters_bias_ = L_self_modules_linear1_parameters_bias_\n",
      "        l_x_ = L_x_\n",
      "        l_self_modules_linear2_parameters_weight_ = L_self_modules_linear2_parameters_weight_\n",
      "        l_self_modules_linear2_parameters_bias_ = L_self_modules_linear2_parameters_bias_\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:10 in forward, code: x = self.linear1(x)\n",
      "        x: \"f32[32, 20]\" = torch._C._nn.linear(l_x_, l_self_modules_linear1_parameters_weight_, l_self_modules_linear1_parameters_bias_);  l_x_ = l_self_modules_linear1_parameters_weight_ = l_self_modules_linear1_parameters_bias_ = None\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:11 in forward, code: x = F.relu(x)\n",
      "        x_1: \"f32[32, 20]\" = torch.nn.functional.relu(x);  x = None\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:14 in forward, code: x = self.linear2(x)\n",
      "        x_2: \"f32[32, 5]\" = torch._C._nn.linear(x_1, l_self_modules_linear2_parameters_weight_, l_self_modules_linear2_parameters_bias_);  x_1 = l_self_modules_linear2_parameters_weight_ = l_self_modules_linear2_parameters_bias_ = None\n",
      "        \n",
      "         # File: /tmp/ipykernel_164821/2361058008.py:15 in forward, code: x = torch.sigmoid(x)\n",
      "        x_3: \"f32[32, 5]\" = torch.sigmoid(x_2);  x_2 = None\n",
      "        return (x_3,)\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# Define a simple model with control flow to showcase graph capture\n",
    "class SimpleBranchModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(10, 20)\n",
    "        self.linear2 = nn.Linear(20, 5)\n",
    "        self.linear3 = nn.Linear(20, 5)\n",
    "\n",
    "    def forward(self, x, condition: bool):\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        if condition:\n",
    "            # Path 1: Different computation branch\n",
    "            x = self.linear2(x)\n",
    "            x = torch.sigmoid(x)\n",
    "        else:\n",
    "            # Path 2: Alternative computation branch\n",
    "            x = self.linear3(x)\n",
    "            x = torch.tanh(x)\n",
    "        return x\n",
    "\n",
    "# Create model instance and test inputs\n",
    "model_graph_capture = SimpleBranchModel().to(device)\n",
    "input_tensor_false = torch.randn(32, 10, device=device)\n",
    "input_tensor_true = torch.randn(32, 10, device=device)\n",
    "\n",
    "print(\"‚úÖ SimpleBranchModel and test inputs created successfully\")\n",
    "print(f\"   Model device: {next(model_graph_capture.parameters()).device}\")\n",
    "print(f\"   Input tensor shape: {input_tensor_false.shape}\")\n",
    "\n",
    "# Stage 1: Graph Capture Demonstration\n",
    "# Show how control flow (if/else) specializes the traced FX graph\n",
    "\n",
    "# Explain graph when condition=False\n",
    "explanation_false = torch._dynamo.explain(model_graph_capture)(input_tensor_false, False)\n",
    "print(\"üîç Graph capture (condition=False):\")\n",
    "print(f\"  ‚Ä¢ Ops captured: {explanation_false.op_count}\")\n",
    "print(f\"  ‚Ä¢ Number of graphs: {len(explanation_false.graphs)}\")\n",
    "print(\"  ‚Ä¢ Generated graph:\")\n",
    "print(explanation_false.graphs[0])\n",
    "print(\"\\n  ‚Ä¢ Detailed debug info:\")\n",
    "print(explanation_false.graphs[0].print_readable())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Explain graph when condition=True\n",
    "explanation_true = torch._dynamo.explain(model_graph_capture)(input_tensor_true, True)\n",
    "print(\"üîç Graph capture (condition=True):\")\n",
    "print(f\"  ‚Ä¢ Ops captured: {explanation_true.op_count}\")\n",
    "print(f\"  ‚Ä¢ Number of graphs: {len(explanation_true.graphs)}\")\n",
    "print(\"  ‚Ä¢ Generated graph:\")\n",
    "print(explanation_true.graphs[0])\n",
    "print(\"\\n  ‚Ä¢ Detailed debug info:\")\n",
    "print(explanation_true.graphs[0].print_readable())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd501ef",
   "metadata": {},
   "source": [
    "The `torch._dynamo.explain()` output reveals TorchDynamo's branch specialization mechanism in action. Each boolean condition generates a distinct compilation path with its own optimization opportunities.\n",
    "\n",
    "**Technical Analysis of Graph Specialization**:\n",
    "\n",
    "- **Operation count**: 4 captured operations per branch (linear‚Üírelu‚Üí{linear2|linear3}‚Üí{sigmoid|tanh})\n",
    "- **Graph independence**: Each condition value produces a separate GraphModule with specialized forward() implementations\n",
    "- **Guard insertion**: TorchDynamo inserts runtime checks to ensure the compiled graph remains valid for future executions with the same condition value\n",
    "\n",
    "**Branch-Specific Optimizations**:  \n",
    "When `condition=False`: `linear3` ‚Üí `tanh` operations may fuse into a single kernel if both are pointwise-compatible  \n",
    "When `condition=True`: `linear2` ‚Üí `sigmoid` follows the same fusion analysis but generates different machine code\n",
    "\n",
    "**Runtime Guard Mechanisms**:  \n",
    "- **Constant specialization**: The boolean `condition` becomes a compile-time constant, enabling dead code elimination for the unused branch\n",
    "- **Tensor metadata guards**: Input shape `[32, 10]` and dtype `float32` are verified before using cached kernels\n",
    "- **Module parameter guards**: Model weights and biases are checked for identity to ensure the correct specialized graph\n",
    "\n",
    "**GraphModule Signatures**:  \n",
    "Generated `forward()` methods accept flattened arguments: `(arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1)` representing the model's six parameters (three linear layer weights and biases). Return values are wrapped in single-element tuples for consistency with PyTorch's functional API.\n",
    "\n",
    "This specialization approach enables aggressive optimization by treating dynamic Python control flow as static at the kernel level, producing highly efficient GPU code at the cost of maintaining multiple compiled versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d410b85",
   "metadata": {},
   "source": [
    "## Stage 2: Graph Optimization (Frontend)\n",
    "### Transforming Computational Graphs for Maximum Efficiency\n",
    "\n",
    "**Primary Function**: Pattern-based graph transformations that exploit mathematical properties and hardware characteristics\n",
    "\n",
    "**Concrete Optimization Techniques**:\n",
    "\n",
    "- **Pointwise fusion analysis**: Operations reading the same memory locations (elementwise add, multiply, activation functions) are identified through dataflow analysis and combined into single kernels\n",
    "- **Memory access pattern optimization**: Tensors with compatible strides and memory layouts are restructured to enable vectorized loads/stores\n",
    "- **Arithmetic simplification**: Mathematical identities (`x * 1.0`, `x + 0.0`) are eliminated; associativity rules enable instruction reordering for better parallelization\n",
    "- **Constant propagation**: Values computed from static inputs (model parameters, frozen batch norm statistics) are pre-calculated and embedded directly into generated kernels\n",
    "\n",
    "**Graph-Level Transformations with Measurable Impact**:\n",
    "\n",
    "```python\n",
    "# Before optimization: 4 separate kernel launches\n",
    "x = F.layer_norm(input, normalized_shape)  # Kernel 1: normalize\n",
    "x = F.gelu(x)                              # Kernel 2: activation  \n",
    "x = x * 1.2                                # Kernel 3: multiply\n",
    "x = x + 0.1                                # Kernel 4: add\n",
    "\n",
    "# After optimization: Single fused kernel\n",
    "x = fused_layernorm_gelu_scale_bias(input, weight, bias, 1.2, 0.1)\n",
    "```\n",
    "\n",
    "**Performance Implications**:\n",
    "\n",
    "- **Memory bandwidth reduction**: Fused operations eliminate intermediate tensor writes/reads, reducing DRAM traffic by 60-80% for sequential pointwise operations\n",
    "- **Kernel launch overhead elimination**: Each CUDA kernel launch incurs ~5-15Œºs overhead; fusion reduces this overhead proportionally\n",
    "- **Register pressure optimization**: Intermediate values remain in GPU registers rather than being written to global memory\n",
    "\n",
    "**Optimization Limitations**:\n",
    "\n",
    "Not all operations fuse effectively: matrix multiplications (GEMM) require specialized libraries (cuBLAS); operations with incompatible memory access patterns (transpose, reshape) may prevent fusion; operations requiring synchronization (reductions, cross-GPU communication) create fusion boundaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e69001",
   "metadata": {},
   "source": [
    "## Stage 3: Backend Selection (Transition)\n",
    "### Algorithmic Backend Assignment Through Cost Modeling\n",
    "\n",
    "**Primary Function**: Graph partitioning based on backend capabilities and performance modeling\n",
    "\n",
    "**Partitioning Algorithm**:\n",
    "\n",
    "- **Pattern recognition**: Operations are classified by computational intensity (FLOP/byte ratios) and memory access patterns\n",
    "- **Backend capability matching**: Triton handles pointwise operations and simple reductions; cuBLAS manages matrix multiplications; custom backends process specialized operations\n",
    "- **Cost modeling**: Each partition receives a performance score based on expected memory bandwidth utilization, arithmetic intensity, and backend-specific optimizations\n",
    "\n",
    "**Backend Specialization Matrix**:\n",
    "\n",
    "- **Triton**: Pointwise operations (element-wise arithmetic, activations), reductions (sum, max), memory-bound kernels with regular access patterns\n",
    "- **ATen/cuBLAS**: Dense linear algebra (GEMM, GEMV), operations requiring highly optimized library implementations\n",
    "- **TensorRT**: Convolution-BatchNorm-Activation patterns, mixed-precision inference workflows\n",
    "- **Custom backends**: Domain-specific operations (quantization, sparse operations, custom attention mechanisms)\n",
    "\n",
    "**Performance Trade-offs**: Backend selection balances compilation speed against execution performance‚ÄîTriton generates faster kernels but requires longer compilation; ATen operations launch immediately but may lack fusion opportunities.\n",
    "\n",
    "---\n",
    "\n",
    "## Stage 4: Kernel Generation (Backend)\n",
    "### From Graph Patterns to Parallel GPU Algorithms\n",
    "\n",
    "**Primary Function**: Translation of high-level operations into optimized GPU kernel implementations\n",
    "\n",
    "**Triton Kernel Generation Process**:\n",
    "\n",
    "Triton's code generation transforms operation patterns into parallel algorithms. For pointwise operations, it generates element-wise kernels with configurable thread block dimensions; for reductions, it implements tree-reduction algorithms with shared memory staging; for memory-intensive operations, it optimizes for coalesced global memory access.\n",
    "\n",
    "```python\n",
    "# Generated Triton kernel structure (conceptual)\n",
    "@triton.jit\n",
    "def fused_pointwise_kernel(\n",
    "    input_ptr, output_ptr, n_elements, \n",
    "    scalar_mult: tl.constexpr, scalar_add: tl.constexpr,\n",
    "    BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(0)\n",
    "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "    \n",
    "    # Coalesced memory access\n",
    "    x = tl.load(input_ptr + offsets, mask=mask)\n",
    "    # Fused arithmetic operations\n",
    "    result = tl.math.gelu(x) * scalar_mult + scalar_add\n",
    "    # Write back with same access pattern\n",
    "    tl.store(output_ptr + offsets, result, mask=mask)\n",
    "```\n",
    "\n",
    "**Autotuning Process**: Triton explores configurations systematically‚Äîthread block sizes (32, 64, 128, 256), memory access patterns (vectorized vs. scalar), and shared memory usage‚Äîmeasuring actual performance on target hardware to select optimal parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## Stage 5: Compilation (Backend)\n",
    "### Hardware-Specific Code Generation Through LLVM\n",
    "\n",
    "**Primary Function**: Multi-stage compilation from high-level kernels to GPU machine instructions\n",
    "\n",
    "**Compilation Toolchain Stages**:\n",
    "```\n",
    "Triton IR ‚Üí LLVM IR ‚Üí PTX Assembly ‚Üí SASS Machine Code\n",
    "```\n",
    "\n",
    "**LLVM Optimization Passes**: Standard LLVM optimizations include instruction combining, loop unrolling, and dead code elimination. GPU-specific passes add memory coalescing analysis, shared memory bank conflict resolution, and instruction scheduling to hide memory latency.\n",
    "\n",
    "**PTX to SASS Compilation**: NVIDIA's ptxas compiler applies architecture-specific optimizations‚Äîregister allocation for Ampere's 65,536 registers per SM, instruction scheduling for maximum throughput, and memory subsystem optimization for the specific L1/L2 cache hierarchy.\n",
    "\n",
    "**Architecture Specialization**: Different GPU generations produce different optimized code‚ÄîAmpere enables matrix fragment instructions for tensor operations; Ada Lovelace optimizes for improved shader efficiency; older architectures receive code tuned for their specific limitations.\n",
    "\n",
    "---\n",
    "\n",
    "## Stage 6: Caching & Execution (Runtime)\n",
    "#### Persistent Kernel Storage and Efficient Execution\n",
    "\n",
    "**Primary Function**: Persistent kernel storage with intelligent cache management\n",
    "\n",
    "**Caching Strategy Implementation**:\n",
    "\n",
    "- **Hierarchical cache keys**: Kernels are indexed by operation signature, tensor metadata (shape, dtype, stride), and hardware fingerprint (GPU model, driver version)\n",
    "- **Cache validation**: Hash-based verification ensures cache entries match current compilation parameters and haven't been corrupted\n",
    "- **LRU eviction**: Least-recently-used kernels are removed when cache size exceeds configured limits (typically 1-10 GB)\n",
    "\n",
    "**Cache Hit Performance**: Successful cache lookups bypass compilation entirely, reducing execution overhead to ~1-5Œºs for kernel launch setup compared to 10-1000ms for full compilation.\n",
    "\n",
    "**Production Implications**: Warm caches in production environments deliver consistent performance; cold cache scenarios (container restarts, new deployment) require cache warming strategies to maintain SLA compliance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bdcd6a",
   "metadata": {},
   "source": [
    "# Section 1.3: Hands-On Pipeline Demonstration\n",
    "\n",
    "## Development Environment Setup {#dev-environment}\n",
    "\n",
    "The following configuration enables comprehensive compilation introspection, exposing each pipeline stage through environment variables and debugging APIs. This setup provides the technical foundation for systematic performance analysis and optimization decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac5e59ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß ADVANCED ENVIRONMENT CONFIGURATION\n",
      "=============================================\n",
      "‚úÖ Environment variables configured for deep introspection\n",
      "   ‚Ä¢ TORCH_LOGS: Dynamo tracing enabled\n",
      "   ‚Ä¢ TORCHDYNAMO_VERBOSE: Detailed compilation logging\n",
      "   ‚Ä¢ TORCH_COMPILE_DEBUG: Expert-level debugging\n",
      "\n",
      "üìö Available Debugging Levels:\n",
      "   üìä Basic: Basic compilation tracing\n",
      "      TORCH_LOGS=+dynamo\n",
      "   ‚ö° Performance: Autotuning and cache analysis\n",
      "      TRITON_PRINT_AUTOTUNING=1\n",
      "      TRITON_PRINT_CACHE_STATS=1\n",
      "   üî¨ Expert: Full kernel source visibility\n",
      "      TORCH_LOGS=output_code\n",
      "      TORCH_COMPILE_DEBUG=1\n",
      "\n",
      "üí° Current configuration: Expert level debugging enabled\n"
     ]
    }
   ],
   "source": [
    "# üîß Essential Environment Variables Configuration\n",
    "\n",
    "# Store original settings for restoration\n",
    "original_env = {}\n",
    "env_vars = ['TORCH_LOGS', 'TORCHDYNAMO_VERBOSE', 'TORCH_COMPILE_DEBUG']\n",
    "\n",
    "for var in env_vars:\n",
    "    original_env[var] = os.environ.get(var)\n",
    "\n",
    "# Set up comprehensive debugging environment\n",
    "os.environ['TORCH_LOGS'] = '+dynamo'\n",
    "os.environ['TORCHDYNAMO_VERBOSE'] = '1'  \n",
    "os.environ['TORCH_COMPILE_DEBUG'] = '1'\n",
    "\n",
    "print(\"üîß ADVANCED ENVIRONMENT CONFIGURATION\")\n",
    "print(\"=\" * 45)\n",
    "print(\"‚úÖ Environment variables configured for deep introspection\")\n",
    "print(\"   ‚Ä¢ TORCH_LOGS: Dynamo tracing enabled\")\n",
    "print(\"   ‚Ä¢ TORCHDYNAMO_VERBOSE: Detailed compilation logging\")\n",
    "print(\"   ‚Ä¢ TORCH_COMPILE_DEBUG: Expert-level debugging\")\n",
    "\n",
    "# Key Environment Variables Reference:\n",
    "debugging_levels = {\n",
    "    \"üìä Basic\": {\n",
    "        \"TORCH_LOGS\": \"+dynamo\",\n",
    "        \"purpose\": \"Basic compilation tracing\"\n",
    "    },\n",
    "    \"‚ö° Performance\": {\n",
    "        \"TRITON_PRINT_AUTOTUNING\": \"1\",\n",
    "        \"TRITON_PRINT_CACHE_STATS\": \"1\", \n",
    "        \"purpose\": \"Autotuning and cache analysis\"\n",
    "    },\n",
    "    \"üî¨ Expert\": {\n",
    "        \"TORCH_LOGS\": \"output_code\",\n",
    "        \"TORCH_COMPILE_DEBUG\": \"1\",\n",
    "        \"purpose\": \"Full kernel source visibility\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nüìö Available Debugging Levels:\")\n",
    "for level, config in debugging_levels.items():\n",
    "    print(f\"   {level}: {config['purpose']}\")\n",
    "    for var, value in config.items():\n",
    "        if var != 'purpose':\n",
    "            print(f\"      {var}={value}\")\n",
    "\n",
    "print(f\"\\nüí° Current configuration: Expert level debugging enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc436a36",
   "metadata": {},
   "source": [
    "## A Scientific Approach to Understanding Compilation Performance\n",
    "\n",
    "The following demonstration establishes a rigorous measurement methodology for evaluating PyTorch compilation effectiveness. Rather than showing simple before/after timings, this approach quantifies every aspect of the compilation investment: overhead costs, performance benefits, memory implications, and economic trade-offs.\n",
    "\n",
    "---\n",
    "\n",
    "## Experimental Design Philosophy\n",
    "\n",
    "### **Measurement Protocol Requirements**\n",
    "\n",
    "This demonstration addresses common benchmarking errors that invalidate performance analysis:\n",
    "\n",
    "**Statistical Rigor**: Multiple measurements (n‚â•10) with mean and standard deviation reporting eliminate measurement noise from thermal effects, GPU scheduling variations, and system background processes.\n",
    "\n",
    "**Synchronization Protocol**: `torch.cuda.synchronize()` calls before and after each measurement ensure GPU operations complete before timing, preventing asynchronous execution from corrupting measurements.\n",
    "\n",
    "**Cache State Management**: `torch._dynamo.reset()` clears compilation artifacts between experiments, ensuring reproducible measurements that aren't influenced by previous compilations.\n",
    "\n",
    "---\n",
    "\n",
    "## Experimental Methodology\n",
    "\n",
    "### **Phase 1: Baseline Establishment**\n",
    "**Objective**: Measure unoptimized performance characteristics with proper statistical sampling\n",
    "\n",
    "**Technical Protocol**:\n",
    "- **Warmup sequence**: 3 iterations to eliminate GPU driver initialization overhead and populate GPU caches\n",
    "- **Statistical sampling**: 10 measurements for mean and standard deviation calculation\n",
    "- **Memory profiling**: Peak GPU memory usage tracking through `torch.cuda.max_memory_allocated()`\n",
    "- **Clean state verification**: Ensure identical starting conditions for each measurement\n",
    "\n",
    "### **Phase 2: Compilation Overhead Analysis**  \n",
    "**Objective**: Quantify the true cost of kernel generation and optimization\n",
    "\n",
    "**Measurement Targets**:\n",
    "- **Total compilation time**: From `torch.compile()` invocation to first successful execution\n",
    "- **Memory overhead**: Additional GPU memory consumed by compilation infrastructure and cached kernels\n",
    "- **Compilation consistency**: Variation in compilation time across multiple identical model architectures\n",
    "\n",
    "### **Phase 3: Cached Performance Evaluation**\n",
    "**Objective**: Measure the benefits of optimized kernel execution\n",
    "\n",
    "**Performance Metrics**:\n",
    "- **Execution speedup**: Ratio of baseline to compiled execution time\n",
    "- **Performance consistency**: Standard deviation reduction in execution timing\n",
    "- **Memory efficiency**: Peak memory usage comparison between eager and compiled execution\n",
    "\n",
    "### **Phase 4: Economic Analysis**\n",
    "**Objective**: Calculate return on investment for compilation decisions\n",
    "\n",
    "**Economic Calculations**:\n",
    "```python\n",
    "compilation_overhead = first_run_time - baseline_time\n",
    "time_saved_per_execution = baseline_time - cached_time  \n",
    "break_even_point = compilation_overhead / time_saved_per_execution\n",
    "roi_after_n_runs = (n_runs * time_saved_per_execution - compilation_overhead) / compilation_overhead\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Understanding the Demonstration Code\n",
    "\n",
    "### **Model Selection Strategy**\n",
    "\n",
    "We'll use a model specifically designed to showcase compilation benefits:\n",
    "\n",
    "```python\n",
    "class FusionDemoModel(nn.Module):\n",
    "    \"\"\"Model designed to demonstrate kernel fusion benefits\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(512)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Operations that benefit from fusion\n",
    "        normalized = self.layer_norm(x)     # Normalization\n",
    "        activated = F.gelu(normalized)      # Activation function  \n",
    "        scaled = activated * 1.2 + 0.1     # Arithmetic operations\n",
    "        return scaled\n",
    "```\n",
    "\n",
    "**Why This Model Works Well**:\n",
    "\n",
    "- **Sequential operations**: Create opportunities for kernel fusion\n",
    "- **Memory bandwidth bound**: Fusion reduces memory traffic\n",
    "- **Mixed operation types**: Showcases different optimization strategies\n",
    "- **Realistic complexity**: Represents common deep learning patterns\n",
    "\n",
    "### **Critical PyTorch APIs for Performance Analysis**\n",
    "\n",
    "#### **1. `torch._dynamo.reset()`** \n",
    "```python\n",
    "torch._dynamo.reset()  # Clear compilation cache\n",
    "```\n",
    "\n",
    "**Purpose**: Ensures clean state for reproducible measurements\n",
    "- **When to use**: Before each experimental run\n",
    "- **What it does**: Clears TorchDynamo's internal cache and compilation artifacts\n",
    "- **‚ö†Ô∏è Important**: This is an internal API‚Äîuse only for debugging and education\n",
    "\n",
    "#### **2. `torch.compile()` with Mode Selection** \n",
    "```python\n",
    "compiled_model = torch.compile(model, mode=\"default\")\n",
    "```\n",
    "\n",
    "**Compilation Modes Explained**:\n",
    "\n",
    "- **`\"default\"`**: Balanced optimization (recommended starting point)\n",
    "- **`\"reduce-overhead\"`**: Minimize compilation time (faster compilation, moderate speedup)\n",
    "- **`\"max-autotune\"`**: Maximum performance (longer compilation, maximum speedup)\n",
    "- **`\"max-autotune-no-cudagraphs\"`**: Max optimization without CUDA graphs\n",
    "\n",
    "**Educational Insight**: Mode selection represents a trade-off between compilation time and execution performance.\n",
    "\n",
    "#### **3. `torch.cuda.synchronize()`** \n",
    "```python\n",
    "torch.cuda.synchronize()  # Wait for GPU operations to complete\n",
    "```\n",
    "\n",
    "**Critical for Accurate Timing**:\n",
    "\n",
    "- **Why needed**: GPU operations are asynchronous‚Äîtiming without sync is meaningless\n",
    "- **When to use**: Before and after each timed operation\n",
    "- **Best practice**: Always synchronize when measuring GPU performance\n",
    "\n",
    "### **Statistical Analysis Framework**\n",
    "\n",
    "#### **Timing Best Practices**\n",
    "```python\n",
    "# Proper timing protocol\n",
    "times = []\n",
    "for _ in range(n_measurements):\n",
    "    torch.cuda.synchronize()  # Ensure clean start\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    # Your operation here\n",
    "    output = model(input_tensor)\n",
    "    \n",
    "    torch.cuda.synchronize()  # Ensure completion\n",
    "    times.append(time.perf_counter() - start)\n",
    "\n",
    "average_time = sum(times) / len(times)\n",
    "std_deviation = statistics.stdev(times)\n",
    "```\n",
    "\n",
    "**Why Multiple Measurements Matter**:\n",
    "\n",
    "- **System noise**: Other processes affect timing\n",
    "- **GPU scheduling**: Different kernel launch overhead\n",
    "- **Thermal effects**: GPU performance varies with temperature\n",
    "- **Statistical confidence**: Better estimates with more samples\n",
    "\n",
    "#### **Break-Even Analysis Mathematics**\n",
    "```python\n",
    "# Economic analysis framework\n",
    "compilation_overhead = first_run_time - baseline_time\n",
    "speedup_per_run = baseline_time - cached_time\n",
    "break_even_runs = compilation_overhead / speedup_per_run\n",
    "\n",
    "# ROI calculation over time\n",
    "def calculate_roi(runs_executed):\n",
    "    time_saved = runs_executed * speedup_per_run\n",
    "    net_benefit = time_saved - compilation_overhead\n",
    "    roi_percentage = (net_benefit / compilation_overhead) * 100\n",
    "    return roi_percentage\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn from Running the Demonstration\n",
    "\n",
    "### **Performance Characteristics You'll Observe**\n",
    "\n",
    "1. **Compilation Overhead Pattern**\n",
    "\n",
    "   - First execution: 10-100x slower than baseline\n",
    "   - Overhead dominated by kernel generation and compilation\n",
    "   - Time varies significantly with model complexity\n",
    "\n",
    "2. **Speedup Patterns**\n",
    "\n",
    "   - Cached execution: 1.5-5x faster than baseline (typical range)\n",
    "   - Speedup depends on fusion opportunities and memory patterns\n",
    "   - Consistency improves with compilation (less variance)\n",
    "\n",
    "3. **Economic Trade-offs**\n",
    "\n",
    "   - Break-even: Usually 5-50 executions for neural networks\n",
    "   - ROI improves over time (compounding benefits)\n",
    "   - Different models have different economic profiles\n",
    "\n",
    "\n",
    "\n",
    "**Ready to see the compilation pipeline in action? Let's run our comprehensive analysis! üöÄ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f861fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ COMPREHENSIVE COMPILATION PIPELINE DEMONSTRATION\n",
      "============================================================\n",
      "üî¨ Experimental Setup:\n",
      "   Model: LayerNorm ‚Üí GELU ‚Üí Arithmetic fusion\n",
      "   Input shape: torch.Size([64, 128, 512])\n",
      "   Device: cuda\n",
      "   Expected optimizations: Kernel fusion, memory optimization\n",
      "   Initial GPU memory: 41.2 MB allocated\n",
      "\n",
      "‚öôÔ∏è  Stages 1-3: Graph Capture ‚Üí Optimization ‚Üí Backend Selection\n",
      "-------------------------------------------------------\n",
      "üìè Measuring baseline (eager mode) performance...\n",
      "   ‚úÖ Baseline performance: 20.253 ms\n",
      "   üìä Baseline peak memory: 119.6 MB\n",
      "\n",
      "üî• Stages 4-6: Kernel Generation ‚Üí Compilation ‚Üí Caching\n",
      "-------------------------------------------------------\n",
      "   Watch for Triton kernel generation output below:\n",
      "   ‚úÖ Baseline performance: 20.253 ms\n",
      "   üìä Baseline peak memory: 119.6 MB\n",
      "\n",
      "üî• Stages 4-6: Kernel Generation ‚Üí Compilation ‚Üí Caching\n",
      "-------------------------------------------------------\n",
      "   Watch for Triton kernel generation output below:\n",
      "\n",
      "üìä Compilation Analysis:\n",
      "   ‚úÖ Total compilation time: 331.0 ms\n",
      "   ‚úÖ First execution time: 323.2 ms\n",
      "   üìà Compilation overhead: 16.0x baseline\n",
      "   üóÑÔ∏è  Compilation memory overhead: 16.0 MB\n",
      "   üìä Compilation peak memory: 73.2 MB\n",
      "\n",
      "‚ö° Cached Performance Analysis\n",
      "------------------------------\n",
      "   ‚úÖ Cached performance: 1.226 ms\n",
      "   üöÄ Speedup achieved: 16.52x\n",
      "   üìä Cached peak memory: 89.2 MB\n",
      "   üß† Memory efficiency ratio: 1.34x\n",
      "      ‚úÖ Compiled version uses 25.4% less peak memory\n",
      "\n",
      "üí∞ Economic Analysis:\n",
      "   Time saved per run: 19.027 ms\n",
      "   Break-even point: 17.4 runs\n",
      "   ‚ö° Good ROI - compile for repeated use\n",
      "\n",
      "üß† Memory Overhead Analysis:\n",
      "   Compilation overhead: 16.0 MB\n",
      "   Baseline peak usage: 119.6 MB\n",
      "   Compiled peak usage: 89.2 MB\n",
      "   Memory overhead percentage: 13.4%\n",
      "   ‚ö° Moderate memory overhead - acceptable for most cases\n",
      "\n",
      "üîç Correctness check: Max difference = 1.19e-06\n",
      "   ‚úÖ Excellent numerical accuracy maintained\n",
      "\n",
      "üéì Pipeline Summary:\n",
      "   üì∏ Stage 1-3: Graph capture and optimization (automatic)\n",
      "   üîß Stage 4-6: Kernel generation and caching (331.0 ms)\n",
      "   ‚ö° Result: 16.52x speedup after 17.4 runs\n",
      "   üß† Memory: 1.34x efficiency, 13.4% overhead\n",
      "\n",
      "üéØ Key Takeaways:\n",
      "   ‚Ä¢ torch.compile() is a sophisticated 6-stage pipeline\n",
      "   ‚Ä¢ Compilation overhead is significant but amortizes quickly\n",
      "   ‚Ä¢ Generated kernels are cached for future use\n",
      "   ‚Ä¢ Performance gains depend on model complexity and hardware\n",
      "   ‚Ä¢ Memory efficiency varies - monitor both speed and memory usage\n",
      "   ‚Ä¢ Consider memory overhead in resource-constrained environments\n",
      "\n",
      "üìä Compilation Analysis:\n",
      "   ‚úÖ Total compilation time: 331.0 ms\n",
      "   ‚úÖ First execution time: 323.2 ms\n",
      "   üìà Compilation overhead: 16.0x baseline\n",
      "   üóÑÔ∏è  Compilation memory overhead: 16.0 MB\n",
      "   üìä Compilation peak memory: 73.2 MB\n",
      "\n",
      "‚ö° Cached Performance Analysis\n",
      "------------------------------\n",
      "   ‚úÖ Cached performance: 1.226 ms\n",
      "   üöÄ Speedup achieved: 16.52x\n",
      "   üìä Cached peak memory: 89.2 MB\n",
      "   üß† Memory efficiency ratio: 1.34x\n",
      "      ‚úÖ Compiled version uses 25.4% less peak memory\n",
      "\n",
      "üí∞ Economic Analysis:\n",
      "   Time saved per run: 19.027 ms\n",
      "   Break-even point: 17.4 runs\n",
      "   ‚ö° Good ROI - compile for repeated use\n",
      "\n",
      "üß† Memory Overhead Analysis:\n",
      "   Compilation overhead: 16.0 MB\n",
      "   Baseline peak usage: 119.6 MB\n",
      "   Compiled peak usage: 89.2 MB\n",
      "   Memory overhead percentage: 13.4%\n",
      "   ‚ö° Moderate memory overhead - acceptable for most cases\n",
      "\n",
      "üîç Correctness check: Max difference = 1.19e-06\n",
      "   ‚úÖ Excellent numerical accuracy maintained\n",
      "\n",
      "üéì Pipeline Summary:\n",
      "   üì∏ Stage 1-3: Graph capture and optimization (automatic)\n",
      "   üîß Stage 4-6: Kernel generation and caching (331.0 ms)\n",
      "   ‚ö° Result: 16.52x speedup after 17.4 runs\n",
      "   üß† Memory: 1.34x efficiency, 13.4% overhead\n",
      "\n",
      "üéØ Key Takeaways:\n",
      "   ‚Ä¢ torch.compile() is a sophisticated 6-stage pipeline\n",
      "   ‚Ä¢ Compilation overhead is significant but amortizes quickly\n",
      "   ‚Ä¢ Generated kernels are cached for future use\n",
      "   ‚Ä¢ Performance gains depend on model complexity and hardware\n",
      "   ‚Ä¢ Memory efficiency varies - monitor both speed and memory usage\n",
      "   ‚Ä¢ Consider memory overhead in resource-constrained environments\n"
     ]
    }
   ],
   "source": [
    "# üß™ Comprehensive Compilation Pipeline Demonstration with Memory Analysis\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current GPU memory usage in MB\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return {\n",
    "            'allocated': torch.cuda.memory_allocated() / 1024**2,\n",
    "            'reserved': torch.cuda.memory_reserved() / 1024**2,\n",
    "            'cached': torch.cuda.memory_reserved() / 1024**2  # Using memory_reserved instead of deprecated memory_cached\n",
    "        }\n",
    "    return {'allocated': 0, 'reserved': 0, 'cached': 0}\n",
    "\n",
    "def demonstrate_compilation_phases():\n",
    "    \"\"\"\n",
    "    Educational demonstration of the complete torch.compile() pipeline\n",
    "    Shows all 6 stages with detailed performance and memory analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üß™ COMPREHENSIVE COMPILATION PIPELINE DEMONSTRATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Define a model that will showcase optimization\n",
    "    class FusionDemoModel(nn.Module):\n",
    "        \"\"\"Model designed to demonstrate kernel fusion benefits\"\"\"\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.layer_norm = nn.LayerNorm(512)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            # Operations that benefit from fusion\n",
    "            normalized = self.layer_norm(x)     # Normalization\n",
    "            activated = F.gelu(normalized)      # Activation function\n",
    "            scaled = activated * 1.2 + 0.1     # Arithmetic operations\n",
    "            return scaled\n",
    "    \n",
    "    # Experimental setup\n",
    "    model = FusionDemoModel().to(device)\n",
    "    test_input = torch.randn(64, 128, 512, device=device)\n",
    "    \n",
    "    print(f\"üî¨ Experimental Setup:\")\n",
    "    print(f\"   Model: LayerNorm ‚Üí GELU ‚Üí Arithmetic fusion\")\n",
    "    print(f\"   Input shape: {test_input.shape}\")\n",
    "    print(f\"   Device: {device}\")\n",
    "    print(f\"   Expected optimizations: Kernel fusion, memory optimization\")\n",
    "    \n",
    "    # Initial memory snapshot\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    initial_memory = get_memory_usage()\n",
    "    print(f\"   Initial GPU memory: {initial_memory['allocated']:.1f} MB allocated\")\n",
    "    \n",
    "    # Stage 1-3: Graph Capture and Optimization (happens during first compile call)\n",
    "    print(f\"\\n‚öôÔ∏è  Stages 1-3: Graph Capture ‚Üí Optimization ‚Üí Backend Selection\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    # Clear any previous compilations for clean demonstration\n",
    "    torch._dynamo.reset()\n",
    "    \n",
    "    # Baseline performance measurement\n",
    "    print(\"üìè Measuring baseline (eager mode) performance...\")\n",
    "    model.eval()\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(3):\n",
    "            _ = model(test_input)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Measure baseline performance and memory\n",
    "    baseline_memory_before = get_memory_usage()\n",
    "    baseline_times = []\n",
    "    baseline_peak_memory = []\n",
    "    \n",
    "    for _ in range(10):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            baseline_output = model(test_input)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "            baseline_peak_memory.append(torch.cuda.max_memory_allocated() / 1024**2)\n",
    "        \n",
    "        baseline_times.append(time.perf_counter() - start)\n",
    "    \n",
    "    baseline_avg = sum(baseline_times) / len(baseline_times)\n",
    "    baseline_memory_avg = sum(baseline_peak_memory) / len(baseline_peak_memory) if baseline_peak_memory else 0\n",
    "    \n",
    "    print(f\"   ‚úÖ Baseline performance: {baseline_avg*1000:.3f} ms\")\n",
    "    print(f\"   üìä Baseline peak memory: {baseline_memory_avg:.1f} MB\")\n",
    "    \n",
    "    # Stages 4-6: Kernel Generation, Compilation, and Caching\n",
    "    print(f\"\\nüî• Stages 4-6: Kernel Generation ‚Üí Compilation ‚Üí Caching\")\n",
    "    print(\"-\" * 55)\n",
    "    print(\"   Watch for Triton kernel generation output below:\")\n",
    "    \n",
    "    # Memory before compilation\n",
    "    memory_before_compile = get_memory_usage()\n",
    "    \n",
    "    # This is where the magic happens - all remaining stages occur here\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    compilation_start = time.perf_counter()\n",
    "    compiled_model = torch.compile(model, mode=\"default\")\n",
    "    \n",
    "    # First execution triggers kernel generation and compilation\n",
    "    start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        compiled_output = compiled_model(test_input)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        compilation_peak_memory = torch.cuda.max_memory_allocated() / 1024**2\n",
    "    else:\n",
    "        compilation_peak_memory = 0\n",
    "    \n",
    "    first_run_time = time.perf_counter() - start\n",
    "    total_compilation_time = time.perf_counter() - compilation_start\n",
    "    \n",
    "    # Memory after compilation\n",
    "    memory_after_compile = get_memory_usage()\n",
    "    compilation_memory_overhead = memory_after_compile['allocated'] - memory_before_compile['allocated']\n",
    "    \n",
    "    print(f\"\\nüìä Compilation Analysis:\")\n",
    "    print(f\"   ‚úÖ Total compilation time: {total_compilation_time*1000:.1f} ms\")\n",
    "    print(f\"   ‚úÖ First execution time: {first_run_time*1000:.1f} ms\")\n",
    "    print(f\"   üìà Compilation overhead: {first_run_time/baseline_avg:.1f}x baseline\")\n",
    "    print(f\"   üóÑÔ∏è  Compilation memory overhead: {compilation_memory_overhead:.1f} MB\")\n",
    "    print(f\"   üìä Compilation peak memory: {compilation_peak_memory:.1f} MB\")\n",
    "    \n",
    "    # Test cached performance (Stage 6: Execution from cache)\n",
    "    print(f\"\\n‚ö° Cached Performance Analysis\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    cached_times = []\n",
    "    cached_peak_memory = []\n",
    "    \n",
    "    for _ in range(10):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            _ = compiled_model(test_input)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "            cached_peak_memory.append(torch.cuda.max_memory_allocated() / 1024**2)\n",
    "        \n",
    "        cached_times.append(time.perf_counter() - start)\n",
    "    \n",
    "    cached_avg = sum(cached_times) / len(cached_times)\n",
    "    cached_memory_avg = sum(cached_peak_memory) / len(cached_peak_memory) if cached_peak_memory else 0\n",
    "    speedup = baseline_avg / cached_avg if cached_avg > 0 else 0\n",
    "    \n",
    "    print(f\"   ‚úÖ Cached performance: {cached_avg*1000:.3f} ms\")\n",
    "    print(f\"   üöÄ Speedup achieved: {speedup:.2f}x\")\n",
    "    print(f\"   üìä Cached peak memory: {cached_memory_avg:.1f} MB\")\n",
    "    \n",
    "    # Memory efficiency analysis\n",
    "    memory_efficiency = baseline_memory_avg / cached_memory_avg if cached_memory_avg > 0 else 1\n",
    "    print(f\"   üß† Memory efficiency ratio: {memory_efficiency:.2f}x\")\n",
    "    \n",
    "    if memory_efficiency > 1:\n",
    "        print(f\"      ‚úÖ Compiled version uses {((1 - 1/memory_efficiency) * 100):.1f}% less peak memory\")\n",
    "    elif memory_efficiency < 1:\n",
    "        print(f\"      ‚ö†Ô∏è  Compiled version uses {((1/memory_efficiency - 1) * 100):.1f}% more peak memory\")\n",
    "    else:\n",
    "        print(f\"      ‚û°Ô∏è  Similar memory usage between versions\")\n",
    "    \n",
    "    # Economic analysis\n",
    "    if speedup > 1:\n",
    "        time_saved_per_run = baseline_avg - cached_avg\n",
    "        break_even_runs = total_compilation_time / time_saved_per_run\n",
    "        \n",
    "        print(f\"\\nüí∞ Economic Analysis:\")\n",
    "        print(f\"   Time saved per run: {time_saved_per_run*1000:.3f} ms\")\n",
    "        print(f\"   Break-even point: {break_even_runs:.1f} runs\")\n",
    "        \n",
    "        if break_even_runs < 10:\n",
    "            print(f\"   ‚úÖ Excellent ROI - compile immediately\")\n",
    "        elif break_even_runs < 50:\n",
    "            print(f\"   ‚ö° Good ROI - compile for repeated use\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  High break-even - evaluate use case\")\n",
    "    \n",
    "    # Memory overhead analysis\n",
    "    print(f\"\\nüß† Memory Overhead Analysis:\")\n",
    "    print(f\"   Compilation overhead: {compilation_memory_overhead:.1f} MB\")\n",
    "    print(f\"   Baseline peak usage: {baseline_memory_avg:.1f} MB\")\n",
    "    print(f\"   Compiled peak usage: {cached_memory_avg:.1f} MB\")\n",
    "    \n",
    "    overhead_percentage = (compilation_memory_overhead / baseline_memory_avg) * 100 if baseline_memory_avg > 0 else 0\n",
    "    print(f\"   Memory overhead percentage: {overhead_percentage:.1f}%\")\n",
    "    \n",
    "    if overhead_percentage < 10:\n",
    "        print(f\"   ‚úÖ Low memory overhead - negligible impact\")\n",
    "    elif overhead_percentage < 25:\n",
    "        print(f\"   ‚ö° Moderate memory overhead - acceptable for most cases\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  High memory overhead - consider memory constraints\")\n",
    "    \n",
    "    # Correctness verification\n",
    "    max_diff = (baseline_output - compiled_output).abs().max().item()\n",
    "    print(f\"\\nüîç Correctness check: Max difference = {max_diff:.2e}\")\n",
    "    if max_diff < 1e-5:\n",
    "        print(f\"   ‚úÖ Excellent numerical accuracy maintained\")\n",
    "    \n",
    "    print(f\"\\nüéì Pipeline Summary:\")\n",
    "    print(f\"   üì∏ Stage 1-3: Graph capture and optimization (automatic)\")\n",
    "    print(f\"   üîß Stage 4-6: Kernel generation and caching ({total_compilation_time*1000:.1f} ms)\")\n",
    "    print(f\"   ‚ö° Result: {speedup:.2f}x speedup after {break_even_runs:.1f} runs\")\n",
    "    print(f\"   üß† Memory: {memory_efficiency:.2f}x efficiency, {overhead_percentage:.1f}% overhead\")\n",
    "    \n",
    "    return {\n",
    "        'baseline_ms': baseline_avg * 1000,\n",
    "        'compiled_ms': cached_avg * 1000,\n",
    "        'compilation_ms': total_compilation_time * 1000,\n",
    "        'speedup': speedup,\n",
    "        'break_even': break_even_runs if speedup > 1 else float('inf'),\n",
    "        'baseline_memory_mb': baseline_memory_avg,\n",
    "        'compiled_memory_mb': cached_memory_avg,\n",
    "        'memory_overhead_mb': compilation_memory_overhead,\n",
    "        'memory_efficiency': memory_efficiency,\n",
    "        'memory_overhead_percent': overhead_percentage\n",
    "    }\n",
    "\n",
    "# Execute the comprehensive demonstration\n",
    "compilation_results = demonstrate_compilation_phases()\n",
    "\n",
    "print(f\"\\nüéØ Key Takeaways:\")\n",
    "print(f\"   ‚Ä¢ torch.compile() is a sophisticated 6-stage pipeline\")\n",
    "print(f\"   ‚Ä¢ Compilation overhead is significant but amortizes quickly\") \n",
    "print(f\"   ‚Ä¢ Generated kernels are cached for future use\")\n",
    "print(f\"   ‚Ä¢ Performance gains depend on model complexity and hardware\")\n",
    "print(f\"   ‚Ä¢ Memory efficiency varies - monitor both speed and memory usage\")\n",
    "print(f\"   ‚Ä¢ Consider memory overhead in resource-constrained environments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67de08e",
   "metadata": {},
   "source": [
    "## üß† Deep Dive: Memory Analysis in torch.compile()\n",
    "\n",
    "### **Understanding Memory Overhead and Efficiency**\n",
    "\n",
    "The enhanced demonstration reveals critical insights about GPU memory utilization patterns during compilation. These metrics directly impact production deployment decisions, especially in memory-constrained environments.\n",
    "\n",
    "#### **Memory Metric Categories**\n",
    "\n",
    "**1. Compilation Memory Overhead (16.0 MB in our example)**\n",
    "- **Kernel cache storage**: Compiled SASS binaries persist in GPU memory for immediate execution\n",
    "- **TorchDynamo metadata**: Graph representations, optimization passes, and execution planning data structures\n",
    "- **Backend infrastructure**: Triton compiler temporaries, LLVM intermediate representations\n",
    "\n",
    "**2. Peak Memory Usage Analysis**\n",
    "- **Baseline execution**: 118.5 MB peak memory from eager mode tensor allocations and intermediate results\n",
    "- **Compiled execution**: 88.1 MB peak memory from optimized kernel execution with reduced intermediate storage\n",
    "- **Memory efficiency ratio**: 1.34x improvement indicates 25.6% reduction in peak memory requirements\n",
    "\n",
    "**3. Memory Efficiency Mechanisms**\n",
    "- **Operator fusion elimination**: Fused kernels eliminate intermediate tensor allocations between operations\n",
    "- **Memory layout optimization**: Optimized stride patterns and memory access reduce fragmentation\n",
    "- **Temporary reduction**: In-place operations and shared intermediate buffers minimize memory footprint\n",
    "\n",
    "#### **Production Memory Planning**\n",
    "\n",
    "**Memory Budget Calculation**:\n",
    "```python\n",
    "# Example production memory planning\n",
    "model_base_memory = 118.5    # MB baseline peak usage\n",
    "compilation_overhead = 16.0   # MB persistent compilation data\n",
    "safety_margin = 1.25         # 25% safety buffer for memory spikes\n",
    "\n",
    "total_memory_requirement = (model_base_memory + compilation_overhead) * safety_margin\n",
    "# Result: 168.1 MB planned allocation per model instance\n",
    "```\n",
    "\n",
    "**Deployment Considerations**:\n",
    "- **Multi-model serving**: Compilation overhead scales linearly with model count (16MB √ó N models)\n",
    "- **Memory pressure scenarios**: High compilation overhead (>20% of baseline) may require compilation mode adjustment\n",
    "- **Container resource planning**: Include compilation overhead in container memory limits to prevent OOM failures\n",
    "\n",
    "**Memory Efficiency Patterns**:\n",
    "- **Batch size scaling**: Memory efficiency improvements compound with larger batch sizes due to improved arithmetic intensity\n",
    "- **Architecture dependencies**: Models with sequential operations (transformers, RNNs) show higher memory efficiency gains than architectures with complex skip connections\n",
    "- **Precision effects**: Mixed-precision models (FP16/FP32) may show different memory efficiency patterns due to precision-specific optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb89308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéØ COMPREHENSIVE COMPILATION ANALYSIS SUMMARY\n",
      "============================================================\n",
      "\n",
      "‚ö° PERFORMANCE METRICS:\n",
      "   Baseline execution time:     20.253 ms\n",
      "   Compiled execution time:     1.226 ms\n",
      "   Compilation overhead:        331.0 ms\n",
      "   Speedup achieved:            16.52x\n",
      "   Break-even point:            17.4 runs\n",
      "\n",
      "üß† MEMORY METRICS:\n",
      "   Baseline peak memory:        119.6 MB\n",
      "   Compiled peak memory:        89.2 MB\n",
      "   Memory overhead:             16.0 MB\n",
      "   Memory efficiency ratio:     1.34x\n",
      "   Memory overhead percentage:  13.4%\n",
      "\n",
      "üí∞ ECONOMIC ANALYSIS:\n",
      "   Time saved per run:          19.027 ms\n",
      "   Total cost (compilation):    331.0 ms\n",
      "   Benefit after 100 runs:      1902.7 ms\n",
      "   Net benefit (100 runs):      1571.7 ms\n",
      "\n",
      "üéØ RECOMMENDATIONS:\n",
      "   ‚úÖ EXCELLENT - Compile immediately for production use\n",
      "   üß† MEMORY: Excellent memory efficiency gained\n",
      "\n",
      "============================================================\n",
      "\n",
      "üéì CONGRATULATIONS!\n",
      "You now have comprehensive memory and performance analysis capabilities!\n",
      "üìä The notebook measures:\n",
      "   ‚Ä¢ Execution time (baseline vs compiled)\n",
      "   ‚Ä¢ Memory overhead (compilation cost)\n",
      "   ‚Ä¢ Memory efficiency (peak usage comparison)\n",
      "   ‚Ä¢ Economic analysis (break-even calculations)\n",
      "   ‚Ä¢ Practical recommendations for production use\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Results Summary\n",
    "\n",
    "def display_compilation_summary(results: dict):\n",
    "    \"\"\"\n",
    "    Display a comprehensive summary of compilation results including memory analysis\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéØ COMPREHENSIVE COMPILATION ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Performance Metrics\n",
    "    print(\"\\n‚ö° PERFORMANCE METRICS:\")\n",
    "    print(f\"   Baseline execution time:     {results['baseline_ms']:.3f} ms\")\n",
    "    print(f\"   Compiled execution time:     {results['compiled_ms']:.3f} ms\")\n",
    "    print(f\"   Compilation overhead:        {results['compilation_ms']:.1f} ms\")\n",
    "    print(f\"   Speedup achieved:            {results['speedup']:.2f}x\")\n",
    "    print(f\"   Break-even point:            {results['break_even']:.1f} runs\")\n",
    "    \n",
    "    # Memory Metrics\n",
    "    print(\"\\nüß† MEMORY METRICS:\")\n",
    "    print(f\"   Baseline peak memory:        {results['baseline_memory_mb']:.1f} MB\")\n",
    "    print(f\"   Compiled peak memory:        {results['compiled_memory_mb']:.1f} MB\")\n",
    "    print(f\"   Memory overhead:             {results['memory_overhead_mb']:.1f} MB\")\n",
    "    print(f\"   Memory efficiency ratio:     {results['memory_efficiency']:.2f}x\")\n",
    "    print(f\"   Memory overhead percentage:  {results['memory_overhead_percent']:.1f}%\")\n",
    "    \n",
    "    # Economic Analysis\n",
    "    print(\"\\nüí∞ ECONOMIC ANALYSIS:\")\n",
    "    time_saved_per_run = results['baseline_ms'] - results['compiled_ms']\n",
    "    total_benefit_100_runs = time_saved_per_run * 100\n",
    "    total_cost = results['compilation_ms']\n",
    "    net_benefit_100_runs = total_benefit_100_runs - total_cost\n",
    "    \n",
    "    print(f\"   Time saved per run:          {time_saved_per_run:.3f} ms\")\n",
    "    print(f\"   Total cost (compilation):    {total_cost:.1f} ms\")\n",
    "    print(f\"   Benefit after 100 runs:      {total_benefit_100_runs:.1f} ms\")\n",
    "    print(f\"   Net benefit (100 runs):      {net_benefit_100_runs:.1f} ms\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\nüéØ RECOMMENDATIONS:\")\n",
    "    if results['speedup'] > 5 and results['break_even'] < 50:\n",
    "        print(\"   ‚úÖ EXCELLENT - Compile immediately for production use\")\n",
    "    elif results['speedup'] > 2 and results['break_even'] < 100:\n",
    "        print(\"   ‚ö° GOOD - Compile for repeated execution scenarios\")\n",
    "    elif results['speedup'] > 1 and results['break_even'] < 500:\n",
    "        print(\"   ‚ö†Ô∏è  MODERATE - Evaluate based on specific use case\")\n",
    "    else:\n",
    "        print(\"   ‚ùå POOR - Consider alternative optimization strategies\")\n",
    "        \n",
    "    if results['memory_efficiency'] > 1.2:\n",
    "        print(\"   üß† MEMORY: Excellent memory efficiency gained\")\n",
    "    elif results['memory_efficiency'] > 1.0:\n",
    "        print(\"   üß† MEMORY: Modest memory efficiency improvement\")\n",
    "    elif results['memory_overhead_percent'] < 20:\n",
    "        print(\"   üß† MEMORY: Acceptable memory overhead\")\n",
    "    else:\n",
    "        print(\"   üß† MEMORY: High memory overhead - monitor carefully\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Display comprehensive summary of our compilation results\n",
    "display_compilation_summary(compilation_results)\n",
    "\n",
    "print(\"\\nüéì CONGRATULATIONS!\")\n",
    "print(\"You now have comprehensive memory and performance analysis capabilities!\")\n",
    "print(\"üìä The notebook measures:\")\n",
    "print(\"   ‚Ä¢ Execution time (baseline vs compiled)\")\n",
    "print(\"   ‚Ä¢ Memory overhead (compilation cost)\")\n",
    "print(\"   ‚Ä¢ Memory efficiency (peak usage comparison)\")\n",
    "print(\"   ‚Ä¢ Economic analysis (break-even calculations)\")\n",
    "print(\"   ‚Ä¢ Practical recommendations for production use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8d4d49",
   "metadata": {},
   "source": [
    "### **Production-Ready Decision Framework**\n",
    "\n",
    "**When to Apply Compilation (Data-Driven Criteria)**:\n",
    "\n",
    "1. **Batch processing workflows**: Execute identical models ‚â•50 times with consistent input shapes\n",
    "2. **Inference serving**: Models serving >100 requests/hour with stable traffic patterns  \n",
    "3. **Training with fixed architectures**: Post-hyperparameter tuning when model structure is finalized\n",
    "4. **Memory-constrained deployments**: Models where 15-30% memory reduction enables larger batch sizes or multi-model serving\n",
    "\n",
    "**When to Avoid Compilation (Risk Mitigation)**:\n",
    "\n",
    "1. **Rapid prototyping phases**: Model architectures changing multiple times per day\n",
    "2. **Single-shot execution**: One-time analysis, testing, or debugging scenarios\n",
    "3. **Complex dynamic control flow**: Models with heavy use of Python conditionals, loops with variable iteration counts\n",
    "4. **Development/debugging**: When Python-level stack traces and variable inspection are required\n",
    "\n",
    "**Optimization Strategy Implementation**:\n",
    "\n",
    "```python\n",
    "# Production compilation decision framework\n",
    "def should_compile_model(execution_count_estimate, model_complexity_score, memory_constraints):\n",
    "    if execution_count_estimate < 20:\n",
    "        return False, \"Insufficient execution count for ROI\"\n",
    "    \n",
    "    if model_complexity_score < 0.3:  # Simple models\n",
    "        return False, \"Limited optimization opportunities\"\n",
    "        \n",
    "    if memory_constraints and compilation_overhead_percent > 25:\n",
    "        return False, \"Memory overhead exceeds constraint threshold\"\n",
    "        \n",
    "    return True, \"Compilation recommended\"\n",
    "```\n",
    "\n",
    "## Critical Dependencies and Limitations\n",
    "\n",
    "### **Environment Dependencies That Affect Results**\n",
    "\n",
    "- **GPU Architecture**: Ampere (RTX 30xx/A100) vs. Ada Lovelace (RTX 40xx) show different optimization patterns due to architectural differences in SM count, memory bandwidth, and instruction sets\n",
    "- **Driver Version**: CUDA driver updates (11.8 vs. 12.0+) affect PTX‚ÜíSASS compilation and may invalidate kernel caches\n",
    "- **PyTorch Version**: Compilation behavior evolves rapidly‚ÄîPyTorch 2.0 vs. 2.1 vs. 2.2 contain different optimization passes and backend improvements\n",
    "- **System Memory Pressure**: Low system RAM affects compilation performance due to LLVM memory requirements during kernel generation\n",
    "\n",
    "### **Model Architecture Effects on Compilation Benefits**\n",
    "\n",
    "- **Pointwise-heavy models**: Architectures with many sequential activations, normalizations, and elementwise operations (Vision Transformers, MobileNets) show 3-8x speedups\n",
    "- **Compute-intensive models**: GEMM-dominated architectures (large language models, dense networks) show 1.5-3x speedups due to limited fusion opportunities  \n",
    "- **Mixed operation patterns**: Models combining convolutions, attention, and pointwise operations show variable speedups depending on operation distribution\n",
    "\n",
    "### **Production Deployment Considerations**\n",
    "\n",
    "- **Cache warming strategies**: Plan for 10-1000ms compilation overhead during container startup or model loading\n",
    "- **Version compatibility**: Compiled kernels are tied to specific PyTorch versions, model architectures, and hardware configurations\n",
    "- **Memory monitoring**: Implement alerting for compilation memory overhead exceeding expected thresholds\n",
    "- **Rollback procedures**: Maintain ability to disable compilation quickly if performance regressions occur in production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f083741",
   "metadata": {},
   "source": [
    "# Summary: PyTorch Compilation Mastery\n",
    "\n",
    "## Chapter 1 Completion: From Theory to Production-Ready Skills\n",
    "\n",
    "You have successfully mastered the foundational elements of PyTorch's compilation system, developing both theoretical understanding and practical analysis capabilities essential for production optimization work.\n",
    "\n",
    "---\n",
    "\n",
    "## Technical Competencies Acquired\n",
    "\n",
    "### **Pipeline Architecture Mastery**\n",
    "\n",
    "You now understand PyTorch compilation as a deterministic six-stage transformation process:\n",
    "\n",
    "1. **TorchDynamo Graph Capture** ‚Üí Bytecode analysis creating FX Graphs with runtime guards\n",
    "2. **Frontend Optimization** ‚Üí Pattern-based fusion, dead code elimination, memory planning\n",
    "3. **Backend Selection** ‚Üí Cost-model-driven partitioning across Triton, ATen, and specialized backends\n",
    "4. **Triton Kernel Generation** ‚Üí Parallel algorithm creation with autotuned block configurations\n",
    "5. **LLVM Compilation** ‚Üí Hardware-specific optimization through PTX‚ÜíSASS transformation\n",
    "6. **Persistent Caching** ‚Üí Disk-based kernel storage with hierarchical cache keys\n",
    "\n",
    "**Performance Investment Model**: Compilation operates as a high-fixed-cost, low-marginal-cost optimization strategy with measurable ROI calculations and break-even analysis.\n",
    "\n",
    "### **Quantitative Analysis Framework**\n",
    "\n",
    "#### **Measurement Methodology**\n",
    "You've implemented a comprehensive benchmarking protocol:\n",
    "\n",
    "```python\n",
    "# Your systematic analysis approach\n",
    "baseline_performance = measure_with_statistical_sampling(eager_model, n_trials=10)\n",
    "compilation_overhead = measure_first_execution_cost(compiled_model) \n",
    "cached_performance = measure_optimized_execution(compiled_model, n_trials=10)\n",
    "break_even_point = compilation_overhead / (baseline_performance - cached_performance)\n",
    "memory_efficiency = baseline_peak_memory / compiled_peak_memory\n",
    "```\n",
    "\n",
    "#### **Economic Decision Framework**\n",
    "You can now calculate compilation ROI with precision:\n",
    "\n",
    "- **Break-even analysis**: Determining execution count thresholds for positive ROI\n",
    "- **Memory overhead planning**: Quantifying persistent memory costs for deployment planning\n",
    "- **Performance consistency evaluation**: Measuring variance reduction in execution timing\n",
    "\n",
    "### **Production Deployment Expertise**\n",
    "\n",
    "#### **Evidence-Based Decision Making**\n",
    "**Compile When**: >50 execution count, stable architecture, pointwise-heavy operations, memory constraints benefiting from efficiency gains\n",
    "\n",
    "**Avoid Compilation When**: Rapid prototyping, single-shot execution, complex dynamic control flow, debugging requirements\n",
    "\n",
    "#### **Risk Management**\n",
    "- **Cache warming strategies**: Planning for compilation overhead during production deployment\n",
    "- **Memory budget allocation**: Including 15-25% overhead for compilation infrastructure\n",
    "- **Version compatibility**: Understanding kernel cache invalidation across PyTorch versions\n",
    "- **Performance monitoring**: Tracking compilation effectiveness metrics in production\n",
    "\n",
    "---\n",
    "\n",
    "## Advanced Technical Insights\n",
    "\n",
    "### **Hardware-Specific Optimization Patterns**\n",
    "\n",
    "**GPU Architecture Dependencies**:\n",
    "- **Ampere (A100, RTX 30xx)**: Excels at tensor operations with matrix fragment instructions\n",
    "- **Ada Lovelace (RTX 40xx)**: Optimized shader efficiency benefits pointwise operation fusion\n",
    "- **Memory Bandwidth Scaling**: Compilation benefits scale with GPU memory bandwidth (HBM2 vs. GDDR6X)\n",
    "\n",
    "**Model Architecture Optimization Profiles**:\n",
    "- **Sequential Operations** (normalization‚Üíactivation‚Üíarithmetic): 5-15x speedup potential\n",
    "- **GEMM-Dominated** (large linear layers): 1.5-3x speedup limited by cuBLAS optimization\n",
    "- **Mixed Patterns** (attention + pointwise): Variable speedup depending on operation distribution\n",
    "\n",
    "### **Production Engineering Best Practices**\n",
    "\n",
    "#### **Systematic Optimization Workflow**\n",
    "1. **Baseline Establishment**: Measure eager mode performance with proper statistical methods\n",
    "2. **Compilation Analysis**: Quantify overhead, benefits, and memory implications\n",
    "3. **Economic Evaluation**: Calculate break-even points and ROI projections\n",
    "4. **Production Planning**: Design cache warming and memory allocation strategies\n",
    "5. **Monitoring Implementation**: Track compilation effectiveness and regression detection\n",
    "\n",
    "#### **Professional Development Integration**\n",
    "- **Code Review Standards**: Include compilation analysis in performance optimization reviews\n",
    "- **Documentation Requirements**: Record compilation decisions with quantitative justification\n",
    "- **Team Knowledge Sharing**: Establish compilation best practices and measurement standards\n",
    "\n",
    "---\n",
    "\n",
    "# Advanced Learning Pathway\n",
    "\n",
    "## **Immediate Application Challenge**\n",
    "\n",
    "Apply your newfound expertise to a real optimization scenario:\n",
    "\n",
    "**Select one of your existing PyTorch models** and conduct a complete compilation analysis:\n",
    "\n",
    "1. **Environment Setup**: Configure debugging and measurement infrastructure\n",
    "2. **Baseline Analysis**: Implement statistical measurement protocol\n",
    "3. **Compilation Evaluation**: Measure all overhead and benefit metrics\n",
    "4. **Economic Assessment**: Calculate ROI and break-even analysis\n",
    "5. **Production Planning**: Design deployment strategy with memory and performance considerations\n",
    "6. **Decision Documentation**: Write technical justification for compilation decision\n",
    "\n",
    "**Success Criteria**: Produce a data-driven recommendation with quantitative evidence supporting your compilation strategy.\n",
    "\n",
    "## **Advanced Topics Preview**\n",
    "\n",
    "### **Chapter 2: Expert Debugging & Kernel Optimization**\n",
    "**Advanced capabilities you'll master**:\n",
    "- **Triton kernel introspection**: Reading and optimizing generated GPU code\n",
    "- **Performance regression debugging**: Systematic analysis of compilation failures\n",
    "- **Custom backend development**: Creating specialized optimization passes\n",
    "- **Advanced autotuning**: Configuring kernel parameters for maximum performance\n",
    "\n",
    "### **Chapter 3: Enterprise Production Deployment**\n",
    "**Production expertise you'll develop**:\n",
    "- **Scalable compilation strategies**: Multi-model deployment with shared kernel caches\n",
    "- **Performance monitoring systems**: Real-time compilation effectiveness tracking\n",
    "- **Deployment automation**: CI/CD integration with compilation validation\n",
    "- **Expert troubleshooting**: Diagnosing and resolving production compilation issues\n",
    "\n",
    "**Your next challenge awaits**: Advance to Chapter 2 to master the expert-level debugging and optimization techniques that distinguish performance engineering specialists from casual users. The foundation you've built here will support sophisticated optimization work that directly impacts production system performance and reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeb73b7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-qat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
