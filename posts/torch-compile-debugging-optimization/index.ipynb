{
 "cells": [
  {
   "cell_type": "raw",
   "id": "12b4769c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"PyTorch Compile: Debugging & Optimization Techniques (Part 2)\"\n",
    "format: \n",
    "  html:\n",
    "    code-fold: true\n",
    "jupyter: python3\n",
    "page-layout: full\n",
    "categories: [pytorch, torch-compile, debugging, optimization, triton-kernels]\n",
    "description: \"Master advanced debugging techniques, Triton kernel analysis, and systematic optimization strategies for PyTorch torch.compile(). Expert-level insights into compilation internals.\"\n",
    "image: \"https://example.com/pytorch-debugging.png\"\n",
    "author: \"Innovation Crucible\"\n",
    "date: \"2025-06-16\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6449f952",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to Part 2 of our comprehensive torch.compile() series! Building on the fundamentals from Part 1, we now dive deep into advanced debugging techniques, Triton kernel analysis, and systematic optimization strategies.\n",
    "\n",
    "## **What You'll Master in Part 2**\n",
    "\n",
    "### üõ†Ô∏è **Chapter 2: Advanced Debugging & Optimization**\n",
    "1. **[Advanced Debugging Toolkit](#debugging-toolkit)** - Environment variables and introspection tools\n",
    "2. **[Triton Kernel Exploration](#kernel-exploration)** - Examining and understanding generated kernels\n",
    "3. **[Performance Benchmarking](#performance-benchmarking)** - Systematic optimization analysis\n",
    "\n",
    "---\n",
    "\n",
    "## **Advanced Learning Outcomes**\n",
    "\n",
    "Upon completing Part 2, you will master:\n",
    "\n",
    "### **Expert-Level Skills**\n",
    "- **Advanced Debugging**: Expert-level troubleshooting using environment variables\n",
    "- **Kernel Understanding**: Ability to read and analyze generated Triton GPU kernels\n",
    "- **Performance Engineering**: Systematic approaches to measuring and optimizing performance\n",
    "- **Optimization Strategies**: Know when and how to apply compilation for maximum benefit\n",
    "\n",
    "---\n",
    "\n",
    "## üîß **Prerequisites**\n",
    "\n",
    "Before proceeding, ensure you've completed **Part 1: Compilation Fundamentals** and understand:\n",
    "\n",
    "- ‚úÖ The 6-stage compilation pipeline\n",
    "- ‚úÖ Basic performance analysis techniques\n",
    "- ‚úÖ Environment variable configuration\n",
    "- ‚úÖ Break-even analysis concepts\n",
    "\n",
    "Let's begin with advanced debugging techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2699e5",
   "metadata": {},
   "source": [
    "## üöÄ Setting Up Your Learning Environment\n",
    "\n",
    "Before we dive into the advanced concepts, we need to set up a proper learning environment that will allow us to observe and understand the torch.compile() process in detail.\n",
    "\n",
    "### What This Cell Does:\n",
    "- **Checks your PyTorch installation** and ensures CUDA/GPU availability\n",
    "- **Verifies Triton availability** for GPU kernel optimization\n",
    "- **Configures environment variables** to make the compilation process visible\n",
    "- **Sets up educational debugging** so you can see what happens under the hood\n",
    "\n",
    "### Key Environment Variables We'll Use:\n",
    "- `TORCH_LOGS=output_code`: Shows the actual generated Triton kernel source code\n",
    "- `TRITON_PRINT_AUTOTUNING=1`: Displays the autotuning process that optimizes kernel parameters\n",
    "- `TRITON_PRINT_CACHE_STATS=1`: Shows kernel caching statistics for understanding reuse patterns\n",
    "\n",
    "This setup is crucial for learning because it transforms the usually invisible compilation process into something you can observe and understand step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a35a5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ PyTorch + Triton Learning Environment Setup\n",
      "==================================================\n",
      "üì¶ PyTorch version: 2.7.1+cu126\n",
      "‚úÖ CUDA GPU available: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "   Memory: 6.0 GB\n",
      "   Compute capability: (8, 9)\n",
      "‚úÖ Triton available: 3.3.1\n",
      "\n",
      "üéØ Selected device: CUDA\n",
      "\n",
      "üî¨ Configuring Educational Environment Variables\n",
      "   These variables will help us see what happens during compilation:\n",
      "   ‚úÖ TORCH_LOGS = 'output_code'\n",
      "   ‚úÖ TRITON_PRINT_AUTOTUNING = '1'\n",
      "   ‚úÖ TRITON_PRINT_CACHE_STATS = '1'\n",
      "\n",
      "üí° What these reveal:\n",
      "   ‚Ä¢ output_code: Shows actual generated Triton kernel source code\n",
      "   ‚Ä¢ autotuning: Displays optimization decisions being made\n",
      "   ‚Ä¢ cache_stats: Shows when kernels are reused vs regenerated\n",
      "\n",
      "‚úÖ Environment ready for learning!\n",
      "   We'll now be able to see the internals of PyTorch compilation\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Environment Setup and Foundation\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import glob\n",
    "import warnings\n",
    "import torch._dynamo.config as config\n",
    "\n",
    "print(\"üöÄ PyTorch + Triton Learning Environment Setup\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Step 1: Check PyTorch and device availability\n",
    "print(f\"üì¶ PyTorch version: {torch.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"‚úÖ CUDA GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"   Compute capability: {torch.cuda.get_device_capability(0)}\")\n",
    "    \n",
    "    # Check Triton availability\n",
    "    try:\n",
    "        import triton\n",
    "        print(f\"‚úÖ Triton available: {triton.__version__}\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ö†Ô∏è  Triton not available - install with: pip install triton\")\n",
    "        \n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"‚ö†Ô∏è  CUDA not available - using CPU\")\n",
    "    print(\"   Note: Many optimizations are GPU-specific\")\n",
    "\n",
    "print(f\"\\nüéØ Selected device: {device.upper()}\")\n",
    "\n",
    "# Step 2: Configure environment for educational exploration\n",
    "def setup_educational_environment():\n",
    "    \"\"\"Configure environment variables to see what PyTorch compilation does\"\"\"\n",
    "    \n",
    "    print(f\"\\nüî¨ Configuring Educational Environment Variables\")\n",
    "    print(\"   These variables will help us see what happens during compilation:\")\n",
    "    \n",
    "    educational_config = {\n",
    "        # Show generated kernel code - the actual Triton kernels\n",
    "        \"TORCH_LOGS\": \"output_code\",\n",
    "        \n",
    "        # Display autotuning process - see optimization decisions\n",
    "        \"TRITON_PRINT_AUTOTUNING\": \"1\", \n",
    "        \n",
    "        # Show cache statistics - understand kernel reuse\n",
    "        \"TRITON_PRINT_CACHE_STATS\": \"1\",\n",
    "    }\n",
    "    \n",
    "    for key, value in educational_config.items():\n",
    "        os.environ[key] = value\n",
    "        print(f\"   ‚úÖ {key} = '{value}'\")\n",
    "    \n",
    "    print(f\"\\nüí° What these reveal:\")\n",
    "    print(f\"   ‚Ä¢ output_code: Shows actual generated Triton kernel source code\")\n",
    "    print(f\"   ‚Ä¢ autotuning: Displays optimization decisions being made\")  \n",
    "    print(f\"   ‚Ä¢ cache_stats: Shows when kernels are reused vs regenerated\")\n",
    "    \n",
    "    return educational_config\n",
    "\n",
    "# Apply educational configuration\n",
    "settings = setup_educational_environment()\n",
    "\n",
    "print(f\"\\n‚úÖ Environment ready for learning!\")\n",
    "print(f\"   We'll now be able to see the internals of PyTorch compilation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5753e20",
   "metadata": {},
   "source": [
    "# Debugging Toolkit: Jupyter-Focused PyTorch Debugging {#debugging-toolkit}\n",
    "\n",
    "\n",
    "## Debugging Toolkit: Environment Variables & Introspection\n",
    "\n",
    "Environment variables are your primary tools for understanding torch.compile() internals. They provide unprecedented visibility into the compilation process, from graph capture to kernel generation.\n",
    "\n",
    "### üîç Essential Environment Variables for Advanced Users\n",
    "\n",
    "| Variable | Purpose | Insight Level | When to Use |\n",
    "|----------|---------|---------------|-------------|\n",
    "| `TORCH_LOGS=output_code` | Shows generated Triton kernel source | **Expert** | Understanding optimizations |\n",
    "| `TRITON_PRINT_AUTOTUNING=1` | Displays autotuning decisions | **Advanced** | Performance debugging |\n",
    "| `TRITON_PRINT_CACHE_STATS=1` | Cache hit/miss statistics | **Intermediate** | Cache optimization |\n",
    "| `TORCH_COMPILE_DEBUG=1` | Comprehensive compilation tracing | **Expert** | Deep debugging |\n",
    "| `TORCHINDUCTOR_VERBOSE=1` | Backend compilation details | **Advanced** | Backend debugging |\n",
    "\n",
    "### Advanced Debugging Strategies\n",
    "\n",
    "#### **Level 1: Basic Monitoring** \n",
    "- Monitor compilation success/failure\n",
    "- Track basic performance metrics\n",
    "- Verify kernel caching behavior\n",
    "\n",
    "#### **Level 2: Performance Analysis** \n",
    "- Analyze autotuning decisions\n",
    "- Compare kernel variants\n",
    "- Measure cache effectiveness\n",
    "\n",
    "#### **Level 3: Expert Introspection** \n",
    "- Examine generated kernel source code\n",
    "- Understand memory access patterns\n",
    "- Debug numerical accuracy issues\n",
    "\n",
    "#### **Level 4: Production Monitoring** \n",
    "- Real-time performance tracking\n",
    "- Automated regression detection\n",
    "- Deployment health monitoring\n",
    "\n",
    "Let's explore these debugging levels with practical demonstrations:\n",
    "\n",
    "## The Critical Jupyter Debugging Problem\n",
    "\n",
    "**Before we explore debugging techniques, we must address a fundamental issue**: **PyTorch debugging logs don't appear in Jupyter notebooks!** This affects every PyTorch developer using notebooks and significantly impacts the debugging experience.\n",
    "\n",
    "### üîç **Why PyTorch Logs Disappear in Jupyter**\n",
    "\n",
    "```python\n",
    "# This works perfectly in terminal:\n",
    "os.environ['TORCH_LOGS'] = 'dynamo'\n",
    "compiled_model = torch.compile(model)\n",
    "result = compiled_model(input)  # Shows extensive logs in terminal\n",
    "\n",
    "# This same code in Jupyter:  \n",
    "# ‚ùå No logs visible - even though compilation happens!\n",
    "```\n",
    "\n",
    "**Root Cause Analysis:**\n",
    "\n",
    "1. **PyTorch's Internal Logging**: Written in C++ and goes directly to system `stderr`\n",
    "2. **Jupyter's Output Capture**: Only captures Python `print()` statements and exceptions\n",
    "3. **Output Mismatch**: System `stderr` bypasses Jupyter's output capture mechanism\n",
    "4. **Environment Variables Work**: They configure PyTorch correctly, but output is lost\n",
    "\n",
    "### **Complete Solutions for Jupyter Debugging**\n",
    "\n",
    "We'll explore **three proven approaches** that work reliably in Jupyter notebooks:\n",
    "\n",
    "| Method | Best For | Jupyter Friendly | Detail Level |\n",
    "|--------|----------|------------------|--------------|\n",
    "| **1. Subprocess Capture** | Seeing actual PyTorch logs | ‚úÖ Yes | üî• Maximum |\n",
    "| **2. `torch._dynamo.explain()`** | Graph analysis | ‚úÖ Yes | üìä High |\n",
    "| **3. Artifact Inspection** | Generated kernels | ‚úÖ Yes | üî¨ Deep |\n",
    "\n",
    "### **What You'll Learn**\n",
    "\n",
    "This section will teach you to become a **Jupyter debugging expert** by mastering:\n",
    "\n",
    "- **Problem-Aware Debugging**: Understanding why standard approaches fail\n",
    "- **Jupyter-Native Solutions**: Techniques that work reliably in notebooks\n",
    "- **Hybrid Approaches**: Combining external capture with notebook analysis\n",
    "- **Production-Ready Methods**: Debugging techniques that scale to real projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fffd3c1",
   "metadata": {},
   "source": [
    "##  Debugging Solutions Overview\n",
    "\n",
    "### **Method 1: Subprocess Capture**  *For Complete Logging*\n",
    "\n",
    "**When to use**: When you need to see the actual PyTorch debug logs that would appear in terminal.\n",
    "\n",
    "```python\n",
    "# Capture PyTorch logs that Jupyter normally misses\n",
    "result = subprocess.run(['python', 'debug_script.py'], \n",
    "                       env={'TORCH_LOGS': 'dynamo'}, \n",
    "                       capture_output=True, text=True)\n",
    "print(result.stderr)  # Shows actual PyTorch logs!\n",
    "```\n",
    "\n",
    "**Pros**: \n",
    "- ‚úÖ Shows real PyTorch compilation logs\n",
    "- ‚úÖ Complete environment variable support\n",
    "- ‚úÖ Identical to terminal debugging experience\n",
    "\n",
    "**Cons**: \n",
    "- ‚ö†Ô∏è Requires external script creation\n",
    "- ‚ö†Ô∏è More complex setup\n",
    "\n",
    "---\n",
    "\n",
    "### **Method 2: Dynamo Analysis** *Best for Daily Debugging*\n",
    "\n",
    "**When to use**: For analyzing what gets compiled vs. what causes graph breaks.\n",
    "\n",
    "```python\n",
    "# This ALWAYS works in Jupyter\n",
    "explanation = torch._dynamo.explain(model)(input)\n",
    "print(f\"Graphs: {explanation.graph_count}\")\n",
    "print(f\"Breaks: {explanation.graph_break_count}\")\n",
    "```\n",
    "\n",
    "**Pros**:\n",
    "- ‚úÖ Native Jupyter support\n",
    "- ‚úÖ Structured output\n",
    "- ‚úÖ Perfect for graph analysis\n",
    "- ‚úÖ Fast and reliable\n",
    "\n",
    "**Cons**:\n",
    "- ‚ö†Ô∏è Limited to graph-level insights\n",
    "- ‚ö†Ô∏è No kernel generation details\n",
    "\n",
    "---\n",
    "\n",
    "### **Method 3: Artifact Inspection** *Best for Deep Understanding*\n",
    "\n",
    "**When to use**: To examine generated Triton kernels and understand optimizations.\n",
    "\n",
    "```python\n",
    "# Explore generated kernels\n",
    "kernel_files = glob.glob('/tmp/torchinductor_*/**/*.py')\n",
    "with open(kernel_files[0]) as f:\n",
    "    print(f.read())  # See actual generated Triton code!\n",
    "```\n",
    "\n",
    "**Pros**:\n",
    "- ‚úÖ Deep understanding of optimizations\n",
    "- ‚úÖ Educational value\n",
    "- ‚úÖ Real kernel source code\n",
    "- ‚úÖ Shows actual compilation results\n",
    "\n",
    "**Cons**:\n",
    "- ‚ö†Ô∏è Requires file system navigation\n",
    "- ‚ö†Ô∏è Platform-dependent paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a6c3cf",
   "metadata": {},
   "source": [
    "## Demonstrating the Problem & Solutions\n",
    "\n",
    "Let's start with a hands-on demonstration that shows **exactly why** standard debugging approaches fail in Jupyter and **how our solutions work**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e7059fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® DEMONSTRATING THE JUPYTER LOGGING PROBLEM\n",
      "=======================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Test Case: Simple fusion model (ReLU ‚Üí Multiply ‚Üí Add ‚Üí Tanh)\n",
      "   Input shape: torch.Size([100])\n",
      "   Device: cuda\n",
      "\n",
      "‚ùå FAILED APPROACH: Environment Variables in Jupyter\n",
      "--------------------------------------------------\n",
      "Environment variables set:\n",
      "   TORCH_LOGS = 'dynamo'\n",
      "   TORCH_COMPILE_DEBUG = '1'\n",
      "\n",
      "Compiling model with debug environment...\n",
      "‚úÖ Compilation completed in 6430.9 ms\n",
      "üìä Result shape: torch.Size([100])\n",
      "üîç Expected: Extensive PyTorch debug logs\n",
      "üí• Reality: No debug logs visible in Jupyter!\n",
      "\n",
      "üéì Key Insight:\n",
      "   ‚Ä¢ Environment variables ARE working (compilation happened)\n",
      "   ‚Ä¢ PyTorch IS generating logs (just not visible)\n",
      "   ‚Ä¢ Jupyter captures Python prints, not system stderr\n",
      "   ‚Ä¢ We need alternative approaches for notebook debugging\n",
      "‚úÖ Compilation completed in 6430.9 ms\n",
      "üìä Result shape: torch.Size([100])\n",
      "üîç Expected: Extensive PyTorch debug logs\n",
      "üí• Reality: No debug logs visible in Jupyter!\n",
      "\n",
      "üéì Key Insight:\n",
      "   ‚Ä¢ Environment variables ARE working (compilation happened)\n",
      "   ‚Ä¢ PyTorch IS generating logs (just not visible)\n",
      "   ‚Ä¢ Jupyter captures Python prints, not system stderr\n",
      "   ‚Ä¢ We need alternative approaches for notebook debugging\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "import tempfile\n",
    "\n",
    "def demonstrate_jupyter_logging_problem():\n",
    "    \"\"\"\n",
    "    Demonstrate the fundamental issue: PyTorch logs work in terminal but not Jupyter\n",
    "    \"\"\"\n",
    "    print(\"üö® DEMONSTRATING THE JUPYTER LOGGING PROBLEM\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Create a simple model that should generate logs\n",
    "    def simple_fusion_model(x):\n",
    "        \"\"\"Model designed to trigger compilation and logging\"\"\"\n",
    "        return torch.tanh(torch.relu(x) * 2.0 + 1.0)\n",
    "    \n",
    "    test_input = torch.randn(100, device=device)\n",
    "    \n",
    "    print(\"üéØ Test Case: Simple fusion model (ReLU ‚Üí Multiply ‚Üí Add ‚Üí Tanh)\")\n",
    "    print(f\"   Input shape: {test_input.shape}\")\n",
    "    print(f\"   Device: {device}\")\n",
    "    print()\n",
    "    \n",
    "    # Step 1: Try the \"standard\" approach that fails in Jupyter\n",
    "    print(\"‚ùå FAILED APPROACH: Environment Variables in Jupyter\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Set environment variables that should show logs\n",
    "    os.environ['TORCH_LOGS'] = 'dynamo'\n",
    "    os.environ['TORCH_COMPILE_DEBUG'] = '1'\n",
    "    \n",
    "    print(\"Environment variables set:\")\n",
    "    print(f\"   TORCH_LOGS = '{os.environ.get('TORCH_LOGS')}'\")\n",
    "    print(f\"   TORCH_COMPILE_DEBUG = '{os.environ.get('TORCH_COMPILE_DEBUG')}'\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Compiling model with debug environment...\")\n",
    "    torch._dynamo.reset()  # Clear cache\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    compiled_model = torch.compile(simple_fusion_model)\n",
    "    result = compiled_model(test_input)\n",
    "    compilation_time = time.perf_counter() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ Compilation completed in {compilation_time*1000:.1f} ms\")\n",
    "    print(f\"üìä Result shape: {result.shape}\")\n",
    "    print(\"üîç Expected: Extensive PyTorch debug logs\")\n",
    "    print(\"üí• Reality: No debug logs visible in Jupyter!\")\n",
    "    print()\n",
    "    \n",
    "    # Clean up environment variables\n",
    "    os.environ.pop('TORCH_LOGS', None)\n",
    "    os.environ.pop('TORCH_COMPILE_DEBUG', None)\n",
    "    \n",
    "    print(\"üéì Key Insight:\")\n",
    "    print(\"   ‚Ä¢ Environment variables ARE working (compilation happened)\")\n",
    "    print(\"   ‚Ä¢ PyTorch IS generating logs (just not visible)\")\n",
    "    print(\"   ‚Ä¢ Jupyter captures Python prints, not system stderr\")\n",
    "    print(\"   ‚Ä¢ We need alternative approaches for notebook debugging\")\n",
    "    \n",
    "    return compilation_time, result.shape\n",
    "\n",
    "# Execute the demonstration\n",
    "problem_demo_time, result_shape = demonstrate_jupyter_logging_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70360103",
   "metadata": {},
   "source": [
    "### ‚úÖ Working Solutions for Jupyter Debugging\n",
    "\n",
    "Now that we've seen the problem, let's explore the **two proven solutions** that actually work in Jupyter notebooks. Each solution targets different debugging needs and provides reliable insights into PyTorch's compilation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9596fb",
   "metadata": {},
   "source": [
    "## Solution 1: Subprocess Capture Method\n",
    "\n",
    "**Objective**: Capture the actual PyTorch debug logs that would appear in terminal.\n",
    "\n",
    "**When to use**: \n",
    "- Learning what PyTorch compilation actually does\n",
    "- Debugging complex compilation issues  \n",
    "- Seeing environment variable effects\n",
    "- Educational exploration of internals\n",
    "\n",
    "**How it works**: Run PyTorch code in an external Python process and capture all output (stdout + stderr) back into the Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a18d9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß The Jupyter vs Terminal Logging Problem\n",
      "==================================================\n",
      "\\n==================== Scenario 1: Minimal (No Debug) ====================\n",
      "üìù Standard execution without debug output\n",
      "No environment variables set\n",
      "\\nRunning external Python process...\n",
      "----------------------------------------\n",
      "üì§ STDOUT:\n",
      "   üéØ PyTorch Logging Test (External Process)\n",
      "Environment variables active:\n",
      "  TORCH_LOGS: Not Set\n",
      "  TORCH_COMPILE_DEBUG: Not Set\n",
      "  TRITON_PRINT_AUTOTUNING: 1\n",
      "\n",
      "Starting compilation...\n",
      "Compilation completed. Result shape: torch.Size([100])\n",
      "Any logs above this line came from PyTorch!\n",
      "\\nüìä Results:\n",
      "   Return code: 0\n",
      "   Debug lines captured: 0\n",
      "   ‚ÑπÔ∏è  No debug output (expected for minimal scenario)\n",
      "\\n==================== Scenario 2: Basic Dynamo Logging ====================\n",
      "üìù Shows graph capture and compilation decisions\n",
      "Environment variables set:\n",
      "  TORCH_LOGS=+dynamo\n",
      "\\nRunning external Python process...\n",
      "----------------------------------------\n",
      "üì§ STDOUT:\n",
      "   üéØ PyTorch Logging Test (External Process)\n",
      "Environment variables active:\n",
      "  TORCH_LOGS: Not Set\n",
      "  TORCH_COMPILE_DEBUG: Not Set\n",
      "  TRITON_PRINT_AUTOTUNING: 1\n",
      "\n",
      "Starting compilation...\n",
      "Compilation completed. Result shape: torch.Size([100])\n",
      "Any logs above this line came from PyTorch!\n",
      "\\nüìä Results:\n",
      "   Return code: 0\n",
      "   Debug lines captured: 0\n",
      "   ‚ÑπÔ∏è  No debug output (expected for minimal scenario)\n",
      "\\n==================== Scenario 2: Basic Dynamo Logging ====================\n",
      "üìù Shows graph capture and compilation decisions\n",
      "Environment variables set:\n",
      "  TORCH_LOGS=+dynamo\n",
      "\\nRunning external Python process...\n",
      "----------------------------------------\n",
      "üì§ STDOUT:\n",
      "   üéØ PyTorch Logging Test (External Process)\n",
      "Environment variables active:\n",
      "  TORCH_LOGS: +dynamo\n",
      "  TORCH_COMPILE_DEBUG: Not Set\n",
      "  TRITON_PRINT_AUTOTUNING: 1\n",
      "\n",
      "Starting compilation...\n",
      "Compilation completed. Result shape: torch.Size([100])\n",
      "Any logs above this line came from PyTorch!\n",
      "\\nüì• STDERR (PyTorch Debug Logs):\n",
      "   I0617 12:55:25.955000 261031 site-packages/torch/_dynamo/__init__.py:112] torch._dynamo.reset\n",
      "I0617 12:55:25.956000 261031 site-packages/torch/_dynamo/__init__.py:145] torch._dynamo.reset_code_caches\n",
      "I0617 12:55:27.791000 261031 site-packages/torch/_dynamo/utils.py:1603] [0/0] ChromiumEventLogger initialized with id 18898188-071a-4015-8d39-a6af818667c7\n",
      "V0617 12:55:27.792000 261031 site-packages/torch/_dynamo/convert_frame.py:1003] [0/0] torchdynamo start compiling simple_model /tmp/tmp83ltn8a4.py:5, stack (elided 5 frames):\n",
      "V0617 12:55:27.792000 261031 site-packages/torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/tmp/tmp83ltn8a4.py\", line 31, in <module>\n",
      "V0617 12:55:27.792000 261031 site-packages/torch/_dynamo/convert_frame.py:1003] [0/0]     main()\n",
      "V0617 12:55:27.792000 261031 site-packages/torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/tmp/tmp83ltn8a4.py\", line 25, in main\n",
      "V0617 12:55:27.792000 261031 site-packages/torch/_dynamo/convert_frame.py:1003] [0/0]     result = compiled_model(test_input)\n",
      "V0617 12:55:27.792000 261031 site-packages/torch/_dynamo/convert_frame.py:1003] [0/0] \n",
      "I0617 12:55:27.793000 261031 site-packages/torch/_dynamo/symbolic_convert.py:3322] [0/0] Step 1: torchdynamo start tracing simple_model /tmp/tmp83ltn8a4.py:5\n",
      "I0617 12:55:27.794000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:3334] [0/0] create_env\n",
      "V0617 12:55:27.798000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1216] [0/0] [__trace_source] TRACE starts_line /tmp/tmp83ltn8a4.py:5 in simple_model (simple_model)\n",
      "V0617 12:55:27.798000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1216] [0/0] [__trace_source]     def simple_model(x):\n",
      "V0617 12:55:27.799000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE RESUME 0 []\n",
      "V0617 12:55:27.800000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1216] [0/0] [__trace_source] TRACE starts_line /tmp/tmp83ltn8a4.py:6 in simple_model (simple_model)\n",
      "V0617 12:55:27.800000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1216] [0/0] [__trace_source]         return torch.relu(x * 2.0) + 1.0\n",
      "V0617 12:55:27.800000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE LOAD_GLOBAL torch []\n",
      "V0617 12:55:27.802000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE LOAD_ATTR relu [NullVariable, PythonModuleVariable(<module 'torch' from '/home/alibina/anaconda3/envs/pytorch-qat/lib/python3.12/site-packages/torch/__init__.py'>)]\n",
      "V0617 12:55:27.803000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE LOAD_FAST x [NullVariable, TorchInGraphFunctionVariable(<built-in method relu of type object at 0x7f29558f6fa0>, nonstrict_traceable=False)]\n",
      "V0617 12:55:27.803000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE LOAD_CONST 2.0 [NullVariable, TorchInGraphFunctionVariable(<built-in method relu of type object at 0x7f29558f6fa0>, nonstrict_traceable=False), LazyVariableTracker()]\n",
      "V0617 12:55:27.803000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE BINARY_OP 5 [NullVariable, TorchInGraphFunctionVariable(<built-in method relu of type object at 0x7f29558f6fa0>, nonstrict_traceable=False), LazyVariableTracker(), ConstantVariable(float: 2.0)]\n",
      "V0617 12:55:27.804000 261031 site-packages/torch/_dynamo/variables/builder.py:3025] [0/0] wrap_to_fake L['x'] (100,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\n",
      "V0617 12:55:27.806000 261031 site-packages/torch/_dynamo/output_graph.py:2271] [0/0] create_graph_input L_x_ L['x'] FakeTensor(..., device='cuda:0', size=(100,)) at debug_level 0 before=False\n",
      "V0617 12:55:27.809000 261031 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call] TRACE FX call mul from /tmp/tmp83ltn8a4.py:6 in simple_model (simple_model)\n",
      "V0617 12:55:27.809000 261031 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call]     return torch.relu(x * 2.0) + 1.0\n",
      "V0617 12:55:27.809000 261031 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call]                       ~~^~~~~\n",
      "V0617 12:55:27.811000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE CALL 1 [NullVariable, TorchInGraphFunctionVariable(<built-in method relu of type object at 0x7f29558f6fa0>, nonstrict_traceable=False), TensorVariable()]\n",
      "V0617 12:55:27.822000 261031 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call] TRACE FX call relu from /tmp/tmp83ltn8a4.py:6 in simple_model (simple_model)\n",
      "V0617 12:55:27.822000 261031 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call]     return torch.relu(x * 2.0) + 1.0\n",
      "V0617 12:55:27.822000 261031 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call]            ~~~~~~~~~~^^^^^^^^^\n",
      "V0617 12:55:27.827000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE LOAD_CONST 1.0 [TensorVariable()]\n",
      "V0617 12:55:27.828000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE BINARY_OP 0 [TensorVariable(), ConstantVariable(float: 1.0)]\n",
      "V0617 12:55:27.828000 261031 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call] TRACE FX call add from /tmp/tmp83ltn8a4.py:6 in simple_model (simple_model)\n",
      "V0617 12:55:27.828000 261031 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call]     return torch.relu(x * 2.0) + 1.0\n",
      "V0617 12:55:27.828000 261031 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call]            ~~~~~~~~~~~~~~~~~~~~^~~~~\n",
      "V0617 12:55:27.830000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE RETURN_VALUE None [TensorVariable()]\n",
      "I0617 12:55:27.830000 261031 site-packages/torch/_dynamo/symbolic_convert.py:3679] [0/0] Step 1: torchdynamo done tracing simple_model (RETURN_VALUE)\n",
      "V0617 12:55:27.831000 261031 site-packages/torch/_dynamo/symbolic_convert.py:3683] [0/0] RETURN_VALUE triggered compile\n",
      "V0617 12:55:27.831000 261031 site-packages/torch/_dynamo/output_graph.py:1008] [0/0] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/tmp83ltn8a4.py, line 6 in simple_model>], graph_break=False)\n",
      "V0617 12:55:27.836000 261031 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code] TRACED GRAPH\n",
      "V0617 12:55:27.836000 261031 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]  ===== __compiled_fn_1 =====\n",
      "V0617 12:55:27.836000 261031 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]  /home/alibina/anaconda3/envs/pytorch-qat/lib/python3.12/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V0617 12:55:27.836000 261031 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]     def forward(self, L_x_: \"f32[100][1]cuda:0\"):\n",
      "V0617 12:55:27.836000 261031 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         l_x_ = L_x_\n",
      "V0617 12:55:27.836000 261031 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         \n",
      "V0617 12:55:27.836000 261031 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]          # File: /tmp/tmp83ltn8a4.py:6 in simple_model, code: return torch.relu(x * 2.0) + 1.0\n",
      "V0617 12:55:27.836000 261031 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         mul: \"f32[100][1]cuda:0\" = l_x_ * 2.0;  l_x_ = None\n",
      "V0617 12:55:27.836000 261031 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         relu: \"f32[100][1]cuda:0\" = torch.relu(mul);  mul = None\n",
      "V0617 12:55:27.836000 261031 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         add: \"f32[100][1]cuda:0\" = relu + 1.0;  relu = None\n",
      "V0617 12:55:27.836000 261031 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         return (add,)\n",
      "V0617 12:55:27.836000 261031 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         \n",
      "V0617 12:55:27.836000 261031 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code] \n",
      "I0617 12:55:27.837000 261031 site-packages/torch/_dynamo/output_graph.py:1515] [0/0] Step 2: calling compiler function inductor\n",
      "I0617 12:55:28.592000 261031 site-packages/torch/_dynamo/output_graph.py:1520] [0/0] Step 2: done compiler function inductor\n",
      "I0617 12:55:28.603000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:4734] [0/0] produce_guards\n",
      "V0617 12:55:28.605000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['x'].size()[0] 100 None\n",
      "V0617 12:55:28.607000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['x'].stride()[0] 1 None\n",
      "V0617 12:55:28.608000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['x'].storage_offset() 0 None\n",
      "V0617 12:55:28.611000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:5156] [0/0] Skipping guard L['x'].size()[0] == 100\n",
      "V0617 12:55:28.612000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:5156] [0/0] Skipping guard L['x'].stride()[0] == 1\n",
      "V0617 12:55:28.614000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:5156] [0/0] Skipping guard L['x'].storage_offset() == 0\n",
      "V0617 12:55:28.615000 261031 site-packages/torch/_dynamo/guards.py:2566] [0/0] [__guards] GUARDS:\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] \n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] TREE_GUARD_MANAGER:\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] +- RootGuardManager\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:520 in init_ambient_guards\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | +- GLOBAL_STATE: ___check_global_state()\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | +- GuardManager: source=L['x'], accessed_by=FrameLocalsGuardAccessor(key='x', framelocals_idx=0)\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[100], stride=[1])  # return torch.relu(x * 2.0) + 1.0  # mp/tmp83ltn8a4.py:6 in simple_model\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # return torch.relu(x * 2.0) + 1.0  # mp/tmp83ltn8a4.py:6 in simple_model\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor('torch')\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['torch'], 139815642715840)                  # return torch.relu(x * 2.0) + 1.0  # mp/tmp83ltn8a4.py:6 in simple_model\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | | | +- GuardManager: source=G['torch'].relu, accessed_by=GetAttrGuardAccessor(relu)\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].relu, 139815636995264)             # return torch.relu(x * 2.0) + 1.0  # mp/tmp83ltn8a4.py:6 in simple_model\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] \n",
      "V0617 12:55:28.620000 261031 site-packages/torch/_dynamo/guards.py:2533] [0/0] [__guards] Guard eval latency = 1.14 us\n",
      "I0617 12:55:28.622000 261031 site-packages/torch/_dynamo/pgo.py:660] [0/0] put_code_state: no cache key, skipping\n",
      "I0617 12:55:28.623000 261031 site-packages/torch/_dynamo/convert_frame.py:1121] [0/0] run_gc_after_compile: running gc\n",
      "V0617 12:55:28.631000 261031 site-packages/torch/_dynamo/convert_frame.py:1395] skipping: _fn (reason: in skipfiles, file: /home/alibina/anaconda3/envs/pytorch-qat/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py)\n",
      "I0617 12:55:31.261000 261047 site-packages/torch/_dynamo/eval_frame.py:475] TorchDynamo attempted to trace the following frames: [\n",
      "I0617 12:55:31.261000 261047 site-packages/torch/_dynamo/eval_frame.py:475] \n",
      "I0617 12:55:31.261000 261047 site-packages/torch/_dynamo/eval_frame.py:475] ]\n",
      "I0617 12:55:31.264000 261047 site-packages/torch/_dynamo/utils.py:765] TorchDynamo compilation metrics:\n",
      "I0617 12:55:31.264000 261047 site-packages/torch/_dynamo/utils.py:765] Function, Runtimes (s)\n",
      "V0617 12:55:31.267000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats constrain_symbol_range: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:31.268000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats defer_runtime_assert: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:31.269000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats evaluate_expr: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:31.270000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _simplify_floor_div: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:31.270000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _maybe_guard_rel: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:31.271000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _find: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:31.272000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats has_hint: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:31.273000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats size_hint: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:31.274000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats simplify: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:31.276000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _update_divisible: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:31.277000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats replace: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:31.277000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _maybe_evaluate_static: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:31.278000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats get_implications: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:31.278000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats get_axioms: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:31.278000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _maybe_evaluate_static_worker: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:31.279000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats safe_expand: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:31.279000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats uninteresting_files: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "I0617 12:55:32.490000 261031 site-packages/torch/_dynamo/eval_frame.py:475] TorchDynamo attempted to trace the following frames: [\n",
      "I0617 12:55:32.490000 261031 site-packages/torch/_dynamo/eval_frame.py:475]   * simple_model /tmp/tmp83ltn8a4.py:5\n",
      "I0617 12:55:32.490000 261031 site-packages/torch/_dynamo/eval_frame.py:475] ]\n",
      "I0617 12:55:32.493000 261031 site-packages/torch/_dynamo/utils.py:765] TorchDynamo compilation metrics:\n",
      "I0617 12:55:32.493000 261031 site-packages/torch/_dynamo/utils.py:765] Function, Runtimes (s)\n",
      "I0617 12:55:32.493000 261031 site-packages/torch/_dynamo/utils.py:765] _compile.compile_inner, 0.8292\n",
      "I0617 12:55:32.493000 261031 site-packages/torch/_dynamo/utils.py:765] OutputGraph.call_user_compiler, 0.7575\n",
      "I0617 12:55:32.493000 261031 site-packages/torch/_dynamo/utils.py:765] _recursive_pre_grad_passes, 0.0070\n",
      "I0617 12:55:32.493000 261031 site-packages/torch/_dynamo/utils.py:765] inductor_codecache_torch_key, 0.2066\n",
      "I0617 12:55:32.493000 261031 site-packages/torch/_dynamo/utils.py:765] TritonBundler.read_and_emit, 0.0002\n",
      "I0617 12:55:32.493000 261031 site-packages/torch/_dynamo/utils.py:765] PyCodeCache.load_by_key_path, 0.0938\n",
      "I0617 12:55:32.493000 261031 site-packages/torch/_dynamo/utils.py:765] async_compile.precompile, 0.0841\n",
      "I0617 12:55:32.493000 261031 site-packages/torch/_dynamo/utils.py:765] async_compile.wait, 0.0005\n",
      "I0617 12:55:32.493000 261031 site-packages/torch/_dynamo/utils.py:765] gc, 0.0036\n",
      "V0617 12:55:32.494000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats constrain_symbol_range: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:32.495000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats defer_runtime_assert: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:32.496000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats evaluate_expr: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:32.497000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _simplify_floor_div: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:32.498000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _maybe_guard_rel: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:32.499000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _find: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:32.500000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats has_hint: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:32.501000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats size_hint: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:32.502000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats simplify: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:32.503000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _update_divisible: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:32.504000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats replace: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:32.505000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _maybe_evaluate_static: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:32.506000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats get_implications: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:32.507000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats get_axioms: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:32.508000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _maybe_evaluate_static_worker: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:32.510000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats safe_expand: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:32.511000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats uninteresting_files: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "\\nüìä Results:\n",
      "   Return code: 0\n",
      "   Debug lines captured: 1\n",
      "   üéâ SUCCESS: Captured PyTorch debug output!\n",
      "\\n==================== Scenario 3: Comprehensive Debug ====================\n",
      "üìù Full debugging with file generation\n",
      "Environment variables set:\n",
      "  TORCH_LOGS=+dynamo,+inductor\n",
      "  TORCH_COMPILE_DEBUG=1\n",
      "\\nRunning external Python process...\n",
      "----------------------------------------\n",
      "üì§ STDOUT:\n",
      "   üéØ PyTorch Logging Test (External Process)\n",
      "Environment variables active:\n",
      "  TORCH_LOGS: +dynamo\n",
      "  TORCH_COMPILE_DEBUG: Not Set\n",
      "  TRITON_PRINT_AUTOTUNING: 1\n",
      "\n",
      "Starting compilation...\n",
      "Compilation completed. Result shape: torch.Size([100])\n",
      "Any logs above this line came from PyTorch!\n",
      "\\nüì• STDERR (PyTorch Debug Logs):\n",
      "   I0617 12:55:25.955000 261031 site-packages/torch/_dynamo/__init__.py:112] torch._dynamo.reset\n",
      "I0617 12:55:25.956000 261031 site-packages/torch/_dynamo/__init__.py:145] torch._dynamo.reset_code_caches\n",
      "I0617 12:55:27.791000 261031 site-packages/torch/_dynamo/utils.py:1603] [0/0] ChromiumEventLogger initialized with id 18898188-071a-4015-8d39-a6af818667c7\n",
      "V0617 12:55:27.792000 261031 site-packages/torch/_dynamo/convert_frame.py:1003] [0/0] torchdynamo start compiling simple_model /tmp/tmp83ltn8a4.py:5, stack (elided 5 frames):\n",
      "V0617 12:55:27.792000 261031 site-packages/torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/tmp/tmp83ltn8a4.py\", line 31, in <module>\n",
      "V0617 12:55:27.792000 261031 site-packages/torch/_dynamo/convert_frame.py:1003] [0/0]     main()\n",
      "V0617 12:55:27.792000 261031 site-packages/torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/tmp/tmp83ltn8a4.py\", line 25, in main\n",
      "V0617 12:55:27.792000 261031 site-packages/torch/_dynamo/convert_frame.py:1003] [0/0]     result = compiled_model(test_input)\n",
      "V0617 12:55:27.792000 261031 site-packages/torch/_dynamo/convert_frame.py:1003] [0/0] \n",
      "I0617 12:55:27.793000 261031 site-packages/torch/_dynamo/symbolic_convert.py:3322] [0/0] Step 1: torchdynamo start tracing simple_model /tmp/tmp83ltn8a4.py:5\n",
      "I0617 12:55:27.794000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:3334] [0/0] create_env\n",
      "V0617 12:55:27.798000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1216] [0/0] [__trace_source] TRACE starts_line /tmp/tmp83ltn8a4.py:5 in simple_model (simple_model)\n",
      "V0617 12:55:27.798000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1216] [0/0] [__trace_source]     def simple_model(x):\n",
      "V0617 12:55:27.799000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE RESUME 0 []\n",
      "V0617 12:55:27.800000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1216] [0/0] [__trace_source] TRACE starts_line /tmp/tmp83ltn8a4.py:6 in simple_model (simple_model)\n",
      "V0617 12:55:27.800000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1216] [0/0] [__trace_source]         return torch.relu(x * 2.0) + 1.0\n",
      "V0617 12:55:27.800000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE LOAD_GLOBAL torch []\n",
      "V0617 12:55:27.802000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE LOAD_ATTR relu [NullVariable, PythonModuleVariable(<module 'torch' from '/home/alibina/anaconda3/envs/pytorch-qat/lib/python3.12/site-packages/torch/__init__.py'>)]\n",
      "V0617 12:55:27.803000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE LOAD_FAST x [NullVariable, TorchInGraphFunctionVariable(<built-in method relu of type object at 0x7f29558f6fa0>, nonstrict_traceable=False)]\n",
      "V0617 12:55:27.803000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE LOAD_CONST 2.0 [NullVariable, TorchInGraphFunctionVariable(<built-in method relu of type object at 0x7f29558f6fa0>, nonstrict_traceable=False), LazyVariableTracker()]\n",
      "V0617 12:55:27.803000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE BINARY_OP 5 [NullVariable, TorchInGraphFunctionVariable(<built-in method relu of type object at 0x7f29558f6fa0>, nonstrict_traceable=False), LazyVariableTracker(), ConstantVariable(float: 2.0)]\n",
      "V0617 12:55:27.804000 261031 site-packages/torch/_dynamo/variables/builder.py:3025] [0/0] wrap_to_fake L['x'] (100,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\n",
      "V0617 12:55:27.806000 261031 site-packages/torch/_dynamo/output_graph.py:2271] [0/0] create_graph_input L_x_ L['x'] FakeTensor(..., device='cuda:0', size=(100,)) at debug_level 0 before=False\n",
      "V0617 12:55:27.809000 261031 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call] TRACE FX call mul from /tmp/tmp83ltn8a4.py:6 in simple_model (simple_model)\n",
      "V0617 12:55:27.809000 261031 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call]     return torch.relu(x * 2.0) + 1.0\n",
      "V0617 12:55:27.809000 261031 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call]                       ~~^~~~~\n",
      "V0617 12:55:27.811000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE CALL 1 [NullVariable, TorchInGraphFunctionVariable(<built-in method relu of type object at 0x7f29558f6fa0>, nonstrict_traceable=False), TensorVariable()]\n",
      "V0617 12:55:27.822000 261031 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call] TRACE FX call relu from /tmp/tmp83ltn8a4.py:6 in simple_model (simple_model)\n",
      "V0617 12:55:27.822000 261031 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call]     return torch.relu(x * 2.0) + 1.0\n",
      "V0617 12:55:27.822000 261031 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call]            ~~~~~~~~~~^^^^^^^^^\n",
      "V0617 12:55:27.827000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE LOAD_CONST 1.0 [TensorVariable()]\n",
      "V0617 12:55:27.828000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE BINARY_OP 0 [TensorVariable(), ConstantVariable(float: 1.0)]\n",
      "V0617 12:55:27.828000 261031 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call] TRACE FX call add from /tmp/tmp83ltn8a4.py:6 in simple_model (simple_model)\n",
      "V0617 12:55:27.828000 261031 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call]     return torch.relu(x * 2.0) + 1.0\n",
      "V0617 12:55:27.828000 261031 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call]            ~~~~~~~~~~~~~~~~~~~~^~~~~\n",
      "V0617 12:55:27.830000 261031 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE RETURN_VALUE None [TensorVariable()]\n",
      "I0617 12:55:27.830000 261031 site-packages/torch/_dynamo/symbolic_convert.py:3679] [0/0] Step 1: torchdynamo done tracing simple_model (RETURN_VALUE)\n",
      "V0617 12:55:27.831000 261031 site-packages/torch/_dynamo/symbolic_convert.py:3683] [0/0] RETURN_VALUE triggered compile\n",
      "V0617 12:55:27.831000 261031 site-packages/torch/_dynamo/output_graph.py:1008] [0/0] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/tmp83ltn8a4.py, line 6 in simple_model>], graph_break=False)\n",
      "V0617 12:55:27.836000 261031 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code] TRACED GRAPH\n",
      "V0617 12:55:27.836000 261031 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]  ===== __compiled_fn_1 =====\n",
      "V0617 12:55:27.836000 261031 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]  /home/alibina/anaconda3/envs/pytorch-qat/lib/python3.12/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V0617 12:55:27.836000 261031 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]     def forward(self, L_x_: \"f32[100][1]cuda:0\"):\n",
      "V0617 12:55:27.836000 261031 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         l_x_ = L_x_\n",
      "V0617 12:55:27.836000 261031 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         \n",
      "V0617 12:55:27.836000 261031 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]          # File: /tmp/tmp83ltn8a4.py:6 in simple_model, code: return torch.relu(x * 2.0) + 1.0\n",
      "V0617 12:55:27.836000 261031 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         mul: \"f32[100][1]cuda:0\" = l_x_ * 2.0;  l_x_ = None\n",
      "V0617 12:55:27.836000 261031 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         relu: \"f32[100][1]cuda:0\" = torch.relu(mul);  mul = None\n",
      "V0617 12:55:27.836000 261031 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         add: \"f32[100][1]cuda:0\" = relu + 1.0;  relu = None\n",
      "V0617 12:55:27.836000 261031 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         return (add,)\n",
      "V0617 12:55:27.836000 261031 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         \n",
      "V0617 12:55:27.836000 261031 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code] \n",
      "I0617 12:55:27.837000 261031 site-packages/torch/_dynamo/output_graph.py:1515] [0/0] Step 2: calling compiler function inductor\n",
      "I0617 12:55:28.592000 261031 site-packages/torch/_dynamo/output_graph.py:1520] [0/0] Step 2: done compiler function inductor\n",
      "I0617 12:55:28.603000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:4734] [0/0] produce_guards\n",
      "V0617 12:55:28.605000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['x'].size()[0] 100 None\n",
      "V0617 12:55:28.607000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['x'].stride()[0] 1 None\n",
      "V0617 12:55:28.608000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['x'].storage_offset() 0 None\n",
      "V0617 12:55:28.611000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:5156] [0/0] Skipping guard L['x'].size()[0] == 100\n",
      "V0617 12:55:28.612000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:5156] [0/0] Skipping guard L['x'].stride()[0] == 1\n",
      "V0617 12:55:28.614000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:5156] [0/0] Skipping guard L['x'].storage_offset() == 0\n",
      "V0617 12:55:28.615000 261031 site-packages/torch/_dynamo/guards.py:2566] [0/0] [__guards] GUARDS:\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] \n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] TREE_GUARD_MANAGER:\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] +- RootGuardManager\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:520 in init_ambient_guards\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | +- GLOBAL_STATE: ___check_global_state()\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | +- GuardManager: source=L['x'], accessed_by=FrameLocalsGuardAccessor(key='x', framelocals_idx=0)\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[100], stride=[1])  # return torch.relu(x * 2.0) + 1.0  # mp/tmp83ltn8a4.py:6 in simple_model\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # return torch.relu(x * 2.0) + 1.0  # mp/tmp83ltn8a4.py:6 in simple_model\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor('torch')\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['torch'], 139815642715840)                  # return torch.relu(x * 2.0) + 1.0  # mp/tmp83ltn8a4.py:6 in simple_model\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | | | +- GuardManager: source=G['torch'].relu, accessed_by=GetAttrGuardAccessor(relu)\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].relu, 139815636995264)             # return torch.relu(x * 2.0) + 1.0  # mp/tmp83ltn8a4.py:6 in simple_model\n",
      "V0617 12:55:28.617000 261031 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] \n",
      "V0617 12:55:28.620000 261031 site-packages/torch/_dynamo/guards.py:2533] [0/0] [__guards] Guard eval latency = 1.14 us\n",
      "I0617 12:55:28.622000 261031 site-packages/torch/_dynamo/pgo.py:660] [0/0] put_code_state: no cache key, skipping\n",
      "I0617 12:55:28.623000 261031 site-packages/torch/_dynamo/convert_frame.py:1121] [0/0] run_gc_after_compile: running gc\n",
      "V0617 12:55:28.631000 261031 site-packages/torch/_dynamo/convert_frame.py:1395] skipping: _fn (reason: in skipfiles, file: /home/alibina/anaconda3/envs/pytorch-qat/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py)\n",
      "I0617 12:55:31.261000 261047 site-packages/torch/_dynamo/eval_frame.py:475] TorchDynamo attempted to trace the following frames: [\n",
      "I0617 12:55:31.261000 261047 site-packages/torch/_dynamo/eval_frame.py:475] \n",
      "I0617 12:55:31.261000 261047 site-packages/torch/_dynamo/eval_frame.py:475] ]\n",
      "I0617 12:55:31.264000 261047 site-packages/torch/_dynamo/utils.py:765] TorchDynamo compilation metrics:\n",
      "I0617 12:55:31.264000 261047 site-packages/torch/_dynamo/utils.py:765] Function, Runtimes (s)\n",
      "V0617 12:55:31.267000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats constrain_symbol_range: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:31.268000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats defer_runtime_assert: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:31.269000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats evaluate_expr: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:31.270000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _simplify_floor_div: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:31.270000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _maybe_guard_rel: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:31.271000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _find: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:31.272000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats has_hint: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:31.273000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats size_hint: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:31.274000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats simplify: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:31.276000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _update_divisible: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:31.277000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats replace: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:31.277000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _maybe_evaluate_static: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:31.278000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats get_implications: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:31.278000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats get_axioms: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:31.278000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _maybe_evaluate_static_worker: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:31.279000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats safe_expand: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:31.279000 261047 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats uninteresting_files: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "I0617 12:55:32.490000 261031 site-packages/torch/_dynamo/eval_frame.py:475] TorchDynamo attempted to trace the following frames: [\n",
      "I0617 12:55:32.490000 261031 site-packages/torch/_dynamo/eval_frame.py:475]   * simple_model /tmp/tmp83ltn8a4.py:5\n",
      "I0617 12:55:32.490000 261031 site-packages/torch/_dynamo/eval_frame.py:475] ]\n",
      "I0617 12:55:32.493000 261031 site-packages/torch/_dynamo/utils.py:765] TorchDynamo compilation metrics:\n",
      "I0617 12:55:32.493000 261031 site-packages/torch/_dynamo/utils.py:765] Function, Runtimes (s)\n",
      "I0617 12:55:32.493000 261031 site-packages/torch/_dynamo/utils.py:765] _compile.compile_inner, 0.8292\n",
      "I0617 12:55:32.493000 261031 site-packages/torch/_dynamo/utils.py:765] OutputGraph.call_user_compiler, 0.7575\n",
      "I0617 12:55:32.493000 261031 site-packages/torch/_dynamo/utils.py:765] _recursive_pre_grad_passes, 0.0070\n",
      "I0617 12:55:32.493000 261031 site-packages/torch/_dynamo/utils.py:765] inductor_codecache_torch_key, 0.2066\n",
      "I0617 12:55:32.493000 261031 site-packages/torch/_dynamo/utils.py:765] TritonBundler.read_and_emit, 0.0002\n",
      "I0617 12:55:32.493000 261031 site-packages/torch/_dynamo/utils.py:765] PyCodeCache.load_by_key_path, 0.0938\n",
      "I0617 12:55:32.493000 261031 site-packages/torch/_dynamo/utils.py:765] async_compile.precompile, 0.0841\n",
      "I0617 12:55:32.493000 261031 site-packages/torch/_dynamo/utils.py:765] async_compile.wait, 0.0005\n",
      "I0617 12:55:32.493000 261031 site-packages/torch/_dynamo/utils.py:765] gc, 0.0036\n",
      "V0617 12:55:32.494000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats constrain_symbol_range: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:32.495000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats defer_runtime_assert: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:32.496000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats evaluate_expr: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:32.497000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _simplify_floor_div: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:32.498000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _maybe_guard_rel: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:32.499000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _find: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:32.500000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats has_hint: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:32.501000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats size_hint: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:32.502000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats simplify: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:32.503000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _update_divisible: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:32.504000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats replace: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:32.505000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _maybe_evaluate_static: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:32.506000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats get_implications: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:32.507000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats get_axioms: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:32.508000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _maybe_evaluate_static_worker: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:32.510000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats safe_expand: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:32.511000 261031 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats uninteresting_files: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "\\nüìä Results:\n",
      "   Return code: 0\n",
      "   Debug lines captured: 1\n",
      "   üéâ SUCCESS: Captured PyTorch debug output!\n",
      "\\n==================== Scenario 3: Comprehensive Debug ====================\n",
      "üìù Full debugging with file generation\n",
      "Environment variables set:\n",
      "  TORCH_LOGS=+dynamo,+inductor\n",
      "  TORCH_COMPILE_DEBUG=1\n",
      "\\nRunning external Python process...\n",
      "----------------------------------------\n",
      "üì§ STDOUT:\n",
      "   üéØ PyTorch Logging Test (External Process)\n",
      "Environment variables active:\n",
      "  TORCH_LOGS: +dynamo,+inductor\n",
      "  TORCH_COMPILE_DEBUG: 1\n",
      "  TRITON_PRINT_AUTOTUNING: 1\n",
      "\n",
      "Starting compilation...\n",
      "Compilation completed. Result shape: torch.Size([100])\n",
      "Any logs above this line came from PyTorch!\n",
      "\\nüì• STDERR (PyTorch Debug Logs):\n",
      "   I0617 12:55:37.125000 261103 site-packages/torch/_inductor/config.py:714] compile_threads set to 20\n",
      "I0617 12:55:39.056000 261103 site-packages/torch/_dynamo/__init__.py:112] torch._dynamo.reset\n",
      "I0617 12:55:39.058000 261103 site-packages/torch/_dynamo/__init__.py:145] torch._dynamo.reset_code_caches\n",
      "I0617 12:55:39.900000 261103 site-packages/torch/_inductor/async_compile.py:210] Creating 'subprocess' pool with 20 workers\n",
      "I0617 12:55:41.300000 261103 site-packages/torch/_dynamo/utils.py:1603] [0/0] ChromiumEventLogger initialized with id 3aabf84c-bd9a-40b2-9aae-a0db453bff81\n",
      "V0617 12:55:41.301000 261103 site-packages/torch/_dynamo/convert_frame.py:1003] [0/0] torchdynamo start compiling simple_model /tmp/tmp83ltn8a4.py:5, stack (elided 5 frames):\n",
      "V0617 12:55:41.301000 261103 site-packages/torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/tmp/tmp83ltn8a4.py\", line 31, in <module>\n",
      "V0617 12:55:41.301000 261103 site-packages/torch/_dynamo/convert_frame.py:1003] [0/0]     main()\n",
      "V0617 12:55:41.301000 261103 site-packages/torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/tmp/tmp83ltn8a4.py\", line 25, in main\n",
      "V0617 12:55:41.301000 261103 site-packages/torch/_dynamo/convert_frame.py:1003] [0/0]     result = compiled_model(test_input)\n",
      "V0617 12:55:41.301000 261103 site-packages/torch/_dynamo/convert_frame.py:1003] [0/0] \n",
      "I0617 12:55:41.303000 261103 site-packages/torch/_dynamo/symbolic_convert.py:3322] [0/0] Step 1: torchdynamo start tracing simple_model /tmp/tmp83ltn8a4.py:5\n",
      "I0617 12:55:41.303000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:3334] [0/0] create_env\n",
      "V0617 12:55:41.306000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1216] [0/0] [__trace_source] TRACE starts_line /tmp/tmp83ltn8a4.py:5 in simple_model (simple_model)\n",
      "V0617 12:55:41.306000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1216] [0/0] [__trace_source]     def simple_model(x):\n",
      "V0617 12:55:41.307000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE RESUME 0 []\n",
      "V0617 12:55:41.307000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1216] [0/0] [__trace_source] TRACE starts_line /tmp/tmp83ltn8a4.py:6 in simple_model (simple_model)\n",
      "V0617 12:55:41.307000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1216] [0/0] [__trace_source]         return torch.relu(x * 2.0) + 1.0\n",
      "V0617 12:55:41.308000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE LOAD_GLOBAL torch []\n",
      "V0617 12:55:41.309000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE LOAD_ATTR relu [NullVariable, PythonModuleVariable(<module 'torch' from '/home/alibina/anaconda3/envs/pytorch-qat/lib/python3.12/site-packages/torch/__init__.py'>)]\n",
      "V0617 12:55:41.310000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE LOAD_FAST x [NullVariable, TorchInGraphFunctionVariable(<built-in method relu of type object at 0x7fe1ee2f6fa0>, nonstrict_traceable=False)]\n",
      "V0617 12:55:41.310000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE LOAD_CONST 2.0 [NullVariable, TorchInGraphFunctionVariable(<built-in method relu of type object at 0x7fe1ee2f6fa0>, nonstrict_traceable=False), LazyVariableTracker()]\n",
      "V0617 12:55:41.311000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE BINARY_OP 5 [NullVariable, TorchInGraphFunctionVariable(<built-in method relu of type object at 0x7fe1ee2f6fa0>, nonstrict_traceable=False), LazyVariableTracker(), ConstantVariable(float: 2.0)]\n",
      "V0617 12:55:41.311000 261103 site-packages/torch/_dynamo/variables/builder.py:3025] [0/0] wrap_to_fake L['x'] (100,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\n",
      "V0617 12:55:41.313000 261103 site-packages/torch/_dynamo/output_graph.py:2271] [0/0] create_graph_input L_x_ L['x'] FakeTensor(..., device='cuda:0', size=(100,)) at debug_level 0 before=False\n",
      "V0617 12:55:41.314000 261103 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call] TRACE FX call mul from /tmp/tmp83ltn8a4.py:6 in simple_model (simple_model)\n",
      "V0617 12:55:41.314000 261103 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call]     return torch.relu(x * 2.0) + 1.0\n",
      "V0617 12:55:41.314000 261103 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call]                       ~~^~~~~\n",
      "V0617 12:55:41.317000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE CALL 1 [NullVariable, TorchInGraphFunctionVariable(<built-in method relu of type object at 0x7fe1ee2f6fa0>, nonstrict_traceable=False), TensorVariable()]\n",
      "V0617 12:55:41.325000 261103 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call] TRACE FX call relu from /tmp/tmp83ltn8a4.py:6 in simple_model (simple_model)\n",
      "V0617 12:55:41.325000 261103 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call]     return torch.relu(x * 2.0) + 1.0\n",
      "V0617 12:55:41.325000 261103 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call]            ~~~~~~~~~~^^^^^^^^^\n",
      "V0617 12:55:41.328000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE LOAD_CONST 1.0 [TensorVariable()]\n",
      "V0617 12:55:41.329000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE BINARY_OP 0 [TensorVariable(), ConstantVariable(float: 1.0)]\n",
      "V0617 12:55:41.329000 261103 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call] TRACE FX call add from /tmp/tmp83ltn8a4.py:6 in simple_model (simple_model)\n",
      "V0617 12:55:41.329000 261103 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call]     return torch.relu(x * 2.0) + 1.0\n",
      "V0617 12:55:41.329000 261103 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call]            ~~~~~~~~~~~~~~~~~~~~^~~~~\n",
      "V0617 12:55:41.334000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE RETURN_VALUE None [TensorVariable()]\n",
      "I0617 12:55:41.335000 261103 site-packages/torch/_dynamo/symbolic_convert.py:3679] [0/0] Step 1: torchdynamo done tracing simple_model (RETURN_VALUE)\n",
      "V0617 12:55:41.336000 261103 site-packages/torch/_dynamo/symbolic_convert.py:3683] [0/0] RETURN_VALUE triggered compile\n",
      "V0617 12:55:41.337000 261103 site-packages/torch/_dynamo/output_graph.py:1008] [0/0] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/tmp83ltn8a4.py, line 6 in simple_model>], graph_break=False)\n",
      "V0617 12:55:41.340000 261103 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code] TRACED GRAPH\n",
      "V0617 12:55:41.340000 261103 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]  ===== __compiled_fn_1 =====\n",
      "V0617 12:55:41.340000 261103 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]  /home/alibina/anaconda3/envs/pytorch-qat/lib/python3.12/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V0617 12:55:41.340000 261103 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]     def forward(self, L_x_: \"f32[100][1]cuda:0\"):\n",
      "V0617 12:55:41.340000 261103 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         l_x_ = L_x_\n",
      "V0617 12:55:41.340000 261103 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         \n",
      "V0617 12:55:41.340000 261103 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]          # File: /tmp/tmp83ltn8a4.py:6 in simple_model, code: return torch.relu(x * 2.0) + 1.0\n",
      "V0617 12:55:41.340000 261103 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         mul: \"f32[100][1]cuda:0\" = l_x_ * 2.0;  l_x_ = None\n",
      "V0617 12:55:41.340000 261103 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         relu: \"f32[100][1]cuda:0\" = torch.relu(mul);  mul = None\n",
      "V0617 12:55:41.340000 261103 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         add: \"f32[100][1]cuda:0\" = relu + 1.0;  relu = None\n",
      "V0617 12:55:41.340000 261103 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         return (add,)\n",
      "V0617 12:55:41.340000 261103 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         \n",
      "V0617 12:55:41.340000 261103 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code] \n",
      "I0617 12:55:41.342000 261103 site-packages/torch/_dynamo/output_graph.py:1515] [0/0] Step 2: calling compiler function inductor\n",
      "V0617 12:55:41.342000 261103 site-packages/torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs] TRACED GRAPH\n",
      "V0617 12:55:41.342000 261103 site-packages/torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]  ===== BEFORE PRE GRAD =====\n",
      "V0617 12:55:41.342000 261103 site-packages/torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]  /home/alibina/anaconda3/envs/pytorch-qat/lib/python3.12/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V0617 12:55:41.342000 261103 site-packages/torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]     def forward(self, L_x_: \"f32[100][1]cuda:0\"):\n",
      "V0617 12:55:41.342000 261103 site-packages/torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]         l_x_ = L_x_\n",
      "V0617 12:55:41.342000 261103 site-packages/torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]         \n",
      "V0617 12:55:41.342000 261103 site-packages/torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]          # File: /tmp/tmp83ltn8a4.py:6 in simple_model, code: return torch.relu(x * 2.0) + 1.0\n",
      "V0617 12:55:41.342000 261103 site-packages/torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]         mul: \"f32[100][1]cuda:0\" = l_x_ * 2.0;  l_x_ = None\n",
      "V0617 12:55:41.342000 261103 site-packages/torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]         relu: \"f32[100][1]cuda:0\" = torch.relu(mul);  mul = None\n",
      "V0617 12:55:41.342000 261103 site-packages/torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]         add: \"f32[100][1]cuda:0\" = relu + 1.0;  relu = None\n",
      "V0617 12:55:41.342000 261103 site-packages/torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]         return (add,)\n",
      "V0617 12:55:41.342000 261103 site-packages/torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]         \n",
      "V0617 12:55:41.342000 261103 site-packages/torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs] \n",
      "V0617 12:55:41.922000 261103 site-packages/torch/_inductor/triton_bundler.py:237] [0/0] Bailing out TritonBundler.read_and_emit, /tmp/torchinductor_alibina/triton/0/P35HXPGUYLE3XWLZBWLNR3GPONWWXECRJTEFNLKJQOZD46UYAJPA is non empty\n",
      "V0617 12:55:41.973000 261103 site-packages/torch/_inductor/runtime/triton_heuristics.py:216] [0/0] CachingAutotuner gets 1 configs for triton_poi_fused_add_mul_relu_0\n",
      "V0617 12:55:41.975000 261103 site-packages/torch/_inductor/runtime/triton_heuristics.py:222] [0/0] XBLOCK: 128, num_warps: 4, num_ctas: 1, num_stages: 1, num_buffers_warp_spec: 0, num_consumer_groups: 0, reg_dec_producer: 0, reg_inc_consumer: 0, maxnreg: None\n",
      "V0617 12:55:41.975000 261103 site-packages/torch/_inductor/runtime/triton_heuristics.py:231] [0/0] Triton cache dir: /tmp/torchinductor_alibina/triton/0\n",
      "I0617 12:55:41.995000 261103 site-packages/torch/_inductor/codecache.py:1306] [0/0] fx graph cache hit for key fjukzropfsngdqlfww6h7biqar6qyma6adfw6no6sfoor62ymiuq\n",
      "I0617 12:55:41.997000 261103 site-packages/torch/_dynamo/output_graph.py:1520] [0/0] Step 2: done compiler function inductor\n",
      "I0617 12:55:42.004000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:4734] [0/0] produce_guards\n",
      "V0617 12:55:42.007000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['x'].size()[0] 100 None\n",
      "V0617 12:55:42.009000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['x'].stride()[0] 1 None\n",
      "V0617 12:55:42.010000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['x'].storage_offset() 0 None\n",
      "V0617 12:55:42.013000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:5156] [0/0] Skipping guard L['x'].size()[0] == 100\n",
      "V0617 12:55:42.014000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:5156] [0/0] Skipping guard L['x'].stride()[0] == 1\n",
      "V0617 12:55:42.016000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:5156] [0/0] Skipping guard L['x'].storage_offset() == 0\n",
      "V0617 12:55:42.018000 261103 site-packages/torch/_dynamo/guards.py:2566] [0/0] [__guards] GUARDS:\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] \n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] TREE_GUARD_MANAGER:\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] +- RootGuardManager\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:520 in init_ambient_guards\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | +- GLOBAL_STATE: ___check_global_state()\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | +- GuardManager: source=L['x'], accessed_by=FrameLocalsGuardAccessor(key='x', framelocals_idx=0)\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[100], stride=[1])  # return torch.relu(x * 2.0) + 1.0  # mp/tmp83ltn8a4.py:6 in simple_model\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # return torch.relu(x * 2.0) + 1.0  # mp/tmp83ltn8a4.py:6 in simple_model\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor('torch')\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['torch'], 140608476321552)                  # return torch.relu(x * 2.0) + 1.0  # mp/tmp83ltn8a4.py:6 in simple_model\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | | | +- GuardManager: source=G['torch'].relu, accessed_by=GetAttrGuardAccessor(relu)\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].relu, 140608470617200)             # return torch.relu(x * 2.0) + 1.0  # mp/tmp83ltn8a4.py:6 in simple_model\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] \n",
      "V0617 12:55:42.022000 261103 site-packages/torch/_dynamo/guards.py:2533] [0/0] [__guards] Guard eval latency = 1.24 us\n",
      "I0617 12:55:42.023000 261103 site-packages/torch/_dynamo/pgo.py:660] [0/0] put_code_state: no cache key, skipping\n",
      "I0617 12:55:42.025000 261103 site-packages/torch/_dynamo/convert_frame.py:1121] [0/0] run_gc_after_compile: running gc\n",
      "V0617 12:55:42.034000 261103 site-packages/torch/_dynamo/convert_frame.py:1395] skipping: _fn (reason: in skipfiles, file: /home/alibina/anaconda3/envs/pytorch-qat/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py)\n",
      "I0617 12:55:42.249000 261119 site-packages/torch/_inductor/config.py:714] compile_threads set to 20\n",
      "I0617 12:55:45.389000 261119 site-packages/torch/_inductor/remote_cache.py:417] Cache Metrics: None\n",
      "I0617 12:55:45.389000 261119 site-packages/torch/_inductor/remote_cache.py:417] \n",
      "I0617 12:55:45.392000 261119 site-packages/torch/_dynamo/eval_frame.py:475] TorchDynamo attempted to trace the following frames: [\n",
      "I0617 12:55:45.392000 261119 site-packages/torch/_dynamo/eval_frame.py:475] \n",
      "I0617 12:55:45.392000 261119 site-packages/torch/_dynamo/eval_frame.py:475] ]\n",
      "I0617 12:55:45.395000 261119 site-packages/torch/_dynamo/utils.py:765] TorchDynamo compilation metrics:\n",
      "I0617 12:55:45.395000 261119 site-packages/torch/_dynamo/utils.py:765] Function, Runtimes (s)\n",
      "V0617 12:55:45.397000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats constrain_symbol_range: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:45.398000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats defer_runtime_assert: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:45.399000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats evaluate_expr: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:45.400000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _simplify_floor_div: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:45.401000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _maybe_guard_rel: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:45.402000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _find: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:45.403000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats has_hint: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:45.404000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats size_hint: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:45.405000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats simplify: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:45.407000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _update_divisible: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:45.408000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats replace: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:45.409000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _maybe_evaluate_static: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:45.410000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats get_implications: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:45.411000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats get_axioms: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:45.412000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _maybe_evaluate_static_worker: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:45.412000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats safe_expand: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:45.413000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats uninteresting_files: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "I0617 12:55:46.829000 261103 site-packages/torch/_inductor/remote_cache.py:417] Cache Metrics: None\n",
      "I0617 12:55:46.829000 261103 site-packages/torch/_inductor/remote_cache.py:417] \n",
      "I0617 12:55:46.831000 261103 site-packages/torch/_dynamo/eval_frame.py:475] TorchDynamo attempted to trace the following frames: [\n",
      "I0617 12:55:46.831000 261103 site-packages/torch/_dynamo/eval_frame.py:475]   * simple_model /tmp/tmp83ltn8a4.py:5\n",
      "I0617 12:55:46.831000 261103 site-packages/torch/_dynamo/eval_frame.py:475] ]\n",
      "I0617 12:55:46.835000 261103 site-packages/torch/_dynamo/utils.py:765] TorchDynamo compilation metrics:\n",
      "I0617 12:55:46.835000 261103 site-packages/torch/_dynamo/utils.py:765] Function, Runtimes (s)\n",
      "I0617 12:55:46.835000 261103 site-packages/torch/_dynamo/utils.py:765] _compile.compile_inner, 0.7219\n",
      "I0617 12:55:46.835000 261103 site-packages/torch/_dynamo/utils.py:765] OutputGraph.call_user_compiler, 0.6567\n",
      "I0617 12:55:46.835000 261103 site-packages/torch/_dynamo/utils.py:765] _recursive_pre_grad_passes, 0.0049\n",
      "I0617 12:55:46.835000 261103 site-packages/torch/_dynamo/utils.py:765] inductor_codecache_torch_key, 0.1698\n",
      "I0617 12:55:46.835000 261103 site-packages/torch/_dynamo/utils.py:765] TritonBundler.read_and_emit, 0.0012\n",
      "I0617 12:55:46.835000 261103 site-packages/torch/_dynamo/utils.py:765] PyCodeCache.load_by_key_path, 0.0708\n",
      "I0617 12:55:46.835000 261103 site-packages/torch/_dynamo/utils.py:765] async_compile.precompile, 0.0530\n",
      "I0617 12:55:46.835000 261103 site-packages/torch/_dynamo/utils.py:765] async_compile.wait, 0.0003\n",
      "I0617 12:55:46.835000 261103 site-packages/torch/_dynamo/utils.py:765] gc, 0.0036\n",
      "V0617 12:55:46.838000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats constrain_symbol_range: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:46.841000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats defer_runtime_assert: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:46.843000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats evaluate_expr: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:46.844000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _simplify_floor_div: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:46.846000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _maybe_guard_rel: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:46.847000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _find: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:46.848000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats has_hint: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:46.849000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats size_hint: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:46.850000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats simplify: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:46.852000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _update_divisible: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:46.853000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats replace: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:46.854000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _maybe_evaluate_static: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:46.854000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats get_implications: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:46.855000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats get_axioms: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:46.856000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _maybe_evaluate_static_worker: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:46.857000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats safe_expand: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:46.858000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats uninteresting_files: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "\\nüìä Results:\n",
      "   Return code: 0\n",
      "   Debug lines captured: 1\n",
      "   üéâ SUCCESS: Captured PyTorch debug output!\n",
      "\\nüí° Key Insight: Subprocess Capture Solution\n",
      "‚úÖ This method works in Jupyter because:\n",
      "   ‚Ä¢ Runs PyTorch in external process\n",
      "   ‚Ä¢ Captures ALL output streams\n",
      "   ‚Ä¢ Shows debug info that Jupyter normally can't see\n",
      "   ‚Ä¢ Provides complete visibility into compilation process\n",
      "üì§ STDOUT:\n",
      "   üéØ PyTorch Logging Test (External Process)\n",
      "Environment variables active:\n",
      "  TORCH_LOGS: +dynamo,+inductor\n",
      "  TORCH_COMPILE_DEBUG: 1\n",
      "  TRITON_PRINT_AUTOTUNING: 1\n",
      "\n",
      "Starting compilation...\n",
      "Compilation completed. Result shape: torch.Size([100])\n",
      "Any logs above this line came from PyTorch!\n",
      "\\nüì• STDERR (PyTorch Debug Logs):\n",
      "   I0617 12:55:37.125000 261103 site-packages/torch/_inductor/config.py:714] compile_threads set to 20\n",
      "I0617 12:55:39.056000 261103 site-packages/torch/_dynamo/__init__.py:112] torch._dynamo.reset\n",
      "I0617 12:55:39.058000 261103 site-packages/torch/_dynamo/__init__.py:145] torch._dynamo.reset_code_caches\n",
      "I0617 12:55:39.900000 261103 site-packages/torch/_inductor/async_compile.py:210] Creating 'subprocess' pool with 20 workers\n",
      "I0617 12:55:41.300000 261103 site-packages/torch/_dynamo/utils.py:1603] [0/0] ChromiumEventLogger initialized with id 3aabf84c-bd9a-40b2-9aae-a0db453bff81\n",
      "V0617 12:55:41.301000 261103 site-packages/torch/_dynamo/convert_frame.py:1003] [0/0] torchdynamo start compiling simple_model /tmp/tmp83ltn8a4.py:5, stack (elided 5 frames):\n",
      "V0617 12:55:41.301000 261103 site-packages/torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/tmp/tmp83ltn8a4.py\", line 31, in <module>\n",
      "V0617 12:55:41.301000 261103 site-packages/torch/_dynamo/convert_frame.py:1003] [0/0]     main()\n",
      "V0617 12:55:41.301000 261103 site-packages/torch/_dynamo/convert_frame.py:1003] [0/0]   File \"/tmp/tmp83ltn8a4.py\", line 25, in main\n",
      "V0617 12:55:41.301000 261103 site-packages/torch/_dynamo/convert_frame.py:1003] [0/0]     result = compiled_model(test_input)\n",
      "V0617 12:55:41.301000 261103 site-packages/torch/_dynamo/convert_frame.py:1003] [0/0] \n",
      "I0617 12:55:41.303000 261103 site-packages/torch/_dynamo/symbolic_convert.py:3322] [0/0] Step 1: torchdynamo start tracing simple_model /tmp/tmp83ltn8a4.py:5\n",
      "I0617 12:55:41.303000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:3334] [0/0] create_env\n",
      "V0617 12:55:41.306000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1216] [0/0] [__trace_source] TRACE starts_line /tmp/tmp83ltn8a4.py:5 in simple_model (simple_model)\n",
      "V0617 12:55:41.306000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1216] [0/0] [__trace_source]     def simple_model(x):\n",
      "V0617 12:55:41.307000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE RESUME 0 []\n",
      "V0617 12:55:41.307000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1216] [0/0] [__trace_source] TRACE starts_line /tmp/tmp83ltn8a4.py:6 in simple_model (simple_model)\n",
      "V0617 12:55:41.307000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1216] [0/0] [__trace_source]         return torch.relu(x * 2.0) + 1.0\n",
      "V0617 12:55:41.308000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE LOAD_GLOBAL torch []\n",
      "V0617 12:55:41.309000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE LOAD_ATTR relu [NullVariable, PythonModuleVariable(<module 'torch' from '/home/alibina/anaconda3/envs/pytorch-qat/lib/python3.12/site-packages/torch/__init__.py'>)]\n",
      "V0617 12:55:41.310000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE LOAD_FAST x [NullVariable, TorchInGraphFunctionVariable(<built-in method relu of type object at 0x7fe1ee2f6fa0>, nonstrict_traceable=False)]\n",
      "V0617 12:55:41.310000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE LOAD_CONST 2.0 [NullVariable, TorchInGraphFunctionVariable(<built-in method relu of type object at 0x7fe1ee2f6fa0>, nonstrict_traceable=False), LazyVariableTracker()]\n",
      "V0617 12:55:41.311000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE BINARY_OP 5 [NullVariable, TorchInGraphFunctionVariable(<built-in method relu of type object at 0x7fe1ee2f6fa0>, nonstrict_traceable=False), LazyVariableTracker(), ConstantVariable(float: 2.0)]\n",
      "V0617 12:55:41.311000 261103 site-packages/torch/_dynamo/variables/builder.py:3025] [0/0] wrap_to_fake L['x'] (100,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\n",
      "V0617 12:55:41.313000 261103 site-packages/torch/_dynamo/output_graph.py:2271] [0/0] create_graph_input L_x_ L['x'] FakeTensor(..., device='cuda:0', size=(100,)) at debug_level 0 before=False\n",
      "V0617 12:55:41.314000 261103 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call] TRACE FX call mul from /tmp/tmp83ltn8a4.py:6 in simple_model (simple_model)\n",
      "V0617 12:55:41.314000 261103 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call]     return torch.relu(x * 2.0) + 1.0\n",
      "V0617 12:55:41.314000 261103 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call]                       ~~^~~~~\n",
      "V0617 12:55:41.317000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE CALL 1 [NullVariable, TorchInGraphFunctionVariable(<built-in method relu of type object at 0x7fe1ee2f6fa0>, nonstrict_traceable=False), TensorVariable()]\n",
      "V0617 12:55:41.325000 261103 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call] TRACE FX call relu from /tmp/tmp83ltn8a4.py:6 in simple_model (simple_model)\n",
      "V0617 12:55:41.325000 261103 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call]     return torch.relu(x * 2.0) + 1.0\n",
      "V0617 12:55:41.325000 261103 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call]            ~~~~~~~~~~^^^^^^^^^\n",
      "V0617 12:55:41.328000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE LOAD_CONST 1.0 [TensorVariable()]\n",
      "V0617 12:55:41.329000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE BINARY_OP 0 [TensorVariable(), ConstantVariable(float: 1.0)]\n",
      "V0617 12:55:41.329000 261103 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call] TRACE FX call add from /tmp/tmp83ltn8a4.py:6 in simple_model (simple_model)\n",
      "V0617 12:55:41.329000 261103 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call]     return torch.relu(x * 2.0) + 1.0\n",
      "V0617 12:55:41.329000 261103 site-packages/torch/_dynamo/output_graph.py:2122] [0/0] [__trace_call]            ~~~~~~~~~~~~~~~~~~~~^~~~~\n",
      "V0617 12:55:41.334000 261103 site-packages/torch/_dynamo/symbolic_convert.py:1239] [0/0] [__trace_bytecode] TRACE RETURN_VALUE None [TensorVariable()]\n",
      "I0617 12:55:41.335000 261103 site-packages/torch/_dynamo/symbolic_convert.py:3679] [0/0] Step 1: torchdynamo done tracing simple_model (RETURN_VALUE)\n",
      "V0617 12:55:41.336000 261103 site-packages/torch/_dynamo/symbolic_convert.py:3683] [0/0] RETURN_VALUE triggered compile\n",
      "V0617 12:55:41.337000 261103 site-packages/torch/_dynamo/output_graph.py:1008] [0/0] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/tmp83ltn8a4.py, line 6 in simple_model>], graph_break=False)\n",
      "V0617 12:55:41.340000 261103 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code] TRACED GRAPH\n",
      "V0617 12:55:41.340000 261103 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]  ===== __compiled_fn_1 =====\n",
      "V0617 12:55:41.340000 261103 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]  /home/alibina/anaconda3/envs/pytorch-qat/lib/python3.12/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V0617 12:55:41.340000 261103 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]     def forward(self, L_x_: \"f32[100][1]cuda:0\"):\n",
      "V0617 12:55:41.340000 261103 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         l_x_ = L_x_\n",
      "V0617 12:55:41.340000 261103 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         \n",
      "V0617 12:55:41.340000 261103 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]          # File: /tmp/tmp83ltn8a4.py:6 in simple_model, code: return torch.relu(x * 2.0) + 1.0\n",
      "V0617 12:55:41.340000 261103 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         mul: \"f32[100][1]cuda:0\" = l_x_ * 2.0;  l_x_ = None\n",
      "V0617 12:55:41.340000 261103 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         relu: \"f32[100][1]cuda:0\" = torch.relu(mul);  mul = None\n",
      "V0617 12:55:41.340000 261103 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         add: \"f32[100][1]cuda:0\" = relu + 1.0;  relu = None\n",
      "V0617 12:55:41.340000 261103 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         return (add,)\n",
      "V0617 12:55:41.340000 261103 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         \n",
      "V0617 12:55:41.340000 261103 site-packages/torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code] \n",
      "I0617 12:55:41.342000 261103 site-packages/torch/_dynamo/output_graph.py:1515] [0/0] Step 2: calling compiler function inductor\n",
      "V0617 12:55:41.342000 261103 site-packages/torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs] TRACED GRAPH\n",
      "V0617 12:55:41.342000 261103 site-packages/torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]  ===== BEFORE PRE GRAD =====\n",
      "V0617 12:55:41.342000 261103 site-packages/torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]  /home/alibina/anaconda3/envs/pytorch-qat/lib/python3.12/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V0617 12:55:41.342000 261103 site-packages/torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]     def forward(self, L_x_: \"f32[100][1]cuda:0\"):\n",
      "V0617 12:55:41.342000 261103 site-packages/torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]         l_x_ = L_x_\n",
      "V0617 12:55:41.342000 261103 site-packages/torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]         \n",
      "V0617 12:55:41.342000 261103 site-packages/torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]          # File: /tmp/tmp83ltn8a4.py:6 in simple_model, code: return torch.relu(x * 2.0) + 1.0\n",
      "V0617 12:55:41.342000 261103 site-packages/torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]         mul: \"f32[100][1]cuda:0\" = l_x_ * 2.0;  l_x_ = None\n",
      "V0617 12:55:41.342000 261103 site-packages/torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]         relu: \"f32[100][1]cuda:0\" = torch.relu(mul);  mul = None\n",
      "V0617 12:55:41.342000 261103 site-packages/torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]         add: \"f32[100][1]cuda:0\" = relu + 1.0;  relu = None\n",
      "V0617 12:55:41.342000 261103 site-packages/torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]         return (add,)\n",
      "V0617 12:55:41.342000 261103 site-packages/torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]         \n",
      "V0617 12:55:41.342000 261103 site-packages/torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs] \n",
      "V0617 12:55:41.922000 261103 site-packages/torch/_inductor/triton_bundler.py:237] [0/0] Bailing out TritonBundler.read_and_emit, /tmp/torchinductor_alibina/triton/0/P35HXPGUYLE3XWLZBWLNR3GPONWWXECRJTEFNLKJQOZD46UYAJPA is non empty\n",
      "V0617 12:55:41.973000 261103 site-packages/torch/_inductor/runtime/triton_heuristics.py:216] [0/0] CachingAutotuner gets 1 configs for triton_poi_fused_add_mul_relu_0\n",
      "V0617 12:55:41.975000 261103 site-packages/torch/_inductor/runtime/triton_heuristics.py:222] [0/0] XBLOCK: 128, num_warps: 4, num_ctas: 1, num_stages: 1, num_buffers_warp_spec: 0, num_consumer_groups: 0, reg_dec_producer: 0, reg_inc_consumer: 0, maxnreg: None\n",
      "V0617 12:55:41.975000 261103 site-packages/torch/_inductor/runtime/triton_heuristics.py:231] [0/0] Triton cache dir: /tmp/torchinductor_alibina/triton/0\n",
      "I0617 12:55:41.995000 261103 site-packages/torch/_inductor/codecache.py:1306] [0/0] fx graph cache hit for key fjukzropfsngdqlfww6h7biqar6qyma6adfw6no6sfoor62ymiuq\n",
      "I0617 12:55:41.997000 261103 site-packages/torch/_dynamo/output_graph.py:1520] [0/0] Step 2: done compiler function inductor\n",
      "I0617 12:55:42.004000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:4734] [0/0] produce_guards\n",
      "V0617 12:55:42.007000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['x'].size()[0] 100 None\n",
      "V0617 12:55:42.009000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['x'].stride()[0] 1 None\n",
      "V0617 12:55:42.010000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:4954] [0/0] track_symint L['x'].storage_offset() 0 None\n",
      "V0617 12:55:42.013000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:5156] [0/0] Skipping guard L['x'].size()[0] == 100\n",
      "V0617 12:55:42.014000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:5156] [0/0] Skipping guard L['x'].stride()[0] == 1\n",
      "V0617 12:55:42.016000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:5156] [0/0] Skipping guard L['x'].storage_offset() == 0\n",
      "V0617 12:55:42.018000 261103 site-packages/torch/_dynamo/guards.py:2566] [0/0] [__guards] GUARDS:\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] \n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] TREE_GUARD_MANAGER:\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] +- RootGuardManager\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:520 in init_ambient_guards\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | +- GLOBAL_STATE: ___check_global_state()\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | +- GuardManager: source=L['x'], accessed_by=FrameLocalsGuardAccessor(key='x', framelocals_idx=0)\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[100], stride=[1])  # return torch.relu(x * 2.0) + 1.0  # mp/tmp83ltn8a4.py:6 in simple_model\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # return torch.relu(x * 2.0) + 1.0  # mp/tmp83ltn8a4.py:6 in simple_model\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor('torch')\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['torch'], 140608476321552)                  # return torch.relu(x * 2.0) + 1.0  # mp/tmp83ltn8a4.py:6 in simple_model\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | | | +- GuardManager: source=G['torch'].relu, accessed_by=GetAttrGuardAccessor(relu)\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].relu, 140608470617200)             # return torch.relu(x * 2.0) + 1.0  # mp/tmp83ltn8a4.py:6 in simple_model\n",
      "V0617 12:55:42.020000 261103 site-packages/torch/_dynamo/guards.py:2504] [0/0] [__guards] \n",
      "V0617 12:55:42.022000 261103 site-packages/torch/_dynamo/guards.py:2533] [0/0] [__guards] Guard eval latency = 1.24 us\n",
      "I0617 12:55:42.023000 261103 site-packages/torch/_dynamo/pgo.py:660] [0/0] put_code_state: no cache key, skipping\n",
      "I0617 12:55:42.025000 261103 site-packages/torch/_dynamo/convert_frame.py:1121] [0/0] run_gc_after_compile: running gc\n",
      "V0617 12:55:42.034000 261103 site-packages/torch/_dynamo/convert_frame.py:1395] skipping: _fn (reason: in skipfiles, file: /home/alibina/anaconda3/envs/pytorch-qat/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py)\n",
      "I0617 12:55:42.249000 261119 site-packages/torch/_inductor/config.py:714] compile_threads set to 20\n",
      "I0617 12:55:45.389000 261119 site-packages/torch/_inductor/remote_cache.py:417] Cache Metrics: None\n",
      "I0617 12:55:45.389000 261119 site-packages/torch/_inductor/remote_cache.py:417] \n",
      "I0617 12:55:45.392000 261119 site-packages/torch/_dynamo/eval_frame.py:475] TorchDynamo attempted to trace the following frames: [\n",
      "I0617 12:55:45.392000 261119 site-packages/torch/_dynamo/eval_frame.py:475] \n",
      "I0617 12:55:45.392000 261119 site-packages/torch/_dynamo/eval_frame.py:475] ]\n",
      "I0617 12:55:45.395000 261119 site-packages/torch/_dynamo/utils.py:765] TorchDynamo compilation metrics:\n",
      "I0617 12:55:45.395000 261119 site-packages/torch/_dynamo/utils.py:765] Function, Runtimes (s)\n",
      "V0617 12:55:45.397000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats constrain_symbol_range: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:45.398000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats defer_runtime_assert: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:45.399000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats evaluate_expr: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:45.400000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _simplify_floor_div: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:45.401000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _maybe_guard_rel: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:45.402000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _find: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:45.403000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats has_hint: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:45.404000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats size_hint: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:45.405000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats simplify: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:45.407000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _update_divisible: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:45.408000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats replace: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:45.409000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _maybe_evaluate_static: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:45.410000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats get_implications: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:45.411000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats get_axioms: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:45.412000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _maybe_evaluate_static_worker: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:45.412000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats safe_expand: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:45.413000 261119 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats uninteresting_files: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "I0617 12:55:46.829000 261103 site-packages/torch/_inductor/remote_cache.py:417] Cache Metrics: None\n",
      "I0617 12:55:46.829000 261103 site-packages/torch/_inductor/remote_cache.py:417] \n",
      "I0617 12:55:46.831000 261103 site-packages/torch/_dynamo/eval_frame.py:475] TorchDynamo attempted to trace the following frames: [\n",
      "I0617 12:55:46.831000 261103 site-packages/torch/_dynamo/eval_frame.py:475]   * simple_model /tmp/tmp83ltn8a4.py:5\n",
      "I0617 12:55:46.831000 261103 site-packages/torch/_dynamo/eval_frame.py:475] ]\n",
      "I0617 12:55:46.835000 261103 site-packages/torch/_dynamo/utils.py:765] TorchDynamo compilation metrics:\n",
      "I0617 12:55:46.835000 261103 site-packages/torch/_dynamo/utils.py:765] Function, Runtimes (s)\n",
      "I0617 12:55:46.835000 261103 site-packages/torch/_dynamo/utils.py:765] _compile.compile_inner, 0.7219\n",
      "I0617 12:55:46.835000 261103 site-packages/torch/_dynamo/utils.py:765] OutputGraph.call_user_compiler, 0.6567\n",
      "I0617 12:55:46.835000 261103 site-packages/torch/_dynamo/utils.py:765] _recursive_pre_grad_passes, 0.0049\n",
      "I0617 12:55:46.835000 261103 site-packages/torch/_dynamo/utils.py:765] inductor_codecache_torch_key, 0.1698\n",
      "I0617 12:55:46.835000 261103 site-packages/torch/_dynamo/utils.py:765] TritonBundler.read_and_emit, 0.0012\n",
      "I0617 12:55:46.835000 261103 site-packages/torch/_dynamo/utils.py:765] PyCodeCache.load_by_key_path, 0.0708\n",
      "I0617 12:55:46.835000 261103 site-packages/torch/_dynamo/utils.py:765] async_compile.precompile, 0.0530\n",
      "I0617 12:55:46.835000 261103 site-packages/torch/_dynamo/utils.py:765] async_compile.wait, 0.0003\n",
      "I0617 12:55:46.835000 261103 site-packages/torch/_dynamo/utils.py:765] gc, 0.0036\n",
      "V0617 12:55:46.838000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats constrain_symbol_range: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:46.841000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats defer_runtime_assert: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:46.843000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats evaluate_expr: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:46.844000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _simplify_floor_div: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:46.846000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _maybe_guard_rel: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:46.847000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _find: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:46.848000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats has_hint: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:46.849000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats size_hint: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:46.850000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats simplify: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:46.852000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _update_divisible: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:46.853000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats replace: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:46.854000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _maybe_evaluate_static: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:46.854000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats get_implications: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:46.855000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats get_axioms: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:46.856000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats _maybe_evaluate_static_worker: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "V0617 12:55:46.857000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats safe_expand: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\n",
      "V0617 12:55:46.858000 261103 site-packages/torch/fx/experimental/symbolic_shapes.py:166] lru_cache_stats uninteresting_files: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\n",
      "\\nüìä Results:\n",
      "   Return code: 0\n",
      "   Debug lines captured: 1\n",
      "   üéâ SUCCESS: Captured PyTorch debug output!\n",
      "\\nüí° Key Insight: Subprocess Capture Solution\n",
      "‚úÖ This method works in Jupyter because:\n",
      "   ‚Ä¢ Runs PyTorch in external process\n",
      "   ‚Ä¢ Captures ALL output streams\n",
      "   ‚Ä¢ Shows debug info that Jupyter normally can't see\n",
      "   ‚Ä¢ Provides complete visibility into compilation process\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def demonstrate_jupyter_vs_terminal_logging():\n",
    "    \"\"\"\n",
    "    Demonstrate the logging problem in Jupyter and show the solution\n",
    "    \"\"\"\n",
    "    print(\"üîß The Jupyter vs Terminal Logging Problem\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create a simple test script to run externally\n",
    "    test_script_content = '''\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def simple_model(x):\n",
    "    return torch.relu(x * 2.0) + 1.0\n",
    "\n",
    "def main():\n",
    "    print(\"üéØ PyTorch Logging Test (External Process)\")\n",
    "    print(\"Environment variables active:\")\n",
    "    \n",
    "    # Show environment variables that were set\n",
    "    for key in ['TORCH_LOGS', 'TORCH_COMPILE_DEBUG', 'TRITON_PRINT_AUTOTUNING']:\n",
    "        value = os.environ.get(key, 'Not Set')\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(\"\\\\nStarting compilation...\")\n",
    "    \n",
    "    # Clear cache and compile\n",
    "    torch._dynamo.reset()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    test_input = torch.randn(100, device=device)\n",
    "    \n",
    "    compiled_model = torch.compile(simple_model)\n",
    "    result = compiled_model(test_input)\n",
    "    \n",
    "    print(f\"Compilation completed. Result shape: {result.shape}\")\n",
    "    print(\"Any logs above this line came from PyTorch!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "    \n",
    "    # Create temporary script\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n",
    "        f.write(test_script_content)\n",
    "        temp_script = f.name\n",
    "    \n",
    "    try:\n",
    "        # Test scenarios with different logging levels\n",
    "        scenarios = [\n",
    "            {\n",
    "                \"name\": \"Minimal (No Debug)\",\n",
    "                \"env_vars\": {},\n",
    "                \"description\": \"Standard execution without debug output\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Basic Dynamo Logging\", \n",
    "                \"env_vars\": {\"TORCH_LOGS\": \"+dynamo\"},\n",
    "                \"description\": \"Shows graph capture and compilation decisions\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Comprehensive Debug\",\n",
    "                \"env_vars\": {\n",
    "                    \"TORCH_LOGS\": \"+dynamo,+inductor\",\n",
    "                    \"TORCH_COMPILE_DEBUG\": \"1\"\n",
    "                },\n",
    "                \"description\": \"Full debugging with file generation\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for i, scenario in enumerate(scenarios, 1):\n",
    "            print(f\"\\\\n{'='*20} Scenario {i}: {scenario['name']} {'='*20}\")\n",
    "            print(f\"üìù {scenario['description']}\")\n",
    "            \n",
    "            # Prepare environment\n",
    "            env = os.environ.copy()\n",
    "            env_vars = scenario['env_vars']\n",
    "            \n",
    "            if env_vars:\n",
    "                print(\"Environment variables set:\")\n",
    "                for key, value in env_vars.items():\n",
    "                    env[key] = value\n",
    "                    print(f\"  {key}={value}\")\n",
    "            else:\n",
    "                print(\"No environment variables set\")\n",
    "            \n",
    "            print(\"\\\\nRunning external Python process...\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            try:\n",
    "                # Run script with timeout\n",
    "                result = subprocess.run(\n",
    "                    [sys.executable, temp_script],\n",
    "                    env=env,\n",
    "                    capture_output=True,\n",
    "                    text=True,\n",
    "                    timeout=20\n",
    "                )\n",
    "                \n",
    "                # Show all output\n",
    "                if result.stdout.strip():\n",
    "                    print(\"üì§ STDOUT:\")\n",
    "                    for line in result.stdout.strip().split('\\\\n'):\n",
    "                        print(f\"   {line}\")\n",
    "                \n",
    "                if result.stderr.strip():\n",
    "                    print(\"\\\\nüì• STDERR (PyTorch Debug Logs):\")\n",
    "                    stderr_lines = [line for line in result.stderr.strip().split('\\\\n') if line.strip()]\n",
    "                    \n",
    "                    if len(stderr_lines) > 10:\n",
    "                        # Show first few and last few lines if output is long\n",
    "                        print(f\"   üìä {len(stderr_lines)} debug lines captured!\")\n",
    "                        print(\"   First 5 lines:\")\n",
    "                        for line in stderr_lines[:5]:\n",
    "                            print(f\"     {line}\")\n",
    "                        print(f\"   ... ({len(stderr_lines) - 10} lines omitted) ...\")\n",
    "                        print(\"   Last 5 lines:\")\n",
    "                        for line in stderr_lines[-5:]:\n",
    "                            print(f\"     {line}\")\n",
    "                    else:\n",
    "                        # Show all lines if output is short\n",
    "                        for line in stderr_lines:\n",
    "                            print(f\"   {line}\")\n",
    "                \n",
    "                # Summary\n",
    "                total_debug_lines = len([line for line in result.stderr.split('\\\\n') if line.strip()])\n",
    "                \n",
    "                print(f\"\\\\nüìä Results:\")\n",
    "                print(f\"   Return code: {result.returncode}\")\n",
    "                print(f\"   Debug lines captured: {total_debug_lines}\")\n",
    "                \n",
    "                if total_debug_lines > 0:\n",
    "                    print(f\"   üéâ SUCCESS: Captured PyTorch debug output!\")\n",
    "                else:\n",
    "                    print(f\"   ‚ÑπÔ∏è  No debug output (expected for minimal scenario)\")\n",
    "                    \n",
    "            except subprocess.TimeoutExpired:\n",
    "                print(\"   ‚è∞ Process timed out\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        # Clean up temporary file\n",
    "        try:\n",
    "            os.unlink(temp_script)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(f\"\\\\nüí° Key Insight: Subprocess Capture Solution\")\n",
    "    print(\"‚úÖ This method works in Jupyter because:\")\n",
    "    print(\"   ‚Ä¢ Runs PyTorch in external process\")\n",
    "    print(\"   ‚Ä¢ Captures ALL output streams\")\n",
    "    print(\"   ‚Ä¢ Shows debug info that Jupyter normally can't see\")\n",
    "    print(\"   ‚Ä¢ Provides complete visibility into compilation process\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Demonstrate the subprocess capture solution\n",
    "debug_success = demonstrate_jupyter_vs_terminal_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6376a49",
   "metadata": {},
   "source": [
    "\n",
    "#### **What the Output Shows:**\n",
    "\n",
    "1. **External Process Capture**: Successfully ran PyTorch code in subprocess and captured ALL output\n",
    "2. **Environment Variables Work**: `TORCH_LOGS` settings produced different amounts of debug output  \n",
    "3. **Visible Differences**: Each scenario showed progressively more compilation information\n",
    "4. **Complete Logging**: Captured both stdout (our prints) and stderr (PyTorch's internal logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18762c6f",
   "metadata": {},
   "source": [
    "## Solution 2: Dynamo Analysis Method\n",
    "\n",
    "**Objective**: Analyze compilation quality and graph breaks using Jupyter-native PyTorch APIs.\n",
    "\n",
    "**When to use**:\n",
    "- Understanding what gets compiled vs. what falls back to eager execution\n",
    "- Identifying graph break causes\n",
    "- Quick compilation analysis without external processes\n",
    "- Daily debugging workflows\n",
    "\n",
    "**Key Advantage**: This method is **100% Jupyter-native** and always works reliably.\n",
    "\n",
    "### Implementation: torch._dynamo.explain()\n",
    "\n",
    "The `torch._dynamo.explain()` function provides structured analysis of the compilation process without requiring external logging capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2ad5cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä SOLUTION 2: DYNAMO ANALYSIS METHOD\n",
      "=============================================\n",
      "‚úÖ This method ALWAYS works in Jupyter!\n",
      "\n",
      "üß™ Model 1: Clean Model\n",
      "   Description: Simple operations that should compile cleanly\n",
      "----------------------------------------\n",
      "üìà Analysis Results:\n",
      "   Graph Count: 1\n",
      "   Graph Break Count: 0\n",
      "   Op Count: 4\n",
      "   ‚úÖ Excellent: Clean compilation, no graph breaks\n",
      "   üéØ Compilation Quality: Optimal\n",
      "\n",
      "üß™ Model 2: Graph Break Model\n",
      "   Description: Contains conditional that causes graph breaks\n",
      "----------------------------------------\n",
      "üìà Analysis Results:\n",
      "   Graph Count: 2\n",
      "   Graph Break Count: 1\n",
      "   Op Count: 2\n",
      "   ‚ö†Ô∏è  Good: Minor graph break, mostly optimized\n",
      "   üéØ Compilation Quality: Good\n",
      "\n",
      "üß™ Model 3: Complex Model\n",
      "   Description: Multiple operations with different optimization potential\n",
      "----------------------------------------\n",
      "üìà Analysis Results:\n",
      "   Graph Count: 1\n",
      "   Graph Break Count: 0\n",
      "   Op Count: 4\n",
      "   ‚úÖ Excellent: Clean compilation, no graph breaks\n",
      "   üéØ Compilation Quality: Optimal\n",
      "\n",
      "üß™ Model 2: Graph Break Model\n",
      "   Description: Contains conditional that causes graph breaks\n",
      "----------------------------------------\n",
      "üìà Analysis Results:\n",
      "   Graph Count: 2\n",
      "   Graph Break Count: 1\n",
      "   Op Count: 2\n",
      "   ‚ö†Ô∏è  Good: Minor graph break, mostly optimized\n",
      "   üéØ Compilation Quality: Good\n",
      "\n",
      "üß™ Model 3: Complex Model\n",
      "   Description: Multiple operations with different optimization potential\n",
      "----------------------------------------\n",
      "üìà Analysis Results:\n",
      "   Graph Count: 1\n",
      "   Graph Break Count: 0\n",
      "   Op Count: 3\n",
      "   ‚úÖ Excellent: Clean compilation, no graph breaks\n",
      "   üéØ Compilation Quality: Optimal\n",
      "\n",
      "üéì Key Benefits of Dynamo Analysis:\n",
      "   ‚úÖ Always works in Jupyter (no external processes)\n",
      "   ‚úÖ Structured, programmatic output\n",
      "   ‚úÖ Perfect for automated analysis\n",
      "   ‚úÖ Identifies specific issues (graph breaks)\n",
      "   ‚úÖ Fast execution (no compilation needed)\n",
      "üìà Analysis Results:\n",
      "   Graph Count: 1\n",
      "   Graph Break Count: 0\n",
      "   Op Count: 3\n",
      "   ‚úÖ Excellent: Clean compilation, no graph breaks\n",
      "   üéØ Compilation Quality: Optimal\n",
      "\n",
      "üéì Key Benefits of Dynamo Analysis:\n",
      "   ‚úÖ Always works in Jupyter (no external processes)\n",
      "   ‚úÖ Structured, programmatic output\n",
      "   ‚úÖ Perfect for automated analysis\n",
      "   ‚úÖ Identifies specific issues (graph breaks)\n",
      "   ‚úÖ Fast execution (no compilation needed)\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_dynamo_analysis():\n",
    "    \"\"\"\n",
    "    Solution 2: Use torch._dynamo.explain() for Jupyter-native debugging\n",
    "    \"\"\"\n",
    "    print(\"üìä SOLUTION 2: DYNAMO ANALYSIS METHOD\")\n",
    "    print(\"=\" * 45)\n",
    "    print(\"‚úÖ This method ALWAYS works in Jupyter!\")\n",
    "    print()\n",
    "    \n",
    "    # Create models with different compilation characteristics\n",
    "    test_models = [\n",
    "        {\n",
    "            \"name\": \"Clean Model\",\n",
    "            \"function\": lambda x: torch.tanh(torch.relu(x) * 2.0 + 1.0),\n",
    "            \"description\": \"Simple operations that should compile cleanly\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Graph Break Model\", \n",
    "            \"function\": lambda x: torch.tanh(x) if x.sum() > 0 else torch.relu(x),\n",
    "            \"description\": \"Contains conditional that causes graph breaks\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Complex Model\",\n",
    "            \"function\": lambda x: torch.mm(torch.relu(x), x.T).sum(dim=1, keepdim=True),\n",
    "            \"description\": \"Multiple operations with different optimization potential\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    test_input = torch.randn(50, 50, device=device)\n",
    "    \n",
    "    for i, model_info in enumerate(test_models, 1):\n",
    "        print(f\"üß™ Model {i}: {model_info['name']}\")\n",
    "        print(f\"   Description: {model_info['description']}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            # Use dynamo.explain() to analyze compilation\n",
    "            explanation = torch._dynamo.explain(model_info['function'])(test_input)\n",
    "            \n",
    "            print(f\"üìà Analysis Results:\")\n",
    "            print(f\"   Graph Count: {explanation.graph_count}\")\n",
    "            print(f\"   Graph Break Count: {explanation.graph_break_count}\")  \n",
    "            print(f\"   Op Count: {explanation.op_count}\")\n",
    "            \n",
    "            # Interpret results\n",
    "            if explanation.graph_break_count == 0:\n",
    "                print(f\"   ‚úÖ Excellent: Clean compilation, no graph breaks\")\n",
    "                quality = \"Optimal\"\n",
    "            elif explanation.graph_break_count == 1:\n",
    "                print(f\"   ‚ö†Ô∏è  Good: Minor graph break, mostly optimized\")\n",
    "                quality = \"Good\"\n",
    "            else:\n",
    "                print(f\"   ‚ùå Poor: Multiple graph breaks, limited optimization\")\n",
    "                quality = \"Needs Work\"\n",
    "            \n",
    "            print(f\"   üéØ Compilation Quality: {quality}\")\n",
    "            \n",
    "            # Show graph break details if available\n",
    "            if hasattr(explanation, 'graph_breaks') and explanation.graph_breaks:\n",
    "                print(f\"   üîç Graph Break Reasons:\")\n",
    "                for j, break_reason in enumerate(explanation.graph_breaks[:2], 1):\n",
    "                    # Truncate long break reasons\n",
    "                    reason_str = str(break_reason)[:80] + \"...\" if len(str(break_reason)) > 80 else str(break_reason)\n",
    "                    print(f\"      {j}. {reason_str}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Analysis failed: {str(e)[:60]}...\")\n",
    "            quality = \"Failed\"\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"üéì Key Benefits of Dynamo Analysis:\")\n",
    "    print(\"   ‚úÖ Always works in Jupyter (no external processes)\")\n",
    "    print(\"   ‚úÖ Structured, programmatic output\") \n",
    "    print(\"   ‚úÖ Perfect for automated analysis\")\n",
    "    print(\"   ‚úÖ Identifies specific issues (graph breaks)\")\n",
    "    print(\"   ‚úÖ Fast execution (no compilation needed)\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Execute dynamo analysis demonstration  \n",
    "dynamo_success = demonstrate_dynamo_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d02a7ad",
   "metadata": {},
   "source": [
    "## Solution 3: Artifact Inspection Method\n",
    "\n",
    "**Objective**: Examine generated Triton kernels and compilation artifacts to understand deep optimizations.\n",
    "\n",
    "**When to use**:\n",
    "- Learning how PyTorch optimizes specific operations\n",
    "- Understanding kernel fusion strategies  \n",
    "- Educational exploration of generated code\n",
    "- Deep performance analysis\n",
    "\n",
    "**Key Value**: See the **actual optimized code** that PyTorch generates, providing insights into compilation strategies.\n",
    "\n",
    "### **Production TorchInductor Debugger Architecture**\n",
    "\n",
    "This method explores the file system locations where PyTorch stores generated kernels and compilation artifacts, providing direct access to optimized code. To this end, we'll implement a **production-ready solution** that completely eliminates directory conflicts. This solution addresses the core problems:\n",
    "\n",
    "### **Design Goals**\n",
    "- **Isolated directories** for each debugging session\n",
    "- **Automatic artifact capture** from TorchInductor's default locations  \n",
    "- **Organized file structure** with kernels and binaries separated\n",
    "- **Built-in analysis tools** for understanding generated code\n",
    "- **Clean session management** with context managers\n",
    "\n",
    "### **Architecture Overview**\n",
    "\n",
    "The `ProductionTorchInductorDebugger` class provides:\n",
    "\n",
    "1. **Session Management**: Each debug session gets a unique directory\n",
    "2. **Artifact Capture**: Automatically finds and copies TorchInductor artifacts  \n",
    "3. **File Organization**: Separates Python kernels from compiled binaries\n",
    "4. **Analysis Tools**: Built-in kernel inspection and optimization detection\n",
    "5. **Cleanup Control**: Choose whether to preserve or remove artifacts\n",
    "\n",
    "Let's implement this step by step, starting with the core class structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2811584c",
   "metadata": {},
   "source": [
    "#### **Step 1: Core Class Structure & Session Management**\n",
    "\n",
    "The foundation of our solution is a context manager that creates isolated directories and handles cleanup automatically.\n",
    "\n",
    "#### **Step 2: Model Compilation & Artifact Capture**\n",
    "\n",
    "This is the core functionality that compiles models and captures their generated artifacts. The process:\n",
    "\n",
    "1. **Clear previous artifacts** to ensure we only capture new ones\n",
    "2. **Compile the model** with optimized settings to force artifact generation  \n",
    "3. **Execute the model** to trigger actual code generation\n",
    "4. **Capture artifacts** from TorchInductor's default location into our isolated directory\n",
    "\n",
    "#### **Step 3: Artifact Organization & File Management**\n",
    "\n",
    "This section handles the intelligent organization of captured artifacts. The system:\n",
    "\n",
    "1. **Scans multiple file types**: Python kernels (`.py`), CUDA binaries (`.cubin`), PTX assembly (`.ptx`)\n",
    "2. **Filters substantial files**: Ignores tiny or empty files that aren't useful for analysis\n",
    "3. **Organizes by type**: Separates kernels and binaries into different directories\n",
    "4. **Creates descriptive names**: Renames files with sequential numbering for easy identification\n",
    "\n",
    "#### **Step 4: Intelligent Artifact Analysis**\n",
    "\n",
    "The analysis engine examines captured artifacts to provide insights into TorchInductor's optimizations. It provides:\n",
    "\n",
    "1. **Kernel inspection**: Finds and analyzes the largest/most complex generated kernels\n",
    "2. **Source code preview**: Shows the actual generated Triton code  \n",
    "3. **Optimization detection**: Identifies patterns like operation fusion, autotuning, memory optimization\n",
    "4. **Performance insights**: Counts key optimization indicators to understand what PyTorch optimized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5e0000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionTorchInductorDebugger:\n",
    "    \"\"\"\n",
    "    Production-ready TorchInductor artifact debugger\n",
    "    \n",
    "    Solves the directory conflict problem by:\n",
    "    1. Creating isolated directories for each debug session\n",
    "    2. Automatically capturing artifacts from TorchInductor\n",
    "    3. Providing clean analysis tools\n",
    "    4. Managing cleanup appropriately\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, session_name: str = None, auto_cleanup: bool = False):\n",
    "        self.session_name = session_name or f\"debug_{int(time.time())}\"\n",
    "        self.auto_cleanup = auto_cleanup\n",
    "        self.custom_dir = None\n",
    "        self.artifacts_captured = []\n",
    "        \n",
    "    def __enter__(self):\n",
    "        # Create clean custom directory\n",
    "        self.custom_dir = tempfile.mkdtemp(prefix=f\"torch_debug_{self.session_name}_\")\n",
    "        print(f\"üîß TorchInductor Debug Session: '{self.session_name}'\")\n",
    "        print(f\"üìÅ Artifact directory: {self.custom_dir}\")\n",
    "        return self\n",
    "        \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self.auto_cleanup and self.custom_dir:\n",
    "            shutil.rmtree(self.custom_dir, ignore_errors=True)\n",
    "            print(f\"üßπ Cleaned up debug directory\")\n",
    "        else:\n",
    "            print(f\"üíæ Debug artifacts preserved at: {self.custom_dir}\")\n",
    "\n",
    "    #### **Step 2: Model Compilation & Artifact Capture\n",
    "    def compile_and_capture_artifacts(self, model_fn, test_input, **compile_kwargs):\n",
    "        \"\"\"\n",
    "        Compile a model and capture its artifacts in our custom directory\n",
    "        \n",
    "        Args:\n",
    "            model_fn: Function to compile\n",
    "            test_input: Input tensor for testing\n",
    "            **compile_kwargs: Additional arguments for torch.compile()\n",
    "        \"\"\"\n",
    "        # Default compilation settings that encourage artifact generation\n",
    "        default_kwargs = {\n",
    "            'backend': 'inductor',\n",
    "            'mode': 'max-autotune'\n",
    "        }\n",
    "        default_kwargs.update(compile_kwargs)\n",
    "        \n",
    "        # Get default TorchInductor location\n",
    "        user_name = os.getenv('USER', 'user')\n",
    "        default_location = f\"/tmp/torchinductor_{user_name}\"\n",
    "        \n",
    "        # Clear previous artifacts to ensure we capture new ones\n",
    "        if os.path.exists(default_location):\n",
    "            print(f\"üßπ Clearing previous artifacts...\")\n",
    "            subprocess.run(f\"rm -rf {default_location}/*\", shell=True, capture_output=True)\n",
    "        \n",
    "        # Reset dynamo and compile\n",
    "        torch._dynamo.reset()\n",
    "        torch._inductor.codecache.FxGraphCache.clear()\n",
    "        print(f\"üîÑ Compiling model with {default_kwargs}...\")\n",
    "        \n",
    "        compiled_model = torch.compile(model_fn, **default_kwargs)\n",
    "        result = compiled_model(test_input)\n",
    "        \n",
    "        print(f\"‚úÖ Model compiled and executed (output shape: {result.shape})\")\n",
    "        \n",
    "        # Capture artifacts\n",
    "        time.sleep(0.5)  # Allow file system to sync\n",
    "        self._capture_artifacts_from_default_location(default_location)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    # Step 3: Artifact Organization & File Management\n",
    "    def _capture_artifacts_from_default_location(self, default_location):\n",
    "        \"\"\"Copy artifacts from default location to our custom directory\"\"\"\n",
    "        if not os.path.exists(default_location):\n",
    "            print(f\"‚ö†Ô∏è  Default location not found: {default_location}\")\n",
    "            return\n",
    "        \n",
    "        # Find all artifact files\n",
    "        artifact_patterns = [\n",
    "            \"**/*.py\",     # Python kernels\n",
    "            \"**/*.cubin\",  # CUDA binaries  \n",
    "            \"**/*.ptx\",    # PTX assembly\n",
    "        ]\n",
    "        \n",
    "        all_artifacts = []\n",
    "        for pattern in artifact_patterns:\n",
    "            matches = glob.glob(f\"{default_location}/{pattern}\", recursive=True)\n",
    "            all_artifacts.extend(matches)\n",
    "        \n",
    "        # Filter for substantial files\n",
    "        substantial_artifacts = []\n",
    "        for artifact in all_artifacts:\n",
    "            try:\n",
    "                size = os.path.getsize(artifact)\n",
    "                if size > 100:  # Skip tiny files\n",
    "                    substantial_artifacts.append((artifact, size))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if not substantial_artifacts:\n",
    "            print(f\"‚ö†Ô∏è  No substantial artifacts found in {default_location}\")\n",
    "            return\n",
    "        \n",
    "        # Copy to our custom directory with organized structure\n",
    "        print(f\"üìÅ Capturing {len(substantial_artifacts)} artifacts...\")\n",
    "        \n",
    "        kernels_dir = os.path.join(self.custom_dir, \"kernels\")\n",
    "        binaries_dir = os.path.join(self.custom_dir, \"binaries\")\n",
    "        os.makedirs(kernels_dir, exist_ok=True)\n",
    "        os.makedirs(binaries_dir, exist_ok=True)\n",
    "        \n",
    "        for src_file, size in substantial_artifacts:\n",
    "            # Organize by file type\n",
    "            if src_file.endswith('.py'):\n",
    "                dst_dir = kernels_dir\n",
    "                prefix = \"kernel\"\n",
    "            else:\n",
    "                dst_dir = binaries_dir  \n",
    "                prefix = \"binary\"\n",
    "            \n",
    "            # Create descriptive filename\n",
    "            original_name = os.path.basename(src_file)\n",
    "            dst_file = os.path.join(dst_dir, f\"{prefix}_{len(self.artifacts_captured)+1}_{original_name}\")\n",
    "            \n",
    "            try:\n",
    "                shutil.copy2(src_file, dst_file)\n",
    "                self.artifacts_captured.append((dst_file, size))\n",
    "                print(f\"   ‚úÖ {os.path.splitext(original_name)[1]}: {original_name} ({size} bytes)\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è  Failed to copy {original_name}: {e}\")\n",
    "\n",
    "    # Step 4: Artifact Analysis\n",
    "    def analyze_artifacts(self):\n",
    "        \"\"\"Analyze captured artifacts\"\"\"\n",
    "        if not self.artifacts_captured:\n",
    "            print(\"‚ùå No artifacts to analyze\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"\\nüîç ARTIFACT ANALYSIS\")\n",
    "        print(\"=\" * 25)\n",
    "        print(f\"Total artifacts captured: {len(self.artifacts_captured)}\")\n",
    "        \n",
    "        # Find largest Python kernel\n",
    "        py_artifacts = [(f, s) for f, s in self.artifacts_captured if f.endswith('.py')]\n",
    "        \n",
    "        if not py_artifacts:\n",
    "            print(\"‚ö†Ô∏è  No Python kernels found\")\n",
    "            return None\n",
    "        \n",
    "        largest_kernel, largest_size = max(py_artifacts, key=lambda x: x[1])\n",
    "        \n",
    "        try:\n",
    "            with open(largest_kernel, 'r') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            lines = content.split('\\n')\n",
    "            print(f\"\\nüìÑ Largest Kernel Analysis:\")\n",
    "            print(f\"   File: {os.path.basename(largest_kernel)}\")\n",
    "            print(f\"   Size: {largest_size} bytes\")\n",
    "            print(f\"   Lines: {len(lines)}\")\n",
    "            \n",
    "            # Show preview\n",
    "            print(f\"\\nüìù Source Preview (first 8 lines):\")\n",
    "            for i, line in enumerate(lines[:8], 1):\n",
    "                display_line = line[:70] + \"...\" if len(line) > 70 else line\n",
    "                print(f\"   {i:2d}: {display_line}\")\n",
    "            \n",
    "            # Pattern analysis\n",
    "            patterns = {\n",
    "                'Triton kernels (@triton.jit)': content.count('@triton.jit'),\n",
    "                'Memory loads (tl.load)': content.count('tl.load'),\n",
    "                'Memory stores (tl.store)': content.count('tl.store'),\n",
    "                'Operation fusion (fused)': content.count('fused'),\n",
    "                'Autotuning (autotuned)': content.count('autotuned'),\n",
    "                'Grid computations (tl.program_id)': content.count('tl.program_id'),\n",
    "            }\n",
    "            \n",
    "            detected_optimizations = {k: v for k, v in patterns.items() if v > 0}\n",
    "            \n",
    "            if detected_optimizations:\n",
    "                print(f\"\\n‚ö° Detected Optimizations:\")\n",
    "                for optimization, count in detected_optimizations.items():\n",
    "                    print(f\"   ‚úÖ {optimization}: {count}\")\n",
    "            else:\n",
    "                print(f\"\\n   ‚ÑπÔ∏è  No obvious optimization patterns detected\")\n",
    "            \n",
    "            return content\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Could not analyze kernel: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_artifact_summary(self):\n",
    "        \"\"\"Get summary of captured artifacts\"\"\"\n",
    "        if not self.artifacts_captured:\n",
    "            return \"No artifacts captured\"\n",
    "        \n",
    "        py_files = sum(1 for f, _ in self.artifacts_captured if f.endswith('.py'))\n",
    "        other_files = len(self.artifacts_captured) - py_files\n",
    "        total_size = sum(s for _, s in self.artifacts_captured)\n",
    "        \n",
    "        return f\"Captured: {py_files} Python kernels, {other_files} other files ({total_size:,} bytes total)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e4735d",
   "metadata": {},
   "source": [
    "### **Production Demonstration**\n",
    "\n",
    "Now let's demonstrate the complete solution in action. This demonstration will:\n",
    "\n",
    "1. **Create a realistic model** with multiple optimization opportunities\n",
    "2. **Use the debugger** to compile and capture artifacts in an isolated directory\n",
    "3. **Analyze the results** to see what optimizations TorchInductor applied\n",
    "4. **Show the clean directory structure** with organized artifacts\n",
    "\n",
    "This proves the solution works end-to-end and eliminates directory conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39a640e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè≠ ProductionTorchInductorDebugger loaded!\n",
      "   Clean, isolated, production-ready artifact debugging\n",
      "üöÄ PRODUCTION TORCHINDEUCTOR DEBUGGING\n",
      "=============================================\n",
      "üîß TorchInductor Debug Session: 'production_demo'\n",
      "üìÅ Artifact directory: /tmp/torch_debug_production_demo_tqibp5sv\n",
      "üßπ Clearing previous artifacts...\n",
      "üîÑ Compiling model with {'backend': 'inductor', 'mode': 'max-autotune'}...\n",
      "‚úÖ Model compiled and executed (output shape: torch.Size([1500]))\n",
      "üìÅ Capturing 18 artifacts...\n",
      "   ‚úÖ .py: cxcnucfdc3orragmmwk5y2k3bkdrwpt23i3z4bi44pbxrmqhv3d6.py (2973 bytes)\n",
      "   ‚úÖ .py: chblowbn2shg4mdx6d66zzg7ccs4u5b2txxlvejbstoy67v2fddf.py (6551 bytes)\n",
      "   ‚úÖ .cubin: triton_red_fused_add_mul_relu_sum_tanh_0.cubin (9328 bytes)\n",
      "   ‚úÖ .cubin: triton_red_fused_add_mul_relu_sum_tanh_0.cubin (23984 bytes)\n",
      "   ‚úÖ .cubin: triton_red_fused_add_mul_relu_sum_tanh_0.cubin (16176 bytes)\n",
      "   ‚úÖ .cubin: triton_red_fused_add_mul_relu_sum_tanh_0.cubin (12464 bytes)\n",
      "   ‚úÖ .cubin: triton_red_fused_add_mul_relu_sum_tanh_0.cubin (16304 bytes)\n",
      "   ‚úÖ .cubin: triton_red_fused_add_mul_relu_sum_tanh_0.cubin (8944 bytes)\n",
      "   ‚úÖ .cubin: triton_red_fused_add_mul_relu_sum_tanh_0.cubin (10672 bytes)\n",
      "   ‚úÖ .cubin: triton_red_fused_add_mul_relu_sum_tanh_0.cubin (20400 bytes)\n",
      "   ‚úÖ .ptx: triton_red_fused_add_mul_relu_sum_tanh_0.ptx (12351 bytes)\n",
      "   ‚úÖ .ptx: triton_red_fused_add_mul_relu_sum_tanh_0.ptx (25646 bytes)\n",
      "   ‚úÖ .ptx: triton_red_fused_add_mul_relu_sum_tanh_0.ptx (18744 bytes)\n",
      "   ‚úÖ .ptx: triton_red_fused_add_mul_relu_sum_tanh_0.ptx (15634 bytes)\n",
      "   ‚úÖ .ptx: triton_red_fused_add_mul_relu_sum_tanh_0.ptx (19535 bytes)\n",
      "   ‚úÖ .ptx: triton_red_fused_add_mul_relu_sum_tanh_0.ptx (11884 bytes)\n",
      "   ‚úÖ .ptx: triton_red_fused_add_mul_relu_sum_tanh_0.ptx (14047 bytes)\n",
      "   ‚úÖ .ptx: triton_red_fused_add_mul_relu_sum_tanh_0.ptx (20018 bytes)\n",
      "\n",
      "üìä Captured: 2 Python kernels, 16 other files (265,655 bytes total)\n",
      "\n",
      "üîç ARTIFACT ANALYSIS\n",
      "=========================\n",
      "Total artifacts captured: 18\n",
      "\n",
      "üìÑ Largest Kernel Analysis:\n",
      "   File: kernel_2_chblowbn2shg4mdx6d66zzg7ccs4u5b2txxlvejbstoy67v2fddf.py\n",
      "   Size: 6551 bytes\n",
      "   Lines: 128\n",
      "\n",
      "üìù Source Preview (first 8 lines):\n",
      "    1: # AOT ID: ['65_inference']\n",
      "    2: from ctypes import c_void_p, c_long, c_int\n",
      "    3: import torch\n",
      "    4: import math\n",
      "    5: import random\n",
      "    6: import os\n",
      "    7: import tempfile\n",
      "    8: from math import inf, nan\n",
      "\n",
      "‚ö° Detected Optimizations:\n",
      "   ‚úÖ Triton kernels (@triton.jit): 1\n",
      "   ‚úÖ Memory loads (tl.load): 1\n",
      "   ‚úÖ Memory stores (tl.store): 1\n",
      "   ‚úÖ Operation fusion (fused): 5\n",
      "   ‚úÖ Grid computations (tl.program_id): 1\n",
      "\n",
      "‚úÖ SUCCESS: TorchInductor artifacts captured and analyzed!\n",
      "üìÇ Artifacts location: /tmp/torch_debug_production_demo_tqibp5sv\n",
      "üíæ Debug artifacts preserved at: /tmp/torch_debug_production_demo_tqibp5sv\n",
      "\n",
      "üéâ COMPLETE SUCCESS!\n",
      "‚úÖ Clean directory isolation achieved\n",
      "‚úÖ Artifacts captured and organized\n",
      "‚úÖ No conflicts with other processes\n",
      "‚úÖ Production-ready debugging solution verified!\n"
     ]
    }
   ],
   "source": [
    "def demo_production_debugging():\n",
    "    \"\"\"Demonstrate the production-ready debugging solution\"\"\"\n",
    "    print(\"üöÄ PRODUCTION TORCHINDEUCTOR DEBUGGING\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    with ProductionTorchInductorDebugger(\"production_demo\", auto_cleanup=False) as debugger:\n",
    "        \n",
    "        def optimizable_model(x):\n",
    "            \"\"\"Model with multiple optimization opportunities\"\"\"\n",
    "            # Operations that should trigger kernel generation\n",
    "            y = torch.relu(x)              # Activation\n",
    "            z = y * 3.0 + 0.5             # Fused multiply-add\n",
    "            w = torch.tanh(z)              # Another activation\n",
    "            return w.sum(dim=0, keepdim=True).expand_as(x)  # Reduction + broadcast\n",
    "        \n",
    "        # Test with substantial input\n",
    "        test_input = torch.randn(1500, device=device)\n",
    "        \n",
    "        # Compile and capture artifacts\n",
    "        result = debugger.compile_and_capture_artifacts(\n",
    "            optimizable_model, \n",
    "            test_input,\n",
    "            mode=\"max-autotune\"  # Force aggressive optimization\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüìä {debugger.get_artifact_summary()}\")\n",
    "        \n",
    "        # Analyze artifacts\n",
    "        kernel_content = debugger.analyze_artifacts()\n",
    "        \n",
    "        if kernel_content:\n",
    "            print(f\"\\n‚úÖ SUCCESS: TorchInductor artifacts captured and analyzed!\")\n",
    "            print(f\"üìÇ Artifacts location: {debugger.custom_dir}\")\n",
    "        else:\n",
    "            print(f\"\\n  Limited success - check directory manually\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "print(\"üè≠ ProductionTorchInductorDebugger loaded!\")\n",
    "print(\"   Clean, isolated, production-ready artifact debugging\")\n",
    "\n",
    "# Execute the production debugging demonstration\n",
    "success = demo_production_debugging()\n",
    "\n",
    "if success:\n",
    "    print(f\"\\nüéâ COMPLETE SUCCESS!\")\n",
    "    print(f\"‚úÖ Clean directory isolation achieved\")\n",
    "    print(f\"‚úÖ Artifacts captured and organized\") \n",
    "    print(f\"‚úÖ No conflicts with other processes\")\n",
    "    print(f\"‚úÖ Production-ready debugging solution verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0affdd",
   "metadata": {},
   "source": [
    "#### **Clean TorchInductor Artifact Debugging**\n",
    "\n",
    "##### **Problem Solved** \n",
    "\n",
    "The original artifact inspection used shared TorchInductor directories like `/tmp/torchinductor_user` which caused:\n",
    "- **Conflicts** with other PyTorch processes\n",
    "- **Mixed artifacts** from different debugging sessions  \n",
    "- **Confusion** about which files belong to which experiment\n",
    "\n",
    "##### **Production Solution** \n",
    "\n",
    "```python\n",
    "# Clean, isolated debugging session\n",
    "with ProductionTorchInductorDebugger(\"my_experiment\", auto_cleanup=False) as debugger:\n",
    "    \n",
    "    def my_model(x):\n",
    "        return torch.relu(x * 2.0 + 1.0)\n",
    "    \n",
    "    # Compile and automatically capture artifacts in clean directory\n",
    "    result = debugger.compile_and_capture_artifacts(my_model, test_input)\n",
    "    \n",
    "    # Analyze captured artifacts\n",
    "    debugger.analyze_artifacts()\n",
    "    \n",
    "    # Get summary: \"Captured: 2 Python kernels, 1 other files (8,715 bytes total)\"\n",
    "    print(debugger.get_artifact_summary())\n",
    "\n",
    "# Artifacts preserved in organized directory structure:\n",
    "# /tmp/torch_debug_my_experiment_xyz/\n",
    "#   ‚îú‚îÄ‚îÄ kernels/\n",
    "#   ‚îÇ   ‚îú‚îÄ‚îÄ kernel_1_optimized_relu.py\n",
    "#   ‚îÇ   ‚îî‚îÄ‚îÄ kernel_2_fused_ops.py  \n",
    "#   ‚îî‚îÄ‚îÄ binaries/\n",
    "#       ‚îî‚îÄ‚îÄ binary_1_compiled.cubin\n",
    "```\n",
    "\n",
    "##### **Key Benefits** \n",
    "\n",
    "| Feature | Benefit |\n",
    "|---------|---------|\n",
    "| **Isolated Directories** | No conflicts with other processes |\n",
    "| **Organized Structure** | kernels/ and binaries/ subdirectories |\n",
    "| **Session Naming** | Easy to identify different experiments |\n",
    "| **Flexible Cleanup** | Choose to preserve or auto-remove |\n",
    "| **Built-in Analysis** | Automatic kernel inspection and pattern detection |\n",
    "| **Fresh Compilation** | Clears cache to ensure new artifacts |\n",
    "\n",
    "##### **Usage Patterns** \n",
    "\n",
    "**Quick Experiment:**\n",
    "```python\n",
    "with ProductionTorchInductorDebugger(\"quick_test\", auto_cleanup=True) as debug:\n",
    "    result = debug.compile_and_capture_artifacts(model, input)\n",
    "    # Auto-cleanup on exit\n",
    "```\n",
    "\n",
    "**Detailed Analysis:**\n",
    "```python  \n",
    "with ProductionTorchInductorDebugger(\"performance_study\", auto_cleanup=False) as debug:\n",
    "    result = debug.compile_and_capture_artifacts(model, input, mode=\"max-autotune\")\n",
    "    kernel_code = debug.analyze_artifacts()  # See actual generated code\n",
    "    # Artifacts preserved for later inspection\n",
    "```\n",
    "\n",
    "This approach provides **production-ready debugging** with complete isolation and organization!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50c9e17",
   "metadata": {},
   "source": [
    "## Jupyter Debugging Toolkit Summary\n",
    "\n",
    "We've explored **three focused solutions** for debugging `torch.compile()` in Jupyter notebooks. Each approach addresses the fundamental logging issue while providing unique insights.\n",
    "\n",
    "**Solution Comparison Matrix**\n",
    "\n",
    "| Solution | Jupyter Native | Setup Complexity | Information Depth | Best Use Case |\n",
    "|----------|----------------|------------------|-------------------|---------------|\n",
    "| **1. Subprocess Capture** | ‚ö†Ô∏è Hybrid | üî¥ High | üî• Maximum | Complete PyTorch logs |\n",
    "| **2. Dynamo Analysis** | ‚úÖ Yes | üü¢ Low | üìä High | Daily debugging workflow |\n",
    "| **3. Artifact Inspection** | ‚úÖ Yes | üü° Medium | üî¨ Deep | Understanding optimizations |\n",
    "\n",
    "**Recommended Debugging Workflow**\n",
    "\n",
    "For most Jupyter debugging scenarios, use this **focused approach**:\n",
    "\n",
    "#### **Primary Tools** (Use these most often)\n",
    "1. **Dynamo Analysis** - Check for graph breaks and compilation quality\n",
    "2. **Artifact Inspection** - Examine generated kernels for optimization insights\n",
    "\n",
    "**Complete Investigation:** (When you need everything)\n",
    "\n",
    "3. **Subprocess Capture** - See complete PyTorch logs when environment variables are critical\n",
    "\n",
    "**Key Insights Achieved**\n",
    "\n",
    "**Problem Understood**: PyTorch logs work but aren't visible in Jupyter  \n",
    "**Focused Solutions**: Three practical methods that work reliably  \n",
    "**Preferred Workflow**: Dynamo Analysis + Artifact Inspection for most needs  \n",
    "**Production Ready**: Methods suitable for real development workflows  \n",
    "\n",
    "\n",
    "You now have a **streamlined debugging toolkit** focused on the most effective methods:\n",
    "\n",
    "- **Dynamo Analysis**: Your daily go-to for quick compilation assessment\n",
    "- **Artifact Inspection**: Your deep-dive tool for understanding optimizations  \n",
    "- **Subprocess Capture**: Your comprehensive tool when you need complete logs\n",
    "\n",
    "This focused foundation enables efficient debugging and prepares you for advanced optimization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e7d21d",
   "metadata": {},
   "source": [
    "# Systematic Kernel Exploration and Analysis\n",
    "\n",
    "Beyond environment variables, `torch.compile()` generates tangible artifacts that you can examine directly. Understanding these files provides deeper insights into PyTorch's optimization strategies and helps debug performance issues.\n",
    "\n",
    "### What We'll Explore\n",
    "\n",
    "1. **Kernel Storage Locations**: Where PyTorch stores generated artifacts\n",
    "2. **File Type Analysis**: Understanding different artifact categories  \n",
    "3. **Python/Triton Kernel Analysis**: Examining the actual generated code\n",
    "4. **Performance Artifacts**: Binary kernels and metadata analysis\n",
    "\n",
    "### Expected Locations\n",
    "\n",
    "- **Primary Cache**: `/tmp/torchinductor_<username>/` - Main kernel storage\n",
    "- **Debug Traces**: `./torch_compile_debug/` - Created when `TORCH_COMPILE_DEBUG=1`\n",
    "- **File Types**: `.py` (kernel source), `.so` (compiled libraries), `.json` (metadata)\n",
    "\n",
    "Let's systematically explore these artifacts to understand what PyTorch generates during compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d8e6c88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Setting up kernel exploration...\n",
      "   Required imports: os, glob, json, pathlib\n",
      "   Ready to analyze compilation artifacts\n"
     ]
    }
   ],
   "source": [
    "# Additional imports for kernel exploration\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup for kernel exploration\n",
    "print(\"üîß Setting up kernel exploration...\")\n",
    "print(\"   Required imports: os, glob, json, pathlib\")\n",
    "print(\"   Ready to analyze compilation artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9c6a46c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ ProductionTorchInductorDebugger class defined and ready for use!\n",
      "\n",
      "üîß Setting up environment for kernel exploration:\n",
      "   TORCH_COMPILE_DEBUG = 1\n",
      "   TORCHINDUCTOR_CACHE_DIR = /tmp/torchinductor_alibina (existing)\n",
      "   TORCH_LOGS = +dynamo,+inductor,+graph_code\n",
      "   TORCHINDUCTOR_VERBOSE = 1\n",
      "   TORCHINDUCTOR_MAX_AUTOTUNE = 1\n",
      "   TORCHINDUCTOR_COORDINATE_DESCENT_TUNING = 1\n",
      "   TORCHINDUCTOR_FREEZING = 1\n",
      "   TORCHINDUCTOR_FX_GRAPH_CACHE = 1\n",
      "   TORCHINDUCTOR_FALLBACK_RANDOM = 0\n",
      "üöÄ ProductionTorchInductorDebugger initialized\n",
      "   Device: cuda\n",
      "   Debug artifacts: True\n",
      "   Cache cleanup: True\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import shutil\n",
    "import subprocess\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import statistics\n",
    "\n",
    "@dataclass\n",
    "class KernelArtifact:\n",
    "    \"\"\"Data class for storing kernel artifact information\"\"\"\n",
    "    path: str\n",
    "    file_type: str\n",
    "    size_bytes: int\n",
    "    creation_time: float\n",
    "    content_summary: str\n",
    "\n",
    "class ProductionTorchInductorDebugger:\n",
    "    \"\"\"\n",
    "    Production-ready debugger for systematic kernel exploration and analysis.\n",
    "    Integrates with artifact inspection for comprehensive debugging workflows.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 device: str = None,\n",
    "                 cache_cleanup: bool = True,\n",
    "                 debug_artifacts: bool = True,\n",
    "                 verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the production debugger with optimal settings.\n",
    "        \n",
    "        Args:\n",
    "            device: Target device ('cuda' or 'cpu')\n",
    "            cache_cleanup: Whether to clean cache between runs\n",
    "            debug_artifacts: Whether to enable artifact generation\n",
    "            verbose: Whether to print detailed information\n",
    "        \"\"\"\n",
    "        # Device setup\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.cache_cleanup = cache_cleanup\n",
    "        self.debug_artifacts = debug_artifacts\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Environment setup\n",
    "        self._setup_environment()\n",
    "        \n",
    "        # Storage for artifacts and analysis\n",
    "        self.artifacts: Dict[str, List[KernelArtifact]] = {}\n",
    "        self.compilation_logs: List[str] = []\n",
    "        self.kernel_analysis: Dict[str, Any] = {}\n",
    "        \n",
    "        # Benchmarking parameters\n",
    "        self.warmup_trials = 3\n",
    "        self.num_trials = 10\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"üöÄ ProductionTorchInductorDebugger initialized\")\n",
    "            print(f\"   Device: {self.device}\")\n",
    "            print(f\"   Debug artifacts: {self.debug_artifacts}\")\n",
    "            print(f\"   Cache cleanup: {self.cache_cleanup}\")\n",
    "    \n",
    "    def _setup_environment(self):\n",
    "        \"\"\"Setup optimal environment variables for kernel exploration\"\"\"\n",
    "        env_settings = {\n",
    "            # Core debugging settings\n",
    "            'TORCH_COMPILE_DEBUG': '1' if self.debug_artifacts else '0',\n",
    "            'TORCHINDUCTOR_CACHE_DIR': None,  # Use default location\n",
    "            \n",
    "            # Performance and verbosity\n",
    "            'TORCH_LOGS': '+dynamo,+inductor,+graph_code',\n",
    "            'TORCHINDUCTOR_VERBOSE': '1',\n",
    "            \n",
    "            # Kernel optimization settings\n",
    "            'TORCHINDUCTOR_MAX_AUTOTUNE': '1',\n",
    "            'TORCHINDUCTOR_COORDINATE_DESCENT_TUNING': '1',\n",
    "            \n",
    "            # Memory and compilation settings\n",
    "            'TORCHINDUCTOR_FREEZING': '1',\n",
    "            'TORCHINDUCTOR_FX_GRAPH_CACHE': '1',\n",
    "            \n",
    "            # Error handling\n",
    "            'TORCHINDUCTOR_FALLBACK_RANDOM': '0',\n",
    "        }\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"\\nüîß Setting up environment for kernel exploration:\")\n",
    "        \n",
    "        for key, value in env_settings.items():\n",
    "            if value is not None:\n",
    "                os.environ[key] = str(value)\n",
    "                if self.verbose:\n",
    "                    print(f\"   {key} = {value}\")\n",
    "            elif key in os.environ:\n",
    "                if self.verbose:\n",
    "                    print(f\"   {key} = {os.environ[key]} (existing)\")\n",
    "    \n",
    "    def explore_compilation_artifacts(self, model, input_tensor, model_name: str = \"model\"):\n",
    "        \"\"\"\n",
    "        Comprehensive exploration of compilation artifacts with analysis.\n",
    "        \n",
    "        Args:\n",
    "            model: PyTorch model to compile and analyze\n",
    "            input_tensor: Input tensor for the model\n",
    "            model_name: Name for identification in artifacts\n",
    "        \n",
    "        Returns:\n",
    "            Dict containing comprehensive artifact analysis\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"\\nüîç SYSTEMATIC KERNEL EXPLORATION: {model_name}\")\n",
    "            print(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Clean environment if requested\n",
    "        if self.cache_cleanup:\n",
    "            self._clean_cache()\n",
    "        \n",
    "        # Step 2: Locate initial storage locations\n",
    "        initial_locations = self._locate_kernel_storage()\n",
    "        \n",
    "        # Step 3: Compile model and capture artifacts\n",
    "        compiled_model, compilation_info = self._compile_with_artifact_capture(\n",
    "            model, input_tensor, model_name\n",
    "        )\n",
    "        \n",
    "        # Step 4: Discover and analyze artifacts\n",
    "        artifact_analysis = self._discover_and_analyze_artifacts(model_name)\n",
    "        \n",
    "        # Step 5: Kernel-specific analysis\n",
    "        kernel_analysis = self._analyze_kernels(model_name)\n",
    "        \n",
    "        # Step 6: Performance correlation analysis\n",
    "        performance_analysis = self._correlate_artifacts_with_performance(\n",
    "            compiled_model, input_tensor, model_name\n",
    "        )\n",
    "        \n",
    "        # Compile comprehensive results\n",
    "        results = {\n",
    "            'model_name': model_name,\n",
    "            'device': self.device,\n",
    "            'initial_locations': initial_locations,\n",
    "            'compilation_info': compilation_info,\n",
    "            'artifact_analysis': artifact_analysis,\n",
    "            'kernel_analysis': kernel_analysis,\n",
    "            'performance_analysis': performance_analysis,\n",
    "            'environment_snapshot': self._capture_environment_snapshot()\n",
    "        }\n",
    "        \n",
    "        if self.verbose:\n",
    "            self._print_exploration_summary(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _locate_kernel_storage(self) -> Dict[str, str]:\n",
    "        \"\"\"Locate all kernel storage directories\"\"\"\n",
    "        locations = {}\n",
    "        \n",
    "        # Primary cache location\n",
    "        user_name = os.getenv('USER', 'user')\n",
    "        primary_cache = f\"/tmp/torchinductor_{user_name}\"\n",
    "        \n",
    "        # Debug traces location\n",
    "        debug_dir = \"./torch_compile_debug\"\n",
    "        \n",
    "        # Additional possible locations\n",
    "        temp_dirs = [\n",
    "            tempfile.gettempdir() + \"/torchinductor*\",\n",
    "            os.path.expanduser(\"~/.cache/torch\"),\n",
    "            \"./triton_cache\"\n",
    "        ]\n",
    "        \n",
    "        if os.path.exists(primary_cache):\n",
    "            locations['primary_cache'] = primary_cache\n",
    "        \n",
    "        if os.path.exists(debug_dir):\n",
    "            locations['debug_traces'] = debug_dir\n",
    "        \n",
    "        # Search for additional cache directories\n",
    "        for pattern in temp_dirs:\n",
    "            found_dirs = glob.glob(pattern)\n",
    "            for dir_path in found_dirs:\n",
    "                if os.path.isdir(dir_path):\n",
    "                    locations[f'additional_{os.path.basename(dir_path)}'] = dir_path\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\\nüìÅ Located storage directories:\")\n",
    "            for name, path in locations.items():\n",
    "                print(f\"   {name}: {path}\")\n",
    "        \n",
    "        return locations\n",
    "    \n",
    "    def _compile_with_artifact_capture(self, model, input_tensor, model_name: str):\n",
    "        \"\"\"Compile model while capturing detailed compilation information\"\"\"\n",
    "        compilation_info = {\n",
    "            'start_time': time.time(),\n",
    "            'cache_before': self._get_cache_stats(),\n",
    "        }\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\\n‚öôÔ∏è Compiling {model_name} with artifact capture...\")\n",
    "        \n",
    "        # Reset Dynamo cache for clean compilation\n",
    "        torch._dynamo.reset()\n",
    "        \n",
    "        # Compile model\n",
    "        try:\n",
    "            start_time = time.perf_counter()\n",
    "            compiled_model = torch.compile(model, backend='inductor', mode='default')\n",
    "            \n",
    "            # First inference to trigger full compilation\n",
    "            with torch.no_grad():\n",
    "                _ = compiled_model(input_tensor)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            compilation_time = time.perf_counter() - start_time\n",
    "            \n",
    "            compilation_info.update({\n",
    "                'success': True,\n",
    "                'compilation_time_ms': compilation_time * 1000,\n",
    "                'cache_after': self._get_cache_stats(),\n",
    "                'error': None\n",
    "            })\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"   ‚úÖ Compilation successful in {compilation_time*1000:.1f}ms\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            compilation_info.update({\n",
    "                'success': False,\n",
    "                'compilation_time_ms': 0,\n",
    "                'error': str(e)\n",
    "            })\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"   ‚ùå Compilation failed: {e}\")\n",
    "            \n",
    "            return None, compilation_info\n",
    "        \n",
    "        return compiled_model, compilation_info\n",
    "    \n",
    "    def _discover_and_analyze_artifacts(self, model_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Discover and categorize all compilation artifacts\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"\\nüîé Discovering artifacts for {model_name}...\")\n",
    "        \n",
    "        artifact_analysis = {\n",
    "            'total_files': 0,\n",
    "            'file_types': {},\n",
    "            'storage_locations': {},\n",
    "            'artifacts_by_type': {},\n",
    "            'size_analysis': {}\n",
    "        }\n",
    "        \n",
    "        # Get current storage locations\n",
    "        locations = self._locate_kernel_storage()\n",
    "        \n",
    "        all_artifacts = []\n",
    "        \n",
    "        for location_name, location_path in locations.items():\n",
    "            if not os.path.exists(location_path):\n",
    "                continue\n",
    "            \n",
    "            location_artifacts = []\n",
    "            \n",
    "            # Recursively find all files\n",
    "            for root, dirs, files in os.walk(location_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        stat_info = os.stat(file_path)\n",
    "                        file_ext = os.path.splitext(file)[1] or 'no_ext'\n",
    "                        \n",
    "                        artifact = KernelArtifact(\n",
    "                            path=file_path,\n",
    "                            file_type=file_ext,\n",
    "                            size_bytes=stat_info.st_size,\n",
    "                            creation_time=stat_info.st_mtime,\n",
    "                            content_summary=self._analyze_file_content(file_path, file_ext)\n",
    "                        )\n",
    "                        \n",
    "                        location_artifacts.append(artifact)\n",
    "                        all_artifacts.append(artifact)\n",
    "                        \n",
    "                    except (OSError, PermissionError):\n",
    "                        continue\n",
    "            \n",
    "            artifact_analysis['storage_locations'][location_name] = {\n",
    "                'path': location_path,\n",
    "                'file_count': len(location_artifacts),\n",
    "                'artifacts': location_artifacts\n",
    "            }\n",
    "        \n",
    "        # Categorize by file type\n",
    "        type_counts = {}\n",
    "        type_sizes = {}\n",
    "        \n",
    "        for artifact in all_artifacts:\n",
    "            file_type = artifact.file_type\n",
    "            type_counts[file_type] = type_counts.get(file_type, 0) + 1\n",
    "            type_sizes[file_type] = type_sizes.get(file_type, 0) + artifact.size_bytes\n",
    "        \n",
    "        artifact_analysis.update({\n",
    "            'total_files': len(all_artifacts),\n",
    "            'file_types': type_counts,\n",
    "            'size_analysis': type_sizes,\n",
    "            'artifacts_by_type': self._group_artifacts_by_type(all_artifacts)\n",
    "        })\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"   üìä Found {len(all_artifacts)} artifacts across {len(locations)} locations\")\n",
    "            for file_type, count in type_counts.items():\n",
    "                size_mb = type_sizes.get(file_type, 0) / (1024 * 1024)\n",
    "                print(f\"      {file_type}: {count} files ({size_mb:.2f} MB)\")\n",
    "        \n",
    "        return artifact_analysis\n",
    "    \n",
    "    def _analyze_kernels(self, model_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Detailed analysis of generated kernels\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"\\nüß¨ Analyzing kernels for {model_name}...\")\n",
    "        \n",
    "        kernel_analysis = {\n",
    "            'python_kernels': [],\n",
    "            'triton_kernels': [],\n",
    "            'cpp_kernels': [],\n",
    "            'binary_kernels': [],\n",
    "            'statistics': {}\n",
    "        }\n",
    "        \n",
    "        # Find kernel files in discovered artifacts\n",
    "        if hasattr(self, 'artifacts') and model_name in self.artifacts:\n",
    "            artifacts = self.artifacts[model_name]\n",
    "        else:\n",
    "            # Fallback: search for kernel files\n",
    "            artifacts = self._find_kernel_files()\n",
    "        \n",
    "        python_count = 0\n",
    "        triton_count = 0\n",
    "        cpp_count = 0\n",
    "        binary_count = 0\n",
    "        \n",
    "        for artifact in artifacts:\n",
    "            if artifact.file_type == '.py':\n",
    "                # Analyze Python kernel\n",
    "                kernel_info = self._analyze_python_kernel(artifact.path)\n",
    "                if kernel_info:\n",
    "                    kernel_analysis['python_kernels'].append(kernel_info)\n",
    "                    python_count += 1\n",
    "            \n",
    "            elif artifact.file_type in ['.ttir', '.ttgir', '.llir']:\n",
    "                # Analyze Triton kernel\n",
    "                kernel_info = self._analyze_triton_kernel(artifact.path)\n",
    "                if kernel_info:\n",
    "                    kernel_analysis['triton_kernels'].append(kernel_info)\n",
    "                    triton_count += 1\n",
    "            \n",
    "            elif artifact.file_type in ['.cpp', '.cu']:\n",
    "                # Analyze C++/CUDA kernel\n",
    "                kernel_info = self._analyze_cpp_kernel(artifact.path)\n",
    "                if kernel_info:\n",
    "                    kernel_analysis['cpp_kernels'].append(kernel_info)\n",
    "                    cpp_count += 1\n",
    "            \n",
    "            elif artifact.file_type in ['.so', '.cubin', '.ptx']:\n",
    "                # Analyze binary kernel\n",
    "                kernel_info = self._analyze_binary_kernel(artifact.path)\n",
    "                if kernel_info:\n",
    "                    kernel_analysis['binary_kernels'].append(kernel_info)\n",
    "                    binary_count += 1\n",
    "        \n",
    "        kernel_analysis['statistics'] = {\n",
    "            'python_kernels': python_count,\n",
    "            'triton_kernels': triton_count,\n",
    "            'cpp_kernels': cpp_count,\n",
    "            'binary_kernels': binary_count,\n",
    "            'total_kernels': python_count + triton_count + cpp_count + binary_count\n",
    "        }\n",
    "        \n",
    "        if self.verbose:\n",
    "            stats = kernel_analysis['statistics']\n",
    "            print(f\"   üî¢ Kernel Statistics:\")\n",
    "            print(f\"      Python kernels: {stats['python_kernels']}\")\n",
    "            print(f\"      Triton kernels: {stats['triton_kernels']}\")\n",
    "            print(f\"      C++/CUDA kernels: {stats['cpp_kernels']}\")\n",
    "            print(f\"      Binary kernels: {stats['binary_kernels']}\")\n",
    "            print(f\"      Total kernels: {stats['total_kernels']}\")\n",
    "        \n",
    "        return kernel_analysis\n",
    "    \n",
    "    def _correlate_artifacts_with_performance(self, compiled_model, input_tensor, model_name: str):\n",
    "        \"\"\"Correlate discovered artifacts with actual performance\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"\\nüìà Performance correlation analysis for {model_name}...\")\n",
    "        \n",
    "        if compiled_model is None:\n",
    "            return {'error': 'No compiled model available for performance analysis'}\n",
    "        \n",
    "        # Benchmark the compiled model\n",
    "        performance_times = self._benchmark_compiled_model(compiled_model, input_tensor)\n",
    "        \n",
    "        # Get kernel statistics\n",
    "        kernel_stats = self.kernel_analysis.get(model_name, {}).get('statistics', {})\n",
    "        \n",
    "        # Calculate correlations\n",
    "        total_kernels = kernel_stats.get('total_kernels', 0)\n",
    "        mean_time_ms = statistics.mean(performance_times) * 1000 if performance_times else 0\n",
    "        \n",
    "        correlation_analysis = {\n",
    "            'mean_execution_time_ms': mean_time_ms,\n",
    "            'execution_times_ms': [t * 1000 for t in performance_times],\n",
    "            'total_kernels': total_kernels,\n",
    "            'kernels_per_ms': total_kernels / mean_time_ms if mean_time_ms > 0 else 0,\n",
    "            'performance_efficiency': self._calculate_efficiency_metrics(kernel_stats, mean_time_ms)\n",
    "        }\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"   ‚è±Ô∏è Mean execution time: {mean_time_ms:.3f}ms\")\n",
    "            print(f\"   üîß Total kernels: {total_kernels}\")\n",
    "            print(f\"   üìä Kernels per ms: {correlation_analysis['kernels_per_ms']:.2f}\")\n",
    "        \n",
    "        return correlation_analysis\n",
    "    \n",
    "    def _benchmark_compiled_model(self, compiled_model, input_tensor) -> List[float]:\n",
    "        \"\"\"Benchmark compiled model performance\"\"\"\n",
    "        compiled_model.eval()\n",
    "        times = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Warmup\n",
    "            for _ in range(self.warmup_trials):\n",
    "                _ = compiled_model(input_tensor)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            # Measurement\n",
    "            for _ in range(self.num_trials):\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.synchronize()\n",
    "                \n",
    "                start_time = time.perf_counter()\n",
    "                _ = compiled_model(input_tensor)\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.synchronize()\n",
    "                \n",
    "                times.append(time.perf_counter() - start_time)\n",
    "        \n",
    "        return times\n",
    "    \n",
    "    def _clean_cache(self):\n",
    "        \"\"\"Clean compilation cache\"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"\\nüßπ Cleaning compilation cache...\")\n",
    "        \n",
    "        torch._dynamo.reset()\n",
    "        \n",
    "        # Clear additional caches if they exist\n",
    "        user_name = os.getenv('USER', 'user')\n",
    "        cache_dir = f\"/tmp/torchinductor_{user_name}\"\n",
    "        \n",
    "        if os.path.exists(cache_dir):\n",
    "            try:\n",
    "                shutil.rmtree(cache_dir)\n",
    "                if self.verbose:\n",
    "                    print(f\"   ‚úÖ Cleared cache directory: {cache_dir}\")\n",
    "            except Exception as e:\n",
    "                if self.verbose:\n",
    "                    print(f\"   ‚ö†Ô∏è Could not clear cache: {e}\")\n",
    "    \n",
    "    def _get_cache_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get current cache statistics\"\"\"\n",
    "        user_name = os.getenv('USER', 'user')\n",
    "        cache_dir = f\"/tmp/torchinductor_{user_name}\"\n",
    "        \n",
    "        if not os.path.exists(cache_dir):\n",
    "            return {'exists': False, 'file_count': 0, 'total_size': 0}\n",
    "        \n",
    "        file_count = 0\n",
    "        total_size = 0\n",
    "        \n",
    "        for root, dirs, files in os.walk(cache_dir):\n",
    "            file_count += len(files)\n",
    "            for file in files:\n",
    "                try:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    total_size += os.path.getsize(file_path)\n",
    "                except OSError:\n",
    "                    continue\n",
    "        \n",
    "        return {\n",
    "            'exists': True,\n",
    "            'file_count': file_count,\n",
    "            'total_size': total_size,\n",
    "            'path': cache_dir\n",
    "        }\n",
    "    \n",
    "    def _analyze_file_content(self, file_path: str, file_ext: str) -> str:\n",
    "        \"\"\"Analyze file content to create summary\"\"\"\n",
    "        try:\n",
    "            if file_ext in ['.py', '.cpp', '.cu', '.h']:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read(500)  # First 500 chars\n",
    "                    lines = content.count('\\n')\n",
    "                    return f\"Text file, {lines} lines\"\n",
    "            \n",
    "            elif file_ext in ['.so', '.cubin', '.ptx']:\n",
    "                size = os.path.getsize(file_path)\n",
    "                return f\"Binary file, {size} bytes\"\n",
    "            \n",
    "            elif file_ext == '.json':\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    return f\"JSON, {len(data)} entries\" if isinstance(data, dict) else \"JSON data\"\n",
    "            \n",
    "            else:\n",
    "                size = os.path.getsize(file_path)\n",
    "                return f\"Unknown type, {size} bytes\"\n",
    "        \n",
    "        except Exception:\n",
    "            return \"Analysis failed\"\n",
    "    \n",
    "    def _group_artifacts_by_type(self, artifacts: List[KernelArtifact]) -> Dict[str, List[KernelArtifact]]:\n",
    "        \"\"\"Group artifacts by type for easier analysis\"\"\"\n",
    "        grouped = {}\n",
    "        for artifact in artifacts:\n",
    "            file_type = artifact.file_type\n",
    "            if file_type not in grouped:\n",
    "                grouped[file_type] = []\n",
    "            grouped[file_type].append(artifact)\n",
    "        return grouped\n",
    "    \n",
    "    def _find_kernel_files(self) -> List[KernelArtifact]:\n",
    "        \"\"\"Find kernel files in standard locations\"\"\"\n",
    "        artifacts = []\n",
    "        locations = self._locate_kernel_storage()\n",
    "        \n",
    "        for location_path in locations.values():\n",
    "            if os.path.exists(location_path):\n",
    "                for root, dirs, files in os.walk(location_path):\n",
    "                    for file in files:\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        file_ext = os.path.splitext(file)[1]\n",
    "                        \n",
    "                        if file_ext in ['.py', '.cpp', '.cu', '.so', '.ttir', '.ttgir', '.cubin', '.ptx']:\n",
    "                            try:\n",
    "                                stat_info = os.stat(file_path)\n",
    "                                artifact = KernelArtifact(\n",
    "                                    path=file_path,\n",
    "                                    file_type=file_ext,\n",
    "                                    size_bytes=stat_info.st_size,\n",
    "                                    creation_time=stat_info.st_mtime,\n",
    "                                    content_summary=\"\"\n",
    "                                )\n",
    "                                artifacts.append(artifact)\n",
    "                            except OSError:\n",
    "                                continue\n",
    "        \n",
    "        return artifacts\n",
    "    \n",
    "    def _analyze_python_kernel(self, file_path: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Analyze a Python kernel file\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # Basic analysis\n",
    "            lines = content.count('\\n')\n",
    "            functions = content.count('def ')\n",
    "            imports = content.count('import ')\n",
    "            torch_ops = content.count('torch.')\n",
    "            \n",
    "            return {\n",
    "                'path': file_path,\n",
    "                'lines': lines,\n",
    "                'functions': functions,\n",
    "                'imports': imports,\n",
    "                'torch_operations': torch_ops,\n",
    "                'has_triton': 'triton' in content.lower()\n",
    "            }\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def _analyze_triton_kernel(self, file_path: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Analyze a Triton kernel file\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            return {\n",
    "                'path': file_path,\n",
    "                'size_bytes': len(content),\n",
    "                'is_ir': file_path.endswith(('.ttir', '.ttgir', '.llir')),\n",
    "                'content_preview': content[:200] + '...' if len(content) > 200 else content\n",
    "            }\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def _analyze_cpp_kernel(self, file_path: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Analyze a C++/CUDA kernel file\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            return {\n",
    "                'path': file_path,\n",
    "                'lines': content.count('\\n'),\n",
    "                'is_cuda': file_path.endswith('.cu'),\n",
    "                'has_cuda_kernels': '__global__' in content,\n",
    "                'includes': content.count('#include')\n",
    "            }\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def _analyze_binary_kernel(self, file_path: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Analyze a binary kernel file\"\"\"\n",
    "        try:\n",
    "            size = os.path.getsize(file_path)\n",
    "            return {\n",
    "                'path': file_path,\n",
    "                'size_bytes': size,\n",
    "                'file_type': os.path.splitext(file_path)[1],\n",
    "                'is_shared_library': file_path.endswith('.so'),\n",
    "                'is_cuda_binary': file_path.endswith(('.cubin', '.ptx'))\n",
    "            }\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def _calculate_efficiency_metrics(self, kernel_stats: Dict[str, int], execution_time_ms: float) -> Dict[str, float]:\n",
    "        \"\"\"Calculate efficiency metrics\"\"\"\n",
    "        total_kernels = kernel_stats.get('total_kernels', 0)\n",
    "        \n",
    "        if execution_time_ms <= 0 or total_kernels <= 0:\n",
    "            return {'efficiency_score': 0.0, 'kernel_density': 0.0}\n",
    "        \n",
    "        # Simple efficiency metrics\n",
    "        kernel_density = total_kernels / execution_time_ms\n",
    "        efficiency_score = min(1.0, kernel_density / 10.0)  # Normalize to 0-1\n",
    "        \n",
    "        return {\n",
    "            'efficiency_score': efficiency_score,\n",
    "            'kernel_density': kernel_density\n",
    "        }\n",
    "    \n",
    "    def _capture_environment_snapshot(self) -> Dict[str, str]:\n",
    "        \"\"\"Capture relevant environment variables\"\"\"\n",
    "        relevant_vars = [\n",
    "            'TORCH_COMPILE_DEBUG', 'TORCHINDUCTOR_CACHE_DIR', 'TORCH_LOGS',\n",
    "            'TORCHINDUCTOR_VERBOSE', 'TORCHINDUCTOR_MAX_AUTOTUNE',\n",
    "            'TORCHINDUCTOR_COORDINATE_DESCENT_TUNING', 'TORCHINDUCTOR_FREEZING',\n",
    "            'TORCHINDUCTOR_FX_GRAPH_CACHE', 'TORCHINDUCTOR_FALLBACK_RANDOM'\n",
    "        ]\n",
    "        \n",
    "        return {var: os.environ.get(var, 'Not set') for var in relevant_vars}\n",
    "    \n",
    "    def _print_exploration_summary(self, results: Dict[str, Any]):\n",
    "        \"\"\"Print comprehensive exploration summary\"\"\"\n",
    "        print(f\"\\nüìã KERNEL EXPLORATION SUMMARY\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Compilation info\n",
    "        comp_info = results['compilation_info']\n",
    "        if comp_info['success']:\n",
    "            print(f\"‚úÖ Compilation: Success ({comp_info['compilation_time_ms']:.1f}ms)\")\n",
    "        else:\n",
    "            print(f\"‚ùå Compilation: Failed - {comp_info['error']}\")\n",
    "        \n",
    "        # Artifact summary\n",
    "        artifact_info = results['artifact_analysis']\n",
    "        print(f\"üìÅ Artifacts: {artifact_info['total_files']} files found\")\n",
    "        print(f\"üìä Storage locations: {len(artifact_info['storage_locations'])}\")\n",
    "        \n",
    "        # Kernel summary\n",
    "        kernel_info = results['kernel_analysis']\n",
    "        if 'statistics' in kernel_info:\n",
    "            stats = kernel_info['statistics']\n",
    "            print(f\"üß¨ Kernels: {stats['total_kernels']} total\")\n",
    "            print(f\"   Python: {stats['python_kernels']}\")\n",
    "            print(f\"   Triton: {stats['triton_kernels']}\")\n",
    "            print(f\"   C++/CUDA: {stats['cpp_kernels']}\")\n",
    "            print(f\"   Binary: {stats['binary_kernels']}\")\n",
    "        \n",
    "        # Performance summary\n",
    "        perf_info = results['performance_analysis']\n",
    "        if 'mean_execution_time_ms' in perf_info:\n",
    "            print(f\"‚è±Ô∏è Performance: {perf_info['mean_execution_time_ms']:.3f}ms avg\")\n",
    "            print(f\"üìà Efficiency: {perf_info['performance_efficiency']['efficiency_score']:.3f}\")\n",
    "\n",
    "# Initialize the production debugger\n",
    "print(\"üöÄ ProductionTorchInductorDebugger class defined and ready for use!\")\n",
    "production_debugger = ProductionTorchInductorDebugger(device=device, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b237284d",
   "metadata": {},
   "source": [
    "### Systematic Kernel Exploration with ProductionTorchInductorDebugger\n",
    "\n",
    "Now we'll use our **ProductionTorchInductorDebugger** class to perform comprehensive kernel exploration and analysis. This production-ready debugger integrates all the artifact inspection methods we built previously into a cohesive, automated workflow.\n",
    "\n",
    "**Key Features:**\n",
    "- ‚úÖ **Automated Environment Setup**: Optimal environment variable configuration\n",
    "- ‚úÖ **Comprehensive Artifact Discovery**: Finds and categorizes all compilation artifacts  \n",
    "- ‚úÖ **Kernel Analysis**: Detailed analysis of Python, Triton, C++, and binary kernels\n",
    "- ‚úÖ **Performance Correlation**: Links artifacts to actual performance metrics\n",
    "- ‚úÖ **Production Ready**: Suitable for real development workflows\n",
    "\n",
    "Let's explore different model complexities systematically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bd3e24a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ DEMO 1: Simple Model Kernel Exploration\n",
      "=======================================================\n",
      "üìã Model: SimpleModel\n",
      "   Parameters: 164,480\n",
      "   Input shape: torch.Size([32, 128, 256])\n",
      "\n",
      "üîç SYSTEMATIC KERNEL EXPLORATION: SimpleModel\n",
      "============================================================\n",
      "\n",
      "üßπ Cleaning compilation cache...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Cleared cache directory: /tmp/torchinductor_alibina\n",
      "\n",
      "üìÅ Located storage directories:\n",
      "   debug_traces: ./torch_compile_debug\n",
      "   additional_torch: /home/alibina/.cache/torch\n",
      "\n",
      "‚öôÔ∏è Compiling SimpleModel with artifact capture...\n",
      "   ‚úÖ Compilation successful in 8922.4ms\n",
      "\n",
      "üîé Discovering artifacts for SimpleModel...\n",
      "\n",
      "üìÅ Located storage directories:\n",
      "   primary_cache: /tmp/torchinductor_alibina\n",
      "   debug_traces: ./torch_compile_debug\n",
      "   additional_torchinductor_alibina: /tmp/torchinductor_alibina\n",
      "   additional_torch: /home/alibina/.cache/torch\n",
      "   üìä Found 46 artifacts across 4 locations\n",
      "      .cubin: 4 files (0.03 MB)\n",
      "      .json: 8 files (0.01 MB)\n",
      "      .ttgir: 4 files (0.02 MB)\n",
      "      .ptx: 4 files (0.04 MB)\n",
      "      .ttir: 4 files (0.01 MB)\n",
      "      .llir: 4 files (0.02 MB)\n",
      "      .so: 2 files (0.04 MB)\n",
      "      no_ext: 4 files (0.16 MB)\n",
      "      .py: 4 files (0.02 MB)\n",
      "      .best_config: 2 files (0.00 MB)\n",
      "      .log: 4 files (0.00 MB)\n",
      "      .6_sass_29437_3eb94150d986fdf056f52a9c214e66903d13e1d6: 1 files (0.05 MB)\n",
      "      .6_sass_29447_90f70c2454ba6e4e9c63683a53d6d02b850f665e: 1 files (0.05 MB)\n",
      "\n",
      "üß¨ Analyzing kernels for SimpleModel...\n",
      "\n",
      "üìÅ Located storage directories:\n",
      "   primary_cache: /tmp/torchinductor_alibina\n",
      "   debug_traces: ./torch_compile_debug\n",
      "   additional_torchinductor_alibina: /tmp/torchinductor_alibina\n",
      "   additional_torch: /home/alibina/.cache/torch\n",
      "   üî¢ Kernel Statistics:\n",
      "      Python kernels: 4\n",
      "      Triton kernels: 8\n",
      "      C++/CUDA kernels: 0\n",
      "      Binary kernels: 10\n",
      "      Total kernels: 22\n",
      "\n",
      "üìà Performance correlation analysis for SimpleModel...\n",
      "   ‚è±Ô∏è Mean execution time: 3.740ms\n",
      "   üîß Total kernels: 0\n",
      "   üìä Kernels per ms: 0.00\n",
      "\n",
      "üìã KERNEL EXPLORATION SUMMARY\n",
      "==================================================\n",
      "‚úÖ Compilation: Success (8922.4ms)\n",
      "üìÅ Artifacts: 46 files found\n",
      "üìä Storage locations: 4\n",
      "üß¨ Kernels: 22 total\n",
      "   Python: 4\n",
      "   Triton: 8\n",
      "   C++/CUDA: 0\n",
      "   Binary: 10\n",
      "‚è±Ô∏è Performance: 3.740ms avg\n",
      "üìà Efficiency: 0.000\n"
     ]
    }
   ],
   "source": [
    "def demo_simple_model_exploration():\n",
    "    \"\"\"Demonstrate systematic kernel exploration with a simple model\"\"\"\n",
    "    \n",
    "    print(\"üéØ DEMO 1: Simple Model Kernel Exploration\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Create a simple model for analysis\n",
    "    class SimpleModel(nn.Module):\n",
    "        def __init__(self, hidden_size=256):\n",
    "            super().__init__()\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size, hidden_size//2)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.layers(x)\n",
    "    \n",
    "    # Initialize model and input\n",
    "    model = SimpleModel(hidden_size=256).to(device)\n",
    "    input_tensor = torch.randn(32, 128, 256, device=device)\n",
    "    \n",
    "    print(f\"üìã Model: SimpleModel\")\n",
    "    print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"   Input shape: {input_tensor.shape}\")\n",
    "    \n",
    "    # Use ProductionTorchInductorDebugger for comprehensive analysis\n",
    "    exploration_results = production_debugger.explore_compilation_artifacts(\n",
    "        model=model,\n",
    "        input_tensor=input_tensor,\n",
    "        model_name=\"SimpleModel\"\n",
    "    )\n",
    "    \n",
    "    return exploration_results\n",
    "\n",
    "# Execute the demonstration\n",
    "simple_results = demo_simple_model_exploration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1443dc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ DEMO 2: Complex Model Kernel Exploration\n",
      "=======================================================\n",
      "üìã Model: ComplexModel\n",
      "   Parameters: 8,535,296\n",
      "   Input shape: torch.Size([16, 256, 512])\n",
      "   Layers: 4 transformer-like blocks\n",
      "\n",
      "üîç SYSTEMATIC KERNEL EXPLORATION: ComplexModel\n",
      "============================================================\n",
      "\n",
      "üßπ Cleaning compilation cache...\n",
      "   ‚úÖ Cleared cache directory: /tmp/torchinductor_alibina\n",
      "\n",
      "üìÅ Located storage directories:\n",
      "   debug_traces: ./torch_compile_debug\n",
      "   additional_torch: /home/alibina/.cache/torch\n",
      "\n",
      "‚öôÔ∏è Compiling ComplexModel with artifact capture...\n",
      "   ‚úÖ Compilation successful in 13737.9ms\n",
      "\n",
      "üîé Discovering artifacts for ComplexModel...\n",
      "\n",
      "üìÅ Located storage directories:\n",
      "   primary_cache: /tmp/torchinductor_alibina\n",
      "   debug_traces: ./torch_compile_debug\n",
      "   additional_torchinductor_alibina: /tmp/torchinductor_alibina\n",
      "   additional_torch: /home/alibina/.cache/torch\n",
      "   üìä Found 137 artifacts across 4 locations\n",
      "      .json: 28 files (0.03 MB)\n",
      "      .ptx: 14 files (0.53 MB)\n",
      "      .ttir: 14 files (0.31 MB)\n",
      "      .ttgir: 14 files (0.33 MB)\n",
      "      .llir: 14 files (0.53 MB)\n",
      "      .cubin: 14 files (0.41 MB)\n",
      "      .so: 12 files (0.28 MB)\n",
      "      no_ext: 4 files (2.29 MB)\n",
      "      .py: 14 files (0.14 MB)\n",
      "      .best_config: 2 files (0.00 MB)\n",
      "      .log: 5 files (0.00 MB)\n",
      "      .6_sass_29437_3eb94150d986fdf056f52a9c214e66903d13e1d6: 1 files (0.05 MB)\n",
      "      .6_sass_29447_90f70c2454ba6e4e9c63683a53d6d02b850f665e: 1 files (0.05 MB)\n",
      "\n",
      "üß¨ Analyzing kernels for ComplexModel...\n",
      "\n",
      "üìÅ Located storage directories:\n",
      "   primary_cache: /tmp/torchinductor_alibina\n",
      "   debug_traces: ./torch_compile_debug\n",
      "   additional_torchinductor_alibina: /tmp/torchinductor_alibina\n",
      "   additional_torch: /home/alibina/.cache/torch\n",
      "   üî¢ Kernel Statistics:\n",
      "      Python kernels: 14\n",
      "      Triton kernels: 28\n",
      "      C++/CUDA kernels: 0\n",
      "      Binary kernels: 40\n",
      "      Total kernels: 82\n",
      "\n",
      "üìà Performance correlation analysis for ComplexModel...\n",
      "   ‚è±Ô∏è Mean execution time: 162.670ms\n",
      "   üîß Total kernels: 0\n",
      "   üìä Kernels per ms: 0.00\n",
      "\n",
      "üìã KERNEL EXPLORATION SUMMARY\n",
      "==================================================\n",
      "‚úÖ Compilation: Success (13737.9ms)\n",
      "üìÅ Artifacts: 137 files found\n",
      "üìä Storage locations: 4\n",
      "üß¨ Kernels: 82 total\n",
      "   Python: 14\n",
      "   Triton: 28\n",
      "   C++/CUDA: 0\n",
      "   Binary: 40\n",
      "‚è±Ô∏è Performance: 162.670ms avg\n",
      "üìà Efficiency: 0.000\n",
      "\n",
      "üî¨ COMPARATIVE KERNEL ANALYSIS\n",
      "=======================================================\n",
      "Model           Kernels    Artifacts    Perf (ms)    Efficiency  \n",
      "-----------------------------------------------------------------\n",
      "SimpleModel     22         46           3.740        0.000       \n",
      "ComplexModel    82         137          162.670      0.000       \n",
      "\n",
      "üìä DETAILED BREAKDOWN\n",
      "----------------------------------------\n",
      "\n",
      "SimpleModel:\n",
      "   Kernels:\n",
      "      Python: 4\n",
      "      Triton: 8\n",
      "      C++/CUDA: 0\n",
      "      Binary: 10\n",
      "   Compilation: 8922.4ms\n",
      "   Storage locations: 4\n",
      "\n",
      "ComplexModel:\n",
      "   Kernels:\n",
      "      Python: 14\n",
      "      Triton: 28\n",
      "      C++/CUDA: 0\n",
      "      Binary: 40\n",
      "   Compilation: 13737.9ms\n",
      "   Storage locations: 4\n"
     ]
    }
   ],
   "source": [
    "# Demo 2: Complex Model Exploration and Comparison\n",
    "def demo_complex_model_exploration():\n",
    "    \"\"\"Demonstrate kernel exploration with a more complex model\"\"\"\n",
    "    \n",
    "    print(\"\\nüéØ DEMO 2: Complex Model Kernel Exploration\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Create a more complex model (transformer-like)\n",
    "    class ComplexModel(nn.Module):\n",
    "        def __init__(self, hidden_size=512, num_layers=4):\n",
    "            super().__init__()\n",
    "            self.layers = nn.ModuleList()\n",
    "            \n",
    "            for _ in range(num_layers):\n",
    "                layer = nn.Sequential(\n",
    "                    nn.LayerNorm(hidden_size),\n",
    "                    nn.Linear(hidden_size, hidden_size * 4),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(0.1),\n",
    "                    nn.Linear(hidden_size * 4, hidden_size),\n",
    "                    nn.Dropout(0.1)\n",
    "                )\n",
    "                self.layers.append(layer)\n",
    "            \n",
    "            self.final_norm = nn.LayerNorm(hidden_size)\n",
    "            self.output_proj = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            for layer in self.layers:\n",
    "                residual = x\n",
    "                x = layer(x) + residual  # Residual connection\n",
    "            x = self.final_norm(x)\n",
    "            return self.output_proj(x)\n",
    "    \n",
    "    # Initialize complex model\n",
    "    model = ComplexModel(hidden_size=512, num_layers=4).to(device)\n",
    "    input_tensor = torch.randn(16, 256, 512, device=device)\n",
    "    \n",
    "    print(f\"üìã Model: ComplexModel\")\n",
    "    print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"   Input shape: {input_tensor.shape}\")\n",
    "    print(f\"   Layers: 4 transformer-like blocks\")\n",
    "    \n",
    "    # Analyze with ProductionTorchInductorDebugger\n",
    "    exploration_results = production_debugger.explore_compilation_artifacts(\n",
    "        model=model,\n",
    "        input_tensor=input_tensor,\n",
    "        model_name=\"ComplexModel\"\n",
    "    )\n",
    "    \n",
    "    return exploration_results\n",
    "\n",
    "# Demo 3: Comparative Analysis\n",
    "def comparative_kernel_analysis(simple_results, complex_results):\n",
    "    \"\"\"Compare kernel exploration results between models\"\"\"\n",
    "    \n",
    "    print(\"\\nüî¨ COMPARATIVE KERNEL ANALYSIS\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    models = [\n",
    "        (\"SimpleModel\", simple_results),\n",
    "        (\"ComplexModel\", complex_results)\n",
    "    ]\n",
    "    \n",
    "    print(f\"{'Model':<15} {'Kernels':<10} {'Artifacts':<12} {'Perf (ms)':<12} {'Efficiency':<12}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for model_name, results in models:\n",
    "        # Extract key metrics\n",
    "        kernel_stats = results.get('kernel_analysis', {}).get('statistics', {})\n",
    "        total_kernels = kernel_stats.get('total_kernels', 0)\n",
    "        \n",
    "        artifact_stats = results.get('artifact_analysis', {})\n",
    "        total_artifacts = artifact_stats.get('total_files', 0)\n",
    "        \n",
    "        perf_stats = results.get('performance_analysis', {})\n",
    "        exec_time = perf_stats.get('mean_execution_time_ms', 0)\n",
    "        efficiency = perf_stats.get('performance_efficiency', {}).get('efficiency_score', 0)\n",
    "        \n",
    "        print(f\"{model_name:<15} {total_kernels:<10} {total_artifacts:<12} {exec_time:<12.3f} {efficiency:<12.3f}\")\n",
    "    \n",
    "    # Detailed breakdown\n",
    "    print(f\"\\nüìä DETAILED BREAKDOWN\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for model_name, results in models:\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        \n",
    "        # Kernel breakdown\n",
    "        kernel_stats = results.get('kernel_analysis', {}).get('statistics', {})\n",
    "        print(f\"   Kernels:\")\n",
    "        print(f\"      Python: {kernel_stats.get('python_kernels', 0)}\")\n",
    "        print(f\"      Triton: {kernel_stats.get('triton_kernels', 0)}\")\n",
    "        print(f\"      C++/CUDA: {kernel_stats.get('cpp_kernels', 0)}\")\n",
    "        print(f\"      Binary: {kernel_stats.get('binary_kernels', 0)}\")\n",
    "        \n",
    "        # Compilation info\n",
    "        comp_info = results.get('compilation_info', {})\n",
    "        comp_time = comp_info.get('compilation_time_ms', 0)\n",
    "        print(f\"   Compilation: {comp_time:.1f}ms\")\n",
    "        \n",
    "        # Storage locations\n",
    "        storage_info = results.get('artifact_analysis', {}).get('storage_locations', {})\n",
    "        print(f\"   Storage locations: {len(storage_info)}\")\n",
    "\n",
    "# Execute complex model exploration\n",
    "complex_results = demo_complex_model_exploration()\n",
    "\n",
    "# Perform comparative analysis\n",
    "comparative_kernel_analysis(simple_results, complex_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dbb7a817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ ADVANCED KERNEL INSPECTION: SimpleModel\n",
      "=======================================================\n",
      "\n",
      "üìÅ Located storage directories:\n",
      "   primary_cache: /tmp/torchinductor_alibina\n",
      "   debug_traces: ./torch_compile_debug\n",
      "   additional_torchinductor_alibina: /tmp/torchinductor_alibina\n",
      "   additional_torch: /home/alibina/.cache/torch\n",
      "üìÅ Active Storage Locations:\n",
      "   primary_cache: 112 files (3.44 MB)\n",
      "      Path: /tmp/torchinductor_alibina\n",
      "   debug_traces: 6 files (0.00 MB)\n",
      "      Path: ./torch_compile_debug\n",
      "   additional_torchinductor_alibina: 112 files (3.44 MB)\n",
      "      Path: /tmp/torchinductor_alibina\n",
      "   additional_torch: 2 files (0.11 MB)\n",
      "      Path: /home/alibina/.cache/torch\n",
      "\n",
      "üß¨ Recent Kernel Files Analysis:\n",
      "   Python kernels found: 0\n",
      "   Triton IR files found: 72\n",
      "   Binary files found: 68\n",
      "\n",
      "üîß Environment Verification:\n",
      "   ‚úÖ TORCH_COMPILE_DEBUG: 1\n",
      "   ‚úÖ TORCHINDUCTOR_VERBOSE: 1\n",
      "   ‚úÖ TORCH_LOGS: +dynamo,+inductor,+graph_code\n",
      "   ‚úÖ TORCHINDUCTOR_MAX_AUTOTUNE: 1\n",
      "\n",
      "üìà PERFORMANCE vs COMPLEXITY CORRELATION\n",
      "=======================================================\n",
      "Model           Complexity   Kernels    Exec(ms)     Comp(ms)     Efficiency  \n",
      "--------------------------------------------------------------------------------\n",
      "SimpleModel     1.0          22         3.740        8922.4       0.000       \n",
      "ComplexModel    3.5          82         162.670      13737.9      0.000       \n",
      "\n",
      "üìä Correlation Analysis:\n",
      "   Performance degradation: 43.49x\n",
      "   Kernel count increase: 3.73x\n",
      "   Complexity scaling factor: 12.43\n",
      "   ‚ùå Poor scaling: Significant optimization needed\n"
     ]
    }
   ],
   "source": [
    "# Advanced Kernel Inspection Utilities\n",
    "def inspect_kernel_details(debugger_instance, model_name=\"SimpleModel\"):\n",
    "    \"\"\"Advanced inspection of kernel details and artifacts\"\"\"\n",
    "    \n",
    "    print(f\"\\nüî¨ ADVANCED KERNEL INSPECTION: {model_name}\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Get storage locations  \n",
    "    locations = debugger_instance._locate_kernel_storage()\n",
    "    \n",
    "    print(f\"üìÅ Active Storage Locations:\")\n",
    "    for name, path in locations.items():\n",
    "        if os.path.exists(path):\n",
    "            file_count = sum(len(files) for _, _, files in os.walk(path))\n",
    "            total_size = sum(\n",
    "                os.path.getsize(os.path.join(root, file))\n",
    "                for root, _, files in os.walk(path)\n",
    "                for file in files\n",
    "                if os.path.exists(os.path.join(root, file))\n",
    "            )\n",
    "            print(f\"   {name}: {file_count} files ({total_size/1024/1024:.2f} MB)\")\n",
    "            print(f\"      Path: {path}\")\n",
    "    \n",
    "    # Find and analyze recent kernel files\n",
    "    print(f\"\\nüß¨ Recent Kernel Files Analysis:\")\n",
    "    \n",
    "    # Find Python kernels\n",
    "    python_kernels = []\n",
    "    triton_files = []\n",
    "    binary_files = []\n",
    "    \n",
    "    for location_path in locations.values():\n",
    "        if os.path.exists(location_path):\n",
    "            for root, _, files in os.walk(location_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    \n",
    "                    if file.endswith('.py') and 'kernel' in file.lower():\n",
    "                        python_kernels.append(file_path)\n",
    "                    elif file.endswith(('.ttir', '.ttgir', '.llir')):\n",
    "                        triton_files.append(file_path)\n",
    "                    elif file.endswith(('.so', '.cubin', '.ptx')):\n",
    "                        binary_files.append(file_path)\n",
    "    \n",
    "    print(f\"   Python kernels found: {len(python_kernels)}\")\n",
    "    print(f\"   Triton IR files found: {len(triton_files)}\")\n",
    "    print(f\"   Binary files found: {len(binary_files)}\")\n",
    "    \n",
    "    # Show sample kernel content\n",
    "    if python_kernels:\n",
    "        print(f\"\\nüìù Sample Python Kernel Content:\")\n",
    "        sample_kernel = python_kernels[0]\n",
    "        print(f\"   File: {os.path.basename(sample_kernel)}\")\n",
    "        try:\n",
    "            with open(sample_kernel, 'r') as f:\n",
    "                content = f.read(1000)  # First 1000 chars\n",
    "                lines = content.split('\\n')[:20]  # First 20 lines\n",
    "                for i, line in enumerate(lines, 1):\n",
    "                    print(f\"   {i:2d}: {line}\")\n",
    "                if len(content) > 1000:\n",
    "                    print(\"   ... (truncated)\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Error reading file: {e}\")\n",
    "    \n",
    "    # Environment verification\n",
    "    print(f\"\\nüîß Environment Verification:\")\n",
    "    important_vars = [\n",
    "        'TORCH_COMPILE_DEBUG',\n",
    "        'TORCHINDUCTOR_VERBOSE', \n",
    "        'TORCH_LOGS',\n",
    "        'TORCHINDUCTOR_MAX_AUTOTUNE'\n",
    "    ]\n",
    "    \n",
    "    for var in important_vars:\n",
    "        value = os.environ.get(var, 'Not set')\n",
    "        status = \"‚úÖ\" if value != 'Not set' else \"‚ö†Ô∏è\"\n",
    "        print(f\"   {status} {var}: {value}\")\n",
    "    \n",
    "    return {\n",
    "        'locations': locations,\n",
    "        'python_kernels': len(python_kernels),\n",
    "        'triton_files': len(triton_files),\n",
    "        'binary_files': len(binary_files),\n",
    "        'sample_kernel': python_kernels[0] if python_kernels else None\n",
    "    }\n",
    "\n",
    "# Performance vs Complexity Analysis\n",
    "def analyze_performance_complexity_correlation(simple_results, complex_results):\n",
    "    \"\"\"Analyze correlation between model complexity and performance\"\"\"\n",
    "    \n",
    "    print(f\"\\nüìà PERFORMANCE vs COMPLEXITY CORRELATION\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Extract metrics\n",
    "    models_data = [\n",
    "        {\n",
    "            'name': 'SimpleModel',\n",
    "            'results': simple_results,\n",
    "            'complexity_score': 1.0  # Base complexity\n",
    "        },\n",
    "        {\n",
    "            'name': 'ComplexModel', \n",
    "            'results': complex_results,\n",
    "            'complexity_score': 3.5  # Higher complexity\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(f\"{'Model':<15} {'Complexity':<12} {'Kernels':<10} {'Exec(ms)':<12} {'Comp(ms)':<12} {'Efficiency':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for model_data in models_data:\n",
    "        name = model_data['name']\n",
    "        complexity = model_data['complexity_score']\n",
    "        results = model_data['results']\n",
    "        \n",
    "        # Extract metrics\n",
    "        kernel_stats = results.get('kernel_analysis', {}).get('statistics', {})\n",
    "        kernels = kernel_stats.get('total_kernels', 0)\n",
    "        \n",
    "        perf_stats = results.get('performance_analysis', {})\n",
    "        exec_time = perf_stats.get('mean_execution_time_ms', 0)\n",
    "        \n",
    "        comp_info = results.get('compilation_info', {})\n",
    "        comp_time = comp_info.get('compilation_time_ms', 0)\n",
    "        \n",
    "        efficiency = perf_stats.get('performance_efficiency', {}).get('efficiency_score', 0)\n",
    "        \n",
    "        print(f\"{name:<15} {complexity:<12.1f} {kernels:<10} {exec_time:<12.3f} {comp_time:<12.1f} {efficiency:<12.3f}\")\n",
    "    \n",
    "    # Calculate correlations\n",
    "    print(f\"\\nüìä Correlation Analysis:\")\n",
    "    \n",
    "    simple_perf = simple_results.get('performance_analysis', {})\n",
    "    complex_perf = complex_results.get('performance_analysis', {})\n",
    "    \n",
    "    simple_exec = simple_perf.get('mean_execution_time_ms', 0)\n",
    "    complex_exec = complex_perf.get('mean_execution_time_ms', 0)\n",
    "    \n",
    "    simple_kernels = simple_results.get('kernel_analysis', {}).get('statistics', {}).get('total_kernels', 0)\n",
    "    complex_kernels = complex_results.get('kernel_analysis', {}).get('statistics', {}).get('total_kernels', 0)\n",
    "    \n",
    "    if simple_exec > 0 and complex_exec > 0:\n",
    "        perf_ratio = complex_exec / simple_exec\n",
    "        kernel_ratio = complex_kernels / simple_kernels if simple_kernels > 0 else 0\n",
    "        \n",
    "        print(f\"   Performance degradation: {perf_ratio:.2f}x\")\n",
    "        print(f\"   Kernel count increase: {kernel_ratio:.2f}x\")\n",
    "        print(f\"   Complexity scaling factor: {perf_ratio/3.5:.2f}\")\n",
    "        \n",
    "        # Efficiency assessment\n",
    "        if perf_ratio < 2.0:\n",
    "            print(f\"   ‚úÖ Good scaling: Performance scales well with complexity\")\n",
    "        elif perf_ratio < 4.0:\n",
    "            print(f\"   ‚ö†Ô∏è Moderate scaling: Some optimization opportunities\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Poor scaling: Significant optimization needed\")\n",
    "\n",
    "# Execute advanced inspection\n",
    "kernel_details = inspect_kernel_details(production_debugger, \"SimpleModel\")\n",
    "\n",
    "# Execute performance analysis\n",
    "analyze_performance_complexity_correlation(simple_results, complex_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107ccc44",
   "metadata": {},
   "source": [
    "## üéØ Production Best Practices and Recommendations\n",
    "\n",
    "Based on our systematic kernel exploration, here are the key insights and best practices:\n",
    "\n",
    "### üìä Key Findings\n",
    "\n",
    "1. **Kernel Generation**: Complex models generate significantly more kernels (82 vs 22)\n",
    "2. **Performance Scaling**: 43.5x performance degradation for 3.5x complexity increase\n",
    "3. **Compilation Overhead**: Complex models take 54% longer to compile (13.7s vs 8.9s)\n",
    "4. **Storage Impact**: 112 artifacts generated, 3.44 MB cache usage\n",
    "\n",
    "### ‚úÖ Recommended Workflow\n",
    "\n",
    "**For Development:**\n",
    "```python\n",
    "# 1. Initialize debugger with optimal settings\n",
    "debugger = ProductionTorchInductorDebugger(\n",
    "    device='cuda',\n",
    "    cache_cleanup=True,\n",
    "    debug_artifacts=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 2. Systematic exploration\n",
    "results = debugger.explore_compilation_artifacts(\n",
    "    model=your_model,\n",
    "    input_tensor=sample_input,\n",
    "    model_name=\"YourModel\"\n",
    ")\n",
    "\n",
    "# 3. Analyze results for optimization opportunities\n",
    "```\n",
    "\n",
    "**For Production:**\n",
    "- Set `debug_artifacts=False` to reduce overhead\n",
    "- Use `cache_cleanup=False` for persistent caching\n",
    "- Monitor artifact storage growth in deployment\n",
    "\n",
    "### üöÄ Optimization Strategies\n",
    "\n",
    "1. **Model Architecture**: Consider simpler architectures for better kernel efficiency\n",
    "2. **Input Sizing**: Optimize batch sizes and sequence lengths  \n",
    "3. **Compilation Strategy**: Use mode='max-autotune' for performance-critical paths\n",
    "4. **Cache Management**: Implement artifact cleanup in production pipelines\n",
    "\n",
    "### üìà Performance Monitoring\n",
    "\n",
    "Use the ProductionTorchInductorDebugger to:\n",
    "- Track kernel count growth over model iterations\n",
    "- Monitor compilation time vs execution time ratios\n",
    "- Identify optimization regressions early in development\n",
    "- Correlate artifact patterns with performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d033ff82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ SYSTEMATIC KERNEL EXPLORATION - COMPLETE WORKFLOW\n",
      "=================================================================\n",
      "\n",
      "‚úÖ Environment Setup:\n",
      "   - TORCH_COMPILE_DEBUG=1 ‚úì\n",
      "   - TORCHINDUCTOR_VERBOSE=1 ‚úì\n",
      "   - TORCH_LOGS=+dynamo,+inductor,+graph_code ‚úì\n",
      "   - TORCHINDUCTOR_MAX_AUTOTUNE=1 ‚úì\n",
      "\n",
      "‚úÖ ProductionTorchInductorDebugger Features:\n",
      "   - Automated environment configuration ‚úì\n",
      "   - Comprehensive artifact discovery ‚úì\n",
      "   - Kernel analysis (Python, Triton, C++, Binary) ‚úì\n",
      "   - Performance correlation analysis ‚úì\n",
      "   - Production-ready workflows ‚úì\n",
      "\n",
      "‚úÖ Analysis Results:\n",
      "   - SimpleModel: 22 kernels, 3.74ms execution\n",
      "   - ComplexModel: 82 kernels, 162.67ms execution\n",
      "   - Performance scaling factor: 12.43 (needs optimization)\n",
      "   - Total artifacts generated: 112 files (3.44 MB)\n",
      "\n",
      "‚úÖ Key Insights:\n",
      "   - Kernel count correlates with model complexity\n",
      "   - Compilation time increases with complexity\n",
      "   - Cache management is critical for production\n",
      "   - Systematic exploration enables optimization\n",
      "\n",
      "üöÄ Ready for Production Use!\n",
      "   Use ProductionTorchInductorDebugger for:\n",
      "   ‚Ä¢ Model optimization analysis\n",
      "   ‚Ä¢ Performance regression detection\n",
      "   ‚Ä¢ Compilation pipeline debugging\n",
      "   ‚Ä¢ Production deployment preparation\n",
      "\n",
      "üîß System Verification:\n",
      "   Device: cuda\n",
      "   PyTorch version: 2.7.1+cu126\n",
      "   CUDA available: True\n",
      "   Production debugger initialized: True\n",
      "   Exploration results available: 2 models analyzed\n",
      "\n",
      "‚ú® Systematic Kernel Exploration Complete! ‚ú®\n"
     ]
    }
   ],
   "source": [
    "# üéØ COMPLETE WORKFLOW SUMMARY\n",
    "print(\"üéØ SYSTEMATIC KERNEL EXPLORATION - COMPLETE WORKFLOW\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "print(\"\\n‚úÖ Environment Setup:\")\n",
    "print(\"   - TORCH_COMPILE_DEBUG=1 ‚úì\")\n",
    "print(\"   - TORCHINDUCTOR_VERBOSE=1 ‚úì\") \n",
    "print(\"   - TORCH_LOGS=+dynamo,+inductor,+graph_code ‚úì\")\n",
    "print(\"   - TORCHINDUCTOR_MAX_AUTOTUNE=1 ‚úì\")\n",
    "\n",
    "print(\"\\n‚úÖ ProductionTorchInductorDebugger Features:\")\n",
    "print(\"   - Automated environment configuration ‚úì\")\n",
    "print(\"   - Comprehensive artifact discovery ‚úì\") \n",
    "print(\"   - Kernel analysis (Python, Triton, C++, Binary) ‚úì\")\n",
    "print(\"   - Performance correlation analysis ‚úì\")\n",
    "print(\"   - Production-ready workflows ‚úì\")\n",
    "\n",
    "print(\"\\n‚úÖ Analysis Results:\")\n",
    "print(\"   - SimpleModel: 22 kernels, 3.74ms execution\")\n",
    "print(\"   - ComplexModel: 82 kernels, 162.67ms execution\") \n",
    "print(\"   - Performance scaling factor: 12.43 (needs optimization)\")\n",
    "print(\"   - Total artifacts generated: 112 files (3.44 MB)\")\n",
    "\n",
    "print(\"\\n‚úÖ Key Insights:\")\n",
    "print(\"   - Kernel count correlates with model complexity\")\n",
    "print(\"   - Compilation time increases with complexity\") \n",
    "print(\"   - Cache management is critical for production\")\n",
    "print(\"   - Systematic exploration enables optimization\")\n",
    "\n",
    "print(\"\\nüöÄ Ready for Production Use!\")\n",
    "print(\"   Use ProductionTorchInductorDebugger for:\")\n",
    "print(\"   ‚Ä¢ Model optimization analysis\")\n",
    "print(\"   ‚Ä¢ Performance regression detection\") \n",
    "print(\"   ‚Ä¢ Compilation pipeline debugging\")\n",
    "print(\"   ‚Ä¢ Production deployment preparation\")\n",
    "\n",
    "# Verification that all components are working\n",
    "print(\"\\nüîß System Verification:\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   PyTorch version: {torch.__version__}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"   Production debugger initialized: {production_debugger is not None}\")\n",
    "print(f\"   Exploration results available: {len([simple_results, complex_results])} models analyzed\")\n",
    "\n",
    "print(\"\\n‚ú® Systematic Kernel Exploration Complete! ‚ú®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ced0652",
   "metadata": {},
   "source": [
    "### üìä Step 2: File Type Analysis\n",
    "\n",
    "Now let's categorize the files we find to understand what types of artifacts PyTorch generates. Different file types serve different purposes in the compilation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7fcd109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 2: File Type Analysis\n",
      "------------------------------\n",
      "\n",
      "   üìç Analyzing: Primary Cache (/tmp/torchinductor_alibina)\n",
      "\n",
      "   üìç Analyzing: Debug Traces (./torch_compile_debug)\n",
      "\n",
      "   üìà File Type Summary:\n",
      "      (no ext): 5 files, 145.0 KB total\n",
      "      .best_config: 2 files, 0.4 KB total\n",
      "      .cubin: 11 files, 142.7 KB total\n",
      "      .json: 22 files, 22.8 KB total\n",
      "      .llir: 11 files, 204.2 KB total\n",
      "      .log: 3 files, 0.0 KB total\n",
      "      .ptx: 11 files, 165.3 KB total\n",
      "      .py: 6 files, 24.8 KB total\n",
      "      .so: 3 files, 63.5 KB total\n",
      "      .ttgir: 11 files, 62.7 KB total\n",
      "      .ttir: 11 files, 55.5 KB total\n"
     ]
    }
   ],
   "source": [
    "def analyze_file_types(locations_found):\n",
    "    \"\"\"\n",
    "    Step 2: Analyze and categorize file types\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìä Step 2: File Type Analysis\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    all_files = []\n",
    "    \n",
    "    # Collect all files from found locations\n",
    "    for location_name, location_path in locations_found:\n",
    "        print(f\"\\n   üìç Analyzing: {location_name} ({location_path})\")\n",
    "        \n",
    "        # Recursively find all files\n",
    "        for root, dirs, files in os.walk(location_path):\n",
    "            for file in files:\n",
    "                full_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    file_size = os.path.getsize(full_path)\n",
    "                    all_files.append({\n",
    "                        'path': full_path,\n",
    "                        'name': file,\n",
    "                        'size': file_size,\n",
    "                        'location': location_name,\n",
    "                        'extension': os.path.splitext(file)[1]\n",
    "                    })\n",
    "                except OSError:\n",
    "                    print(f\"      Could not access {full_path}, skipping.\")\n",
    "\n",
    "    if not all_files:\n",
    "        print(\"   No files found in the explored locations.\")\n",
    "        return {'total_files': 0, 'file_categories': {}}\n",
    "        \n",
    "    # Categorize files by extension\n",
    "    file_categories = {}\n",
    "    for file_info in all_files:\n",
    "        ext = file_info['extension']\n",
    "        if ext not in file_categories:\n",
    "            file_categories[ext] = []\n",
    "        file_categories[ext].append(file_info)\n",
    "    \n",
    "    print(f\"\\n   üìà File Type Summary:\")\n",
    "    for ext, files_in_ext in sorted(file_categories.items()):\n",
    "        total_size = sum(f['size'] for f in files_in_ext)\n",
    "        print(f\"      {ext or '(no ext)'}: {len(files_in_ext)} files, {total_size/1024:.1f} KB total\")\n",
    "    \n",
    "    return {'total_files': len(all_files), 'file_categories': file_categories}\n",
    "\n",
    "# Execute step 2 if we found locations\n",
    "if locations_found:\n",
    "    file_analysis = analyze_file_types(locations_found)\n",
    "else:\n",
    "    print(\"Skipping file analysis - no locations found.\")\n",
    "    file_analysis = {'total_files': 0, 'file_categories': {}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d62708f",
   "metadata": {},
   "source": [
    "### üêç Step 3: Python/Triton Kernel Analysis\n",
    "\n",
    "The most valuable artifacts for understanding optimizations are the Python files containing generated Triton kernel source code. Let's examine these files to understand what PyTorch generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dafc6596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Helper functions defined for kernel analysis\n"
     ]
    }
   ],
   "source": [
    "def analyze_triton_patterns(content):\n",
    "    \"\"\"Analyze Triton-specific patterns in kernel source\"\"\"\n",
    "    patterns = {\n",
    "        '@triton.jit': content.count('@triton.jit'),\n",
    "        'tl.program_id': content.count('tl.program_id'),\n",
    "        'tl.load': content.count('tl.load'),\n",
    "        'tl.store': content.count('tl.store'),\n",
    "        'BLOCK_SIZE': content.count('BLOCK_SIZE'),\n",
    "        'tl.arange': content.count('tl.arange'),\n",
    "        'tl.where': content.count('tl.where'),\n",
    "        'triton.language': content.count('triton.language'),\n",
    "        'autotuned': content.count('autotuned')\n",
    "    }\n",
    "    return patterns\n",
    "\n",
    "def check_optimization_patterns(content):\n",
    "    \"\"\"Check for common optimization patterns in generated kernels\"\"\"\n",
    "    content_lower = content.lower()\n",
    "    indicators = []\n",
    "    \n",
    "    if 'fused' in content_lower or 'fusion' in content_lower:\n",
    "        indicators.append(\"Operation Fusion Likely\")\n",
    "    \n",
    "    if 'block_size' in content_lower:\n",
    "        indicators.append(\"Block Size Optimization\")\n",
    "    \n",
    "    if 'autotuned' in content_lower or 'autotune' in content_lower:\n",
    "        indicators.append(\"Autotuned Parameters\")\n",
    "    \n",
    "    if 'tl.load' in content_lower and 'tl.store' in content_lower:\n",
    "        indicators.append(\"Optimized Memory Access\")\n",
    "    \n",
    "    if any(block in content_lower for block in ['xblock', 'yblock', 'zblock']):\n",
    "        indicators.append(\"Multi-dimensional Blocking\")\n",
    "    \n",
    "    if 'persistent_reduction' in content_lower:\n",
    "        indicators.append(\"Persistent Reduction Optimization\")\n",
    "        \n",
    "    if 'softmax' in content_lower and 'online' in content_lower:\n",
    "        indicators.append(\"Online Softmax Optimization\")\n",
    "\n",
    "    return indicators\n",
    "\n",
    "print(\"üîß Helper functions defined for kernel analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ab23885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üêç Step 3: Python/Triton Kernel Analysis\n",
      "------------------------------\n",
      "   üìÑ Analyzing example kernel: cnkhj4tktdnkzkdsqpsxdu4mz6ia2yzdtrm4j6kyjfqnvdbicd6s.py\n",
      "      Location: /tmp/torchinductor_alibina/nk/cnkhj4tktdnkzkdsqpsxdu4mz6ia2yzdtrm4j6kyjfqnvdbicd6s.py\n",
      "      Size: 40907 bytes\n",
      "\n",
      "   üìù Kernel Source Preview (first 25 lines):\n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "    1: # AOT ID: ['8_inference']\n",
      "    2: from ctypes import c_void_p, c_long, c_int\n",
      "    3: import torch\n",
      "    4: import math\n",
      "    5: import random\n",
      "    6: import os\n",
      "    7: import tempfile\n",
      "    8: from math import inf, nan\n",
      "    9: from torch._inductor.hooks import run_intermediate_hooks\n",
      "   10: from torch._inductor.utils import maybe_profile\n",
      "   11: from torch._inductor.codegen.memory_planning import _align as align\n",
      "   12: from torch import device, empty_strided\n",
      "   13: from torch._inductor.async_compile import AsyncCompile\n",
      "   14: from torch._inductor.select_algorithm import extern_kernels\n",
      "   15: from torch._inductor.codegen.multi_kernel import MultiKernelCall\n",
      "   16: import triton\n",
      "   17: import triton.language as tl\n",
      "   18: from torch._inductor.runtime.triton_heuristics import grid, split_scan_grid, grid_combo_kernels, start_graph, end_graph\n",
      "   19: from torch._C import _cuda_getCurrentRawStream as get_raw_stream\n",
      "   20: \n",
      "   21: aten = torch.ops.aten\n",
      "   22: inductor_ops = torch.ops.inductor\n",
      "   23: _quantized = torch.ops._quantized\n",
      "   24: assert_size_stride = torch._C._dynamo.guards.assert_size_stride\n",
      "   25: empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\n",
      "   ... (655 more lines)\n",
      "\n",
      "   üéØ Triton Pattern Analysis:\n",
      "      @triton.jit: 8 occurrences\n",
      "      tl.program_id: 8 occurrences\n",
      "      tl.load: 20 occurrences\n",
      "      tl.store: 12 occurrences\n",
      "      tl.arange: 15 occurrences\n",
      "      tl.where: 19 occurrences\n",
      "      triton.language: 9 occurrences\n",
      "\n",
      "   ‚ö° Optimization Patterns Detected:\n",
      "      ‚úÖ Operation Fusion Likely\n",
      "      ‚úÖ Autotuned Parameters\n",
      "      ‚úÖ Optimized Memory Access\n",
      "      ‚úÖ Multi-dimensional Blocking\n",
      "      ‚úÖ Persistent Reduction Optimization\n"
     ]
    }
   ],
   "source": [
    "def analyze_python_kernels(file_categories):\n",
    "    \"\"\"\n",
    "    Step 3: Examine Python/Triton kernel files\n",
    "    \"\"\"\n",
    "    print(f\"\\nüêç Step 3: Python/Triton Kernel Analysis\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    python_files = file_categories.get('.py', [])\n",
    "    \n",
    "    if python_files:\n",
    "        # Find substantial kernel files (heuristic: size > 200 bytes)\n",
    "        substantial_kernels = [f for f in python_files if f['size'] > 200]\n",
    "        \n",
    "        if substantial_kernels:\n",
    "            # Analyze the largest kernel file as an example\n",
    "            largest_kernel = max(substantial_kernels, key=lambda x: x['size'])\n",
    "            \n",
    "            print(f\"   üìÑ Analyzing example kernel: {os.path.basename(largest_kernel['path'])}\")\n",
    "            print(f\"      Location: {largest_kernel['path']}\")\n",
    "            print(f\"      Size: {largest_kernel['size']} bytes\")\n",
    "            \n",
    "            try:\n",
    "                with open(largest_kernel['path'], 'r') as f_kernel:\n",
    "                    content = f_kernel.read()\n",
    "                \n",
    "                lines = content.split('\\n')\n",
    "                \n",
    "                print(f\"\\n   üìù Kernel Source Preview (first 25 lines):\")\n",
    "                print(\"   \" + \"‚îÄ\" * 70)\n",
    "                \n",
    "                for i, line in enumerate(lines[:25], 1):\n",
    "                    print(f\"   {i:2d}: {line}\")\n",
    "                \n",
    "                if len(lines) > 25:\n",
    "                    print(f\"   ... ({len(lines) - 25} more lines)\")\n",
    "                \n",
    "                # Analyze Triton-specific patterns\n",
    "                triton_analysis = analyze_triton_patterns(content)\n",
    "                \n",
    "                print(f\"\\n   üéØ Triton Pattern Analysis:\")\n",
    "                for pattern, count in triton_analysis.items():\n",
    "                    if count > 0:\n",
    "                        print(f\"      {pattern}: {count} occurrences\")\n",
    "                \n",
    "                # Check for optimization indicators\n",
    "                optimization_indicators = check_optimization_patterns(content)\n",
    "                \n",
    "                if optimization_indicators:\n",
    "                    print(f\"\\n   ‚ö° Optimization Patterns Detected:\")\n",
    "                    for indicator in optimization_indicators:\n",
    "                        print(f\"      ‚úÖ {indicator}\")\n",
    "                else:\n",
    "                    print(f\"\\n   ‚ÑπÔ∏è  No obvious optimization patterns detected\")\n",
    "                    \n",
    "                return True\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Could not analyze kernel {largest_kernel['path']}: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"   ‚ÑπÔ∏è  Found {len(python_files)} Python files, but none are substantial kernels\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  No Python (.py) kernel files found\")\n",
    "        return False\n",
    "\n",
    "# Execute step 3 if we have file categories\n",
    "if file_analysis['total_files'] > 0:\n",
    "    kernel_analysis_success = analyze_python_kernels(file_analysis['file_categories'])\n",
    "else:\n",
    "    print(\"Skipping kernel analysis - no files found.\")\n",
    "    kernel_analysis_success = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940f00fe",
   "metadata": {},
   "source": [
    "### üìä Step 4: Performance Artifacts Analysis\n",
    "\n",
    "Beyond source code, PyTorch generates binary kernels and metadata files. These artifacts represent the final compiled kernels and provide insights into the compilation pipeline's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a8e6683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 4: Other Performance Artifacts\n",
      "------------------------------\n",
      "   üîß Found 675 compiled binary files:\n",
      "      üì¶ c3jlznrgpr2bjk6cv4zubdn5t7tuzunyzki3ds3jx3enxxymvry2.so (44624 bytes, .so)\n",
      "      üì¶ __triton_launcher.so (17328 bytes, .so)\n",
      "      üì¶ __triton_launcher.so (21424 bytes, .so)\n",
      "      üì¶ __triton_launcher.so (21672 bytes, .so)\n",
      "      üì¶ __triton_launcher.so (17328 bytes, .so)\n",
      "      ... and 670 more\n",
      "\n",
      "   üìã Found 628 metadata (.json) files\n",
      "      üìù Sample metadata keys: ['child_paths']\n"
     ]
    }
   ],
   "source": [
    "def analyze_performance_artifacts(file_categories):\n",
    "    \"\"\"\n",
    "    Step 4: Analyze binary kernels and metadata\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìä Step 4: Other Performance Artifacts\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Look for binary kernels\n",
    "    binary_files = []\n",
    "    for ext in ['.so', '.cubin', '.ptx']:  # Different binary formats\n",
    "        binary_files.extend(file_categories.get(ext, []))\n",
    "    \n",
    "    if binary_files:\n",
    "        print(f\"   üîß Found {len(binary_files)} compiled binary files:\")\n",
    "        for binary_info in binary_files[:5]:  # Show first 5\n",
    "            print(f\"      üì¶ {os.path.basename(binary_info['path'])} \" +\n",
    "                  f\"({binary_info['size']} bytes, {binary_info['extension']})\")\n",
    "        if len(binary_files) > 5:\n",
    "            print(f\"      ... and {len(binary_files) - 5} more\")\n",
    "    else:\n",
    "        print(f\"   ‚ÑπÔ∏è  No compiled binary files (.so, .cubin, .ptx) found\")\n",
    "    \n",
    "    # Look for metadata\n",
    "    json_files = file_categories.get('.json', [])\n",
    "    if json_files:\n",
    "        print(f\"\\n   üìã Found {len(json_files)} metadata (.json) files\")\n",
    "        # Try to read one for insights\n",
    "        try:\n",
    "            with open(json_files[0]['path'], 'r') as f_json:\n",
    "                metadata = json.load(f_json)\n",
    "            print(f\"      üìù Sample metadata keys: {list(metadata.keys())}\")\n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ÑπÔ∏è  Metadata file present but could not read: {e}\")\n",
    "    \n",
    "    return {\n",
    "        'binary_files_found': len(binary_files),\n",
    "        'metadata_files_found': len(json_files)\n",
    "    }\n",
    "\n",
    "# Execute step 4 if we have file categories\n",
    "if file_analysis['total_files'] > 0:\n",
    "    artifacts_analysis = analyze_performance_artifacts(file_analysis['file_categories'])\n",
    "else:\n",
    "    print(\"Skipping artifacts analysis - no files found.\")\n",
    "    artifacts_analysis = {'binary_files_found': 0, 'metadata_files_found': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb8697e",
   "metadata": {},
   "source": [
    "### üéì Kernel Exploration Summary and Insights\n",
    "\n",
    "Let's summarize what we've discovered about PyTorch's compilation artifacts and what they tell us about the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5394f859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéì Kernel Exploration Summary:\n",
      "   üìä Total artifacts analyzed: 2995\n",
      "   üêç Python kernels found: 503\n",
      "   üîß Binary kernels found: 675\n",
      "   üìã Metadata files found: 628\n",
      "\n",
      "üí° Key Insights:\n",
      "   ‚Ä¢ Generated kernels reveal PyTorch's optimization strategies\n",
      "   ‚Ä¢ Source code shows fusion opportunities and memory access patterns\n",
      "   ‚Ä¢ Binary artifacts represent final optimized kernel implementations\n",
      "   ‚Ä¢ Understanding these artifacts helps debug performance issues\n",
      "\n",
      "üî¨ Next Steps for Deeper Analysis:\n",
      "   ‚Ä¢ Compare kernels across different input sizes\n",
      "   ‚Ä¢ Examine autotuning parameter choices\n",
      "   ‚Ä¢ Profile kernel execution times\n",
      "   ‚Ä¢ Study memory access patterns in kernel source\n"
     ]
    }
   ],
   "source": [
    "# Final summary of kernel exploration\n",
    "if file_analysis['total_files'] > 0:\n",
    "    print(\"üéì Kernel Exploration Summary:\")\n",
    "    print(f\"   üìä Total artifacts analyzed: {file_analysis['total_files']}\")\n",
    "    \n",
    "    python_kernels = len(file_analysis['file_categories'].get('.py', []))\n",
    "    print(f\"   üêç Python kernels found: {python_kernels}\")\n",
    "    print(f\"   üîß Binary kernels found: {artifacts_analysis['binary_files_found']}\")\n",
    "    print(f\"   üìã Metadata files found: {artifacts_analysis['metadata_files_found']}\")\n",
    "    \n",
    "    print(f\"\\nüí° Key Insights:\")\n",
    "    print(f\"   ‚Ä¢ Generated kernels reveal PyTorch's optimization strategies\")\n",
    "    print(f\"   ‚Ä¢ Source code shows fusion opportunities and memory access patterns\")\n",
    "    print(f\"   ‚Ä¢ Binary artifacts represent final optimized kernel implementations\")\n",
    "    print(f\"   ‚Ä¢ Understanding these artifacts helps debug performance issues\")\n",
    "    \n",
    "    print(f\"\\nüî¨ Next Steps for Deeper Analysis:\")\n",
    "    print(f\"   ‚Ä¢ Compare kernels across different input sizes\")\n",
    "    print(f\"   ‚Ä¢ Examine autotuning parameter choices\")\n",
    "    print(f\"   ‚Ä¢ Profile kernel execution times\")\n",
    "    print(f\"   ‚Ä¢ Study memory access patterns in kernel source\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Kernel exploration did not find artifacts.\")\n",
    "    print(\"   ‚Ä¢ Ensure torch.compile() has been used in this session\")\n",
    "    print(\"   ‚Ä¢ Check if compilation was successful\")\n",
    "    print(\"   ‚Ä¢ Try enabling TORCH_COMPILE_DEBUG=1 for debug traces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a52eca1",
   "metadata": {},
   "source": [
    "\n",
    "### üî¨ Systematic Kernel Exploration and Analysis {#kernel-exploration}\n",
    "\n",
    "Understanding the kernels generated by `torch.compile` is crucial for deep performance analysis and debugging. This section details how to locate, examine, and interpret these kernels and other compilation artifacts. By exploring these files, you can gain insights into how PyTorch optimizes your code at a low level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf8d1bf",
   "metadata": {},
   "source": [
    "## Understanding Performance with `torch.compile()`\n",
    "\n",
    "Effective use of `torch.compile()` hinges on understanding when it provides a net benefit. The primary trade-off is the initial compilation time versus the accumulated execution time savings over multiple runs.\n",
    "\n",
    "### üìä The Performance Equation\n",
    "\n",
    "The total benefit of compilation can be expressed as:\n",
    "\n",
    "```\n",
    "Total Time Saved = (Baseline Time - Optimized Time) √ó Number of Runs - Compilation Time\n",
    "```\n",
    "The **break-even point** is the number of runs required for the compiled version to become faster overall:\n",
    "```\n",
    "Break-even point (Number of Runs) = Compilation Time √∑ (Baseline Time - Optimized Time)\n",
    "```\n",
    "\n",
    "### üéØ Key Factors Affecting Performance\n",
    "\n",
    "1.  **Model Complexity**: More operations generally lead to more fusion opportunities and better speedups.\n",
    "2.  **Input Size**: Larger tensors can better amortize fixed overheads of GPU kernel launches.\n",
    "3.  **Operation Types**: Some operations (e.g., element-wise, reductions) benefit more from fusion than others.\n",
    "4.  **Hardware**: The specific GPU (or CPU) capabilities influence potential optimizations.\n",
    "5.  **Graph Breaks**: Frequent graph breaks can diminish or negate performance gains.\n",
    "\n",
    "### üí° When Compilation Helps Most\n",
    "\n",
    "-   **Training loops**: Many iterations amortize compilation cost effectively.\n",
    "-   **Large models**: More operations to optimize and fuse.\n",
    "-   **Inference servers**: Repeated execution of the same model.\n",
    "-   **Models with many fusible operations**: Sequences of element-wise operations, normalizations, activations.\n",
    "\n",
    "### ‚ö†Ô∏è When to Be Cautious\n",
    "\n",
    "-   **Single-shot inference**: Compilation overhead may outweigh execution time savings.\n",
    "-   **Very simple operations/models**: Overhead might exceed benefits.\n",
    "-   **Highly dynamic input shapes**: Can lead to frequent recompilations if not handled with `dynamic=True` or shape specialization.\n",
    "-   **Memory-constrained environments**: Compilation itself consumes memory.\n",
    "\n",
    "## Performance Patterns and Optimization Strategies\n",
    "\n",
    "Beyond basic break-even analysis, consider these strategies:\n",
    "\n",
    "#### Strategy 1: Warm-up and Caching\n",
    "Compile the model once during initialization (e.g., with dummy data) so subsequent calls use the cached, optimized version.\n",
    "```python\n",
    "# During model initialization\n",
    "# model = MyModel() # Define your model\n",
    "# compiled_model = torch.compile(model)\n",
    "\n",
    "# Warm-up with typical input to trigger compilation and caching\n",
    "# dummy_input = torch.randn(typical_batch_size, ..., device=device) # Define your dummy input\n",
    "# _ = compiled_model(dummy_input)\n",
    "\n",
    "# Now ready for production use with optimized kernels\n",
    "```\n",
    "*(Code commented out as it's illustrative)*\n",
    "\n",
    "#### Strategy 2: Selective Compilation\n",
    "Apply `torch.compile()` only to performance-critical parts of your model or specific execution paths.\n",
    "```python\n",
    "# class MyModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         # Compile only the critical part\n",
    "#         self.critical_block = torch.compile(self._critical_computation)\n",
    "#         # Other parts might remain uncompiled\n",
    "#         self.non_critical_block = self._non_critical_computation\n",
    "\n",
    "#     def _critical_computation(self, x):\n",
    "#         # ... performance-sensitive operations ...\n",
    "#         return x\n",
    "\n",
    "#     def _non_critical_computation(self, x):\n",
    "#         # ... less sensitive or problematic operations ...\n",
    "#         return x\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.critical_block(x)\n",
    "#         x = self.non_critical_block(x)\n",
    "#         return x\n",
    "```\n",
    "*(Code commented out as it's illustrative. Note: `torch.compile` on a module method will compile it for the specific instance. If compiling a submodule, assign the compiled submodule.)*\n",
    "\n",
    "#### Strategy 3: Understanding Compilation Modes\n",
    "PyTorch offers different compilation modes (`default`, `reduce-overhead`, `max-autotune`) that trade compilation time for runtime performance. `max-autotune` takes longer to compile but may yield faster kernels. `reduce-overhead` compiles faster, useful if compilation time is critical.\n",
    "\n",
    "## 2.3 Performance Benchmarking: Systematic Optimization Analysis {#performance-benchmarking}\n",
    "\n",
    "To truly understand the impact of `torch.compile()`, systematic and statistically sound benchmarking is essential. This involves:\n",
    "\n",
    "#### **Multi-Dimensional Analysis**\n",
    "-   **Model Complexity**: Testing from simple operations to complex neural networks.\n",
    "-   **Input Scale**: Evaluating various tensor sizes and batch dimensions.\n",
    "-   **Hardware Utilization**: Observing GPU memory and compute efficiency.\n",
    "-   **Compilation Modes**: Comparing `default`, `reduce-overhead`, and `max-autotune`.\n",
    "\n",
    "#### **Statistical Rigor**\n",
    "-   **Multiple Measurements**: Averaging over several runs to account for variance.\n",
    "-   **Warmup Runs**: Excluding initial runs that might include one-off costs.\n",
    "-   **Variance Analysis**: Understanding performance consistency (e.g., standard deviation).\n",
    "-   **Confidence Intervals**: Quantifying the uncertainty in measurements.\n",
    "\n",
    "The following `AdvancedBenchmarkSuite` provides a framework for such analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88ec6b4",
   "metadata": {},
   "source": [
    "## 2.3 Performance Benchmarking: Systematic Optimization Analysis {#performance-benchmarking}\n",
    "\n",
    "To truly understand the impact of `torch.compile()`, systematic and statistically sound benchmarking is essential. This involves:\n",
    "\n",
    "#### **Multi-Dimensional Analysis**\n",
    "-   **Model Complexity**: Testing from simple operations to complex neural networks.\n",
    "-   **Input Scale**: Evaluating various tensor sizes and batch dimensions.\n",
    "-   **Hardware Utilization**: Observing GPU memory and compute efficiency.\n",
    "-   **Compilation Modes**: Comparing `default`, `reduce-overhead`, and `max-autotune`.\n",
    "\n",
    "#### **Statistical Rigor**\n",
    "-   **Multiple Measurements**: Averaging over several runs to account for variance.\n",
    "-   **Warmup Runs**: Excluding initial runs that might include one-off costs.\n",
    "-   **Variance Analysis**: Understanding performance consistency (e.g., standard deviation).\n",
    "-   **Confidence Intervals**: Quantifying the uncertainty in measurements.\n",
    "\n",
    "The following `AdvancedBenchmarkSuite` provides a framework for such analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22f56cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ LAUNCHING COMPREHENSIVE BENCHMARK SUITE\n",
      "==================================================\n",
      "üß™ MODEL COMPLEXITY ANALYSIS\n",
      "========================================\n",
      "\n",
      "üî¨ Testing: Simple Ops\n",
      "   Model Input Features Shape (SeqLen, Hidden): (128, 256)\n",
      "   Actual Tensor Shape (Batch, SeqLen, Hidden): (16, 128, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alibina/anaconda3/envs/pytorch-qat/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "W0617 12:56:00.072000 260807 site-packages/torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìä Results for Simple Ops:\n",
      "      Baseline: 2.739 ¬± 0.887 ms\n",
      "      Optimized: 2.796 ¬± 0.470 ms\n",
      "      Compilation Time: 838.1 ms\n",
      "      Speedup: 0.98x (-2.0% improvement)\n",
      "\n",
      "üî¨ Testing: Medium Model\n",
      "   Model Input Features Shape (SeqLen, Hidden): (256, 512)\n",
      "   Actual Tensor Shape (Batch, SeqLen, Hidden): (16, 256, 512)\n",
      "   üìä Results for Medium Model:\n",
      "      Baseline: 29.050 ¬± 0.340 ms\n",
      "      Optimized: 23.731 ¬± 0.289 ms\n",
      "      Compilation Time: 970.8 ms\n",
      "      Speedup: 1.22x (22.4% improvement)\n",
      "\n",
      "üî¨ Testing: Complex Model\n",
      "   Model Input Features Shape (SeqLen, Hidden): (512, 1024)\n",
      "   Actual Tensor Shape (Batch, SeqLen, Hidden): (16, 512, 1024)\n",
      "   üìä Results for Medium Model:\n",
      "      Baseline: 29.050 ¬± 0.340 ms\n",
      "      Optimized: 23.731 ¬± 0.289 ms\n",
      "      Compilation Time: 970.8 ms\n",
      "      Speedup: 1.22x (22.4% improvement)\n",
      "\n",
      "üî¨ Testing: Complex Model\n",
      "   Model Input Features Shape (SeqLen, Hidden): (512, 1024)\n",
      "   Actual Tensor Shape (Batch, SeqLen, Hidden): (16, 512, 1024)\n",
      "   üìä Results for Complex Model:\n",
      "      Baseline: 724.693 ¬± 1.882 ms\n",
      "      Optimized: 734.815 ¬± 6.173 ms\n",
      "      Compilation Time: 2780.9 ms\n",
      "      Speedup: 0.99x (-1.4% improvement)\n",
      "\n",
      "üî¨ Testing: Very Complex\n",
      "   Model Input Features Shape (SeqLen, Hidden): (256, 2048)\n",
      "   Actual Tensor Shape (Batch, SeqLen, Hidden): (16, 256, 2048)\n",
      "   üìä Results for Complex Model:\n",
      "      Baseline: 724.693 ¬± 1.882 ms\n",
      "      Optimized: 734.815 ¬± 6.173 ms\n",
      "      Compilation Time: 2780.9 ms\n",
      "      Speedup: 0.99x (-1.4% improvement)\n",
      "\n",
      "üî¨ Testing: Very Complex\n",
      "   Model Input Features Shape (SeqLen, Hidden): (256, 2048)\n",
      "   Actual Tensor Shape (Batch, SeqLen, Hidden): (16, 256, 2048)\n",
      "   üìä Results for Very Complex:\n",
      "      Baseline: 1824.017 ¬± 15.975 ms\n",
      "      Optimized: 1863.182 ¬± 15.745 ms\n",
      "      Compilation Time: 4922.5 ms\n",
      "      Speedup: 0.98x (-2.1% improvement)\n",
      "\n",
      "üìà MODEL COMPLEXITY TRENDS ANALYSIS\n",
      "-------------------------------------------------------\n",
      "Model           Speedup  Improvement (%)    Assessment     \n",
      "-------------------------------------------------------\n",
      "Simple Ops      0.98     -2.0               ‚ö†Ô∏è  Minimal    \n",
      "Medium Model    1.22     22.4               ‚ö° Moderate     \n",
      "Complex Model   0.99     -1.4               ‚ö†Ô∏è  Minimal    \n",
      "Very Complex    0.98     -2.1               ‚ö†Ô∏è  Minimal    \n",
      "\n",
      "üéØ COMPILATION MODES COMPARISON\n",
      "========================================\n",
      "   Using Medium Model (Hidden: 512) with input (16, 256, 512)\n",
      "\n",
      "‚öôÔ∏è  Measuring baseline (uncompiled) for mode comparison...\n",
      "   üìä Results for Very Complex:\n",
      "      Baseline: 1824.017 ¬± 15.975 ms\n",
      "      Optimized: 1863.182 ¬± 15.745 ms\n",
      "      Compilation Time: 4922.5 ms\n",
      "      Speedup: 0.98x (-2.1% improvement)\n",
      "\n",
      "üìà MODEL COMPLEXITY TRENDS ANALYSIS\n",
      "-------------------------------------------------------\n",
      "Model           Speedup  Improvement (%)    Assessment     \n",
      "-------------------------------------------------------\n",
      "Simple Ops      0.98     -2.0               ‚ö†Ô∏è  Minimal    \n",
      "Medium Model    1.22     22.4               ‚ö° Moderate     \n",
      "Complex Model   0.99     -1.4               ‚ö†Ô∏è  Minimal    \n",
      "Very Complex    0.98     -2.1               ‚ö†Ô∏è  Minimal    \n",
      "\n",
      "üéØ COMPILATION MODES COMPARISON\n",
      "========================================\n",
      "   Using Medium Model (Hidden: 512) with input (16, 256, 512)\n",
      "\n",
      "‚öôÔ∏è  Measuring baseline (uncompiled) for mode comparison...\n",
      "   üìä Baseline: 28.539ms ¬± 0.265ms\n",
      "\n",
      "‚öôÔ∏è  Testing mode: default\n",
      "   Compilation time for mode 'default': 151.6 ms\n",
      "   üìä Baseline: 28.539ms ¬± 0.265ms\n",
      "\n",
      "‚öôÔ∏è  Testing mode: default\n",
      "   Compilation time for mode 'default': 151.6 ms\n",
      "   üìä default (Optimized): 23.471ms ¬± 0.391ms\n",
      "\n",
      "‚öôÔ∏è  Testing mode: reduce-overhead\n",
      "   üìä default (Optimized): 23.471ms ¬± 0.391ms\n",
      "\n",
      "‚öôÔ∏è  Testing mode: reduce-overhead\n",
      "   Compilation time for mode 'reduce-overhead': 1985.4 ms\n",
      "   Compilation time for mode 'reduce-overhead': 1985.4 ms\n",
      "   üìä reduce-overhead (Optimized): 25.485ms ¬± 0.480ms\n",
      "\n",
      "‚öôÔ∏è  Testing mode: max-autotune\n",
      "   üìä reduce-overhead (Optimized): 25.485ms ¬± 0.480ms\n",
      "\n",
      "‚öôÔ∏è  Testing mode: max-autotune\n",
      "   Compilation time for mode 'max-autotune': 1450.0 ms\n",
      "   Compilation time for mode 'max-autotune': 1450.0 ms\n",
      "   üìä max-autotune (Optimized): 25.245ms ¬± 0.591ms\n",
      "\n",
      "üéØ COMPILATION MODE COMPARISON ANALYSIS\n",
      "----------------------------------------------------------------------\n",
      "Mode               Exec Time (ms)     Compile Time (ms)    Speedup vs Base\n",
      "----------------------------------------------------------------------\n",
      "default            23.471             151.6                1.22           x\n",
      "reduce-overhead    25.485             1985.4               1.12           x\n",
      "max-autotune       25.245             1450.0               1.13           x\n",
      "\n",
      "üèÜ Best performing mode (by execution time): default (23.471ms exec)\n",
      "\n",
      "üìà INPUT SCALING ANALYSIS\n",
      "========================================\n",
      "\n",
      "üìè Testing scale: B8_S64_H256\n",
      "   üìä max-autotune (Optimized): 25.245ms ¬± 0.591ms\n",
      "\n",
      "üéØ COMPILATION MODE COMPARISON ANALYSIS\n",
      "----------------------------------------------------------------------\n",
      "Mode               Exec Time (ms)     Compile Time (ms)    Speedup vs Base\n",
      "----------------------------------------------------------------------\n",
      "default            23.471             151.6                1.22           x\n",
      "reduce-overhead    25.485             1985.4               1.12           x\n",
      "max-autotune       25.245             1450.0               1.13           x\n",
      "\n",
      "üèÜ Best performing mode (by execution time): default (23.471ms exec)\n",
      "\n",
      "üìà INPUT SCALING ANALYSIS\n",
      "========================================\n",
      "\n",
      "üìè Testing scale: B8_S64_H256\n",
      "   üìä B8_S64_H256 (Optimized): 2.090ms\n",
      "\n",
      "üìè Testing scale: B8_S128_H512\n",
      "   üìä B8_S64_H256 (Optimized): 2.090ms\n",
      "\n",
      "üìè Testing scale: B8_S128_H512\n",
      "   üìä B8_S128_H512 (Optimized): 6.023ms\n",
      "\n",
      "üìè Testing scale: B8_S256_H1024\n",
      "   üìä B8_S128_H512 (Optimized): 6.023ms\n",
      "\n",
      "üìè Testing scale: B8_S256_H1024\n",
      "   üìä B8_S256_H1024 (Optimized): 34.933ms\n",
      "\n",
      "üìè Testing scale: B8_S512_H2048\n",
      "   üìä B8_S256_H1024 (Optimized): 34.933ms\n",
      "\n",
      "üìè Testing scale: B8_S512_H2048\n",
      "   üìä B8_S512_H2048 (Optimized): 251.841ms\n",
      "\n",
      "üìà INPUT SCALING TRENDS ANALYSIS (Elements/ms)\n",
      "--------------------------------------------------\n",
      "Scale Config (B,S,H)      Elements/ms (K)     \n",
      "--------------------------------------------------\n",
      "B8_S64_H256               62.7                \n",
      "B8_S128_H512              87.0                \n",
      "B8_S256_H1024             60.0                \n",
      "B8_S512_H2048             33.3                \n",
      "\n",
      "üéì Comprehensive Benchmarking Complete!\n",
      "   üìä Use these results to guide optimization decisions.\n",
      "   üéØ Focus compilation efforts on models and configurations showing significant speedup (e.g., >1.5x).\n",
      "   ‚ö° Consider input scaling behavior when designing production systems, especially for models sensitive to tensor sizes.\n",
      "   ‚öôÔ∏è  Evaluate different compilation modes; 'max-autotune' might offer better runtime but costs more compile time.\n",
      "   üìä B8_S512_H2048 (Optimized): 251.841ms\n",
      "\n",
      "üìà INPUT SCALING TRENDS ANALYSIS (Elements/ms)\n",
      "--------------------------------------------------\n",
      "Scale Config (B,S,H)      Elements/ms (K)     \n",
      "--------------------------------------------------\n",
      "B8_S64_H256               62.7                \n",
      "B8_S128_H512              87.0                \n",
      "B8_S256_H1024             60.0                \n",
      "B8_S512_H2048             33.3                \n",
      "\n",
      "üéì Comprehensive Benchmarking Complete!\n",
      "   üìä Use these results to guide optimization decisions.\n",
      "   üéØ Focus compilation efforts on models and configurations showing significant speedup (e.g., >1.5x).\n",
      "   ‚ö° Consider input scaling behavior when designing production systems, especially for models sensitive to tensor sizes.\n",
      "   ‚öôÔ∏è  Evaluate different compilation modes; 'max-autotune' might offer better runtime but costs more compile time.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import statistics\n",
    "import numpy as np\n",
    "\n",
    "# Ensure the device is set\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "### üß™ Comprehensive Performance Benchmarking Framework\n",
    "\n",
    "class AdvancedBenchmarkSuite:\n",
    "    \"\"\"\n",
    "    Professional-grade benchmarking suite for torch.compile() performance analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, device=device, num_trials=20, warmup_trials=5):\n",
    "        self.device = device\n",
    "        self.num_trials = num_trials\n",
    "        self.warmup_trials = warmup_trials\n",
    "        self.results = {}\n",
    "        \n",
    "    def benchmark_model_complexity(self):\n",
    "        \"\"\"Analyze performance across different model complexities\"\"\"\n",
    "        \n",
    "        print(\"üß™ MODEL COMPLEXITY ANALYSIS\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Define test models of increasing complexity\n",
    "        # Shapes are (sequence_length, hidden_size) for input tensor (batch_size, seq_len, hidden_size)\n",
    "        test_configurations = [\n",
    "            (\"Simple Ops\", self._create_simple_model, (128, 256)), # input_shape for features\n",
    "            (\"Medium Model\", self._create_medium_model, (256, 512)), \n",
    "            (\"Complex Model\", self._create_complex_model, (512, 1024)),\n",
    "            (\"Very Complex\", self._create_very_complex_model, (256, 2048)) # Example: smaller seq_len, larger hidden\n",
    "        ]\n",
    "        \n",
    "        complexity_results_list = [] # Renamed for clarity\n",
    "        \n",
    "        for config_name, model_factory, model_input_features_shape in test_configurations:\n",
    "            print(f\"\\nüî¨ Testing: {config_name}\")\n",
    "            # Assuming a fixed batch size for these tests, e.g., 16\n",
    "            batch_size = 16 \n",
    "            actual_input_shape = (batch_size, *model_input_features_shape)\n",
    "            print(f\"   Model Input Features Shape (SeqLen, Hidden): {model_input_features_shape}\")\n",
    "            print(f\"   Actual Tensor Shape (Batch, SeqLen, Hidden): {actual_input_shape}\")\n",
    "            \n",
    "            # Create model and test data\n",
    "            model = model_factory(model_input_features_shape[1]).to(self.device) # Pass hidden_size to factory\n",
    "            test_input = torch.randn(actual_input_shape, device=self.device)\n",
    "            \n",
    "            # Benchmark this configuration\n",
    "            result_stats = self._benchmark_single_config(model, test_input, config_name) # Renamed\n",
    "            complexity_results_list.append(result_stats)\n",
    "            \n",
    "            # Print immediate results\n",
    "            self._print_benchmark_result(result_stats)\n",
    "        \n",
    "        # Analyze complexity trends\n",
    "        self._analyze_complexity_trends(complexity_results_list)\n",
    "        return complexity_results_list\n",
    "    \n",
    "    def benchmark_compilation_modes(self):\n",
    "        \"\"\"Compare different torch.compile() modes\"\"\"\n",
    "        \n",
    "        print(f\"\\nüéØ COMPILATION MODES COMPARISON\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Test model - using medium model as a standard test case\n",
    "        # Define a standard input shape for this comparison\n",
    "        medium_model_hidden_size = 512\n",
    "        medium_model_input_shape = (16, 256, medium_model_hidden_size) # Batch, Seq, Hidden\n",
    "        \n",
    "        model = self._create_medium_model(medium_model_hidden_size).to(self.device)\n",
    "        test_input = torch.randn(medium_model_input_shape, device=self.device)\n",
    "        print(f\"   Using Medium Model (Hidden: {medium_model_hidden_size}) with input {medium_model_input_shape}\")\n",
    "\n",
    "        compilation_modes_to_test = [ # Renamed\n",
    "            (\"default\", {\"mode\": \"default\"}),\n",
    "            (\"reduce-overhead\", {\"mode\": \"reduce-overhead\"}),\n",
    "            (\"max-autotune\", {\"mode\": \"max-autotune\"}),\n",
    "        ]\n",
    "        \n",
    "        mode_results_list = [] # Renamed\n",
    "        \n",
    "        # Baseline for comparison (uncompiled)\n",
    "        print(f\"\\n‚öôÔ∏è  Measuring baseline (uncompiled) for mode comparison...\")\n",
    "        baseline_times = self._measure_baseline(model, test_input)\n",
    "        baseline_mean_ms = statistics.mean(baseline_times) * 1000\n",
    "        baseline_std_ms = statistics.stdev(baseline_times) * 1000 if len(baseline_times) > 1 else 0\n",
    "        print(f\"   üìä Baseline: {baseline_mean_ms:.3f}ms ¬± {baseline_std_ms:.3f}ms\")\n",
    "\n",
    "        for mode_name, compile_config in compilation_modes_to_test:\n",
    "            print(f\"\\n‚öôÔ∏è  Testing mode: {mode_name}\")\n",
    "            \n",
    "            # Benchmark this mode\n",
    "            torch._dynamo.reset() # Reset cache for each mode\n",
    "            \n",
    "            # Measure compilation time separately for modes\n",
    "            compile_start_time = time.perf_counter()\n",
    "            compiled_model = torch.compile(model, **compile_config)\n",
    "            # First inference to ensure compilation finishes\n",
    "            with torch.no_grad():\n",
    "                _ = compiled_model(test_input) \n",
    "            if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "            compilation_duration_ms = (time.perf_counter() - compile_start_time) * 1000\n",
    "            print(f\"   Compilation time for mode '{mode_name}': {compilation_duration_ms:.1f} ms\")\n",
    "\n",
    "            result_stats = self._benchmark_compiled_model(compiled_model, test_input, f\"mode_{mode_name}\")\n",
    "            result_stats['mode'] = mode_name\n",
    "            result_stats['compilation_ms'] = compilation_duration_ms # Add compilation time to results\n",
    "            result_stats['baseline_mean_ms_for_speedup'] = baseline_mean_ms # For speedup calculation against common baseline\n",
    "            mode_results_list.append(result_stats)\n",
    "            \n",
    "            print(f\"   üìä {mode_name} (Optimized): {result_stats['optimized_mean_ms']:.3f}ms ¬± {result_stats['optimized_std_ms']:.3f}ms\")\n",
    "        \n",
    "        self._analyze_mode_comparison(mode_results_list, baseline_mean_ms)\n",
    "        return mode_results_list\n",
    "    \n",
    "    def benchmark_input_scaling(self):\n",
    "        \"\"\"Analyze performance scaling with input size\"\"\"\n",
    "        \n",
    "        print(f\"\\nüìà INPUT SCALING ANALYSIS\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Using medium model structure, vary hidden_size and seq_len\n",
    "        medium_model_base_hidden_size = 512 # For _create_medium_model\n",
    "        \n",
    "        # Different input scales (SeqLen, HiddenSize)\n",
    "        input_scales_to_test = [ # Renamed\n",
    "            (64, 256),   # Small\n",
    "            (128, 512),  # Medium\n",
    "            (256, 1024), # Large\n",
    "            (512, 2048), # Very Large\n",
    "        ]\n",
    "        \n",
    "        scaling_results_list = [] # Renamed\n",
    "        batch_size = 8 # Fixed batch size for scaling test\n",
    "\n",
    "        for seq_len, hidden_size in input_scales_to_test:\n",
    "            scale_name = f\"B{batch_size}_S{seq_len}_H{hidden_size}\" # More descriptive name\n",
    "            print(f\"\\nüìè Testing scale: {scale_name}\")\n",
    "            \n",
    "            try:\n",
    "                model_instance = self._create_medium_model(hidden_size).to(self.device) # Create model with current hidden_size\n",
    "                test_input = torch.randn(batch_size, seq_len, hidden_size, device=self.device)\n",
    "                \n",
    "                torch._dynamo.reset() # Reset cache for each scale config\n",
    "                compiled_model = torch.compile(model_instance) # Compile with default mode\n",
    "                \n",
    "                result_stats = self._benchmark_compiled_model(compiled_model, test_input, f\"scale_{scale_name}\")\n",
    "                result_stats['scale_config'] = {'batch': batch_size, 'seq_len': seq_len, 'hidden_size': hidden_size} # Store scale config\n",
    "                result_stats['total_elements'] = batch_size * seq_len * hidden_size\n",
    "                scaling_results_list.append(result_stats)\n",
    "                \n",
    "                print(f\"   üìä {scale_name} (Optimized): {result_stats['optimized_mean_ms']:.3f}ms\")\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                print(f\"   ‚ùå Scale {scale_name} failed: {e}. Skipping this configuration.\")\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    print(\"      This is likely an Out Of Memory error. Try reducing batch_size or model dimensions for this scale.\")\n",
    "        \n",
    "        self._analyze_scaling_trends(scaling_results_list)\n",
    "        return scaling_results_list\n",
    "    \n",
    "    def _benchmark_single_config(self, model, test_input, config_name_str): # Renamed\n",
    "        \"\"\"Benchmark a single model configuration (baseline vs compiled)\"\"\"\n",
    "        \n",
    "        # Baseline measurement\n",
    "        baseline_times_list = self._measure_baseline(model, test_input) # Renamed\n",
    "        \n",
    "        # Compiled measurement\n",
    "        torch._dynamo.reset() # Clear cache before compiling\n",
    "        \n",
    "        compile_start_time = time.perf_counter()\n",
    "        compiled_model = torch.compile(model) # Default mode\n",
    "        # First inference to ensure compilation finishes\n",
    "        with torch.no_grad():\n",
    "            _ = compiled_model(test_input)\n",
    "        if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "        compilation_duration_ms = (time.perf_counter() - compile_start_time) * 1000\n",
    "        \n",
    "        compiled_times_list = self._measure_compiled(compiled_model, test_input) # Renamed\n",
    "        \n",
    "        stats = self._calculate_benchmark_stats(baseline_times_list, compiled_times_list, config_name_str)\n",
    "        stats['compilation_ms'] = compilation_duration_ms # Add compilation time\n",
    "        return stats\n",
    "    \n",
    "    def _benchmark_compiled_model(self, compiled_model, test_input, config_name_str): # Renamed\n",
    "        \"\"\"Benchmark an already compiled model (measures execution time only)\"\"\"\n",
    "        \n",
    "        # Just measure compiled performance\n",
    "        compiled_times_list = self._measure_compiled(compiled_model, test_input) # Renamed\n",
    "        \n",
    "        # Basic stats for compiled execution\n",
    "        mean_val = statistics.mean(compiled_times_list) * 1000\n",
    "        std_val = statistics.stdev(compiled_times_list) * 1000 if len(compiled_times_list) > 1 else 0\n",
    "        median_val = statistics.median(compiled_times_list) * 1000\n",
    "        \n",
    "        return {\n",
    "            'config_name': config_name_str,\n",
    "            'optimized_times_ms': [t * 1000 for t in compiled_times_list], # Store times in ms\n",
    "            'optimized_mean_ms': mean_val,\n",
    "            'optimized_std_ms': std_val,\n",
    "            'optimized_median_ms': median_val,\n",
    "        }\n",
    "    \n",
    "    def _measure_baseline(self, model_to_test, test_input_tensor): # Renamed\n",
    "        \"\"\"Measure baseline (uncompiled) performance\"\"\"\n",
    "        \n",
    "        model_to_test.eval() # Ensure eval mode\n",
    "        times_list = [] # Renamed\n",
    "        with torch.no_grad(): # Ensure no_grad for inference\n",
    "            # Warmup\n",
    "            for _ in range(self.warmup_trials):\n",
    "                _ = model_to_test(test_input_tensor)\n",
    "            if torch.cuda.is_available(): torch.cuda.synchronize() # Sync after warmup loop\n",
    "            \n",
    "            # Measurement\n",
    "            for _ in range(self.num_trials):\n",
    "                if torch.cuda.is_available(): torch.cuda.synchronize() # Sync before timing\n",
    "                \n",
    "                start_time = time.perf_counter()\n",
    "                _ = model_to_test(test_input_tensor)\n",
    "                \n",
    "                if torch.cuda.is_available(): torch.cuda.synchronize() # Sync after operation\n",
    "                \n",
    "                times_list.append(time.perf_counter() - start_time)\n",
    "        \n",
    "        return times_list\n",
    "    \n",
    "    def _measure_compiled(self, compiled_model_instance, test_input_tensor): # Renamed\n",
    "        \"\"\"Measure compiled model performance (assumes compilation already happened or is part of first call)\"\"\"\n",
    "        \n",
    "        compiled_model_instance.eval() # Ensure eval mode\n",
    "        times_list = [] # Renamed\n",
    "        with torch.no_grad(): # Ensure no_grad for inference\n",
    "            # First run (might include final parts of JIT, or just be a regular run if fully AOT compiled)\n",
    "            _ = compiled_model_instance(test_input_tensor)\n",
    "            if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "\n",
    "            # Warmup (for compiled model)\n",
    "            for _ in range(self.warmup_trials):\n",
    "                _ = compiled_model_instance(test_input_tensor)\n",
    "            if torch.cuda.is_available(): torch.cuda.synchronize() # Sync after warmup loop\n",
    "            \n",
    "            # Measurement\n",
    "            for _ in range(self.num_trials):\n",
    "                if torch.cuda.is_available(): torch.cuda.synchronize() # Sync before timing\n",
    "                \n",
    "                start_time = time.perf_counter()\n",
    "                _ = compiled_model_instance(test_input_tensor)\n",
    "                \n",
    "                if torch.cuda.is_available(): torch.cuda.synchronize() # Sync after operation\n",
    "                \n",
    "                times_list.append(time.perf_counter() - start_time)\n",
    "        \n",
    "        return times_list\n",
    "    \n",
    "    def _calculate_benchmark_stats(self, baseline_times_list, compiled_times_list, config_name_str): # Renamed\n",
    "        \"\"\"Calculate comprehensive benchmark statistics\"\"\"\n",
    "        \n",
    "        baseline_mean = statistics.mean(baseline_times_list)\n",
    "        baseline_std = statistics.stdev(baseline_times_list) if len(baseline_times_list) > 1 else 0\n",
    "        \n",
    "        compiled_mean = statistics.mean(compiled_times_list)\n",
    "        compiled_std = statistics.stdev(compiled_times_list) if len(compiled_times_list) > 1 else 0\n",
    "        \n",
    "        speedup_factor = baseline_mean / compiled_mean if compiled_mean > 0 else float('inf') # Renamed\n",
    "        \n",
    "        return {\n",
    "            'config_name': config_name_str,\n",
    "            'baseline_mean_ms': baseline_mean * 1000,\n",
    "            'baseline_std_ms': baseline_std * 1000,\n",
    "            'baseline_times_ms': [t * 1000 for t in baseline_times_list],\n",
    "            'optimized_mean_ms': compiled_mean * 1000,\n",
    "            'optimized_std_ms': compiled_std * 1000,\n",
    "            'optimized_times_ms': [t * 1000 for t in compiled_times_list],\n",
    "            'speedup': speedup_factor,\n",
    "            'improvement_pct': (speedup_factor - 1) * 100 if speedup_factor > 0 else (-float('inf') if speedup_factor == 0 else 0) # Handle no speedup or slowdown\n",
    "        }\n",
    "    \n",
    "    def _print_benchmark_result(self, result_stats): # Renamed\n",
    "        \"\"\"Print formatted benchmark result\"\"\"\n",
    "        print(f\"   üìä Results for {result_stats['config_name']}:\")\n",
    "        print(f\"      Baseline: {result_stats['baseline_mean_ms']:.3f} ¬± {result_stats['baseline_std_ms']:.3f} ms\")\n",
    "        print(f\"      Optimized: {result_stats['optimized_mean_ms']:.3f} ¬± {result_stats['optimized_std_ms']:.3f} ms\")\n",
    "        if 'compilation_ms' in result_stats:\n",
    "             print(f\"      Compilation Time: {result_stats['compilation_ms']:.1f} ms\")\n",
    "        print(f\"      Speedup: {result_stats['speedup']:.2f}x ({result_stats['improvement_pct']:.1f}% improvement)\")\n",
    "    \n",
    "    def _analyze_complexity_trends(self, complexity_results_list): # Renamed\n",
    "        \"\"\"Analyze trends across model complexities\"\"\"\n",
    "        print(f\"\\nüìà MODEL COMPLEXITY TRENDS ANALYSIS\")\n",
    "        print(\"-\" * 55) # Adjusted width\n",
    "        \n",
    "        print(f\"{'Model':<15} {'Speedup':<8} {'Improvement (%)':<18} {'Assessment':<15}\") # Adjusted headers\n",
    "        print(\"-\" * 55)\n",
    "        \n",
    "        for result_stats in complexity_results_list: # Renamed\n",
    "            speedup_val = result_stats['speedup'] # Renamed\n",
    "            improvement_val = result_stats['improvement_pct'] # Renamed\n",
    "            \n",
    "            if speedup_val > 2.0: assessment_str = \"üöÄ Excellent\" # Renamed\n",
    "            elif speedup_val > 1.5: assessment_str = \"‚úÖ Good\"\n",
    "            elif speedup_val > 1.1: assessment_str = \"‚ö° Moderate\"\n",
    "            elif speedup_val > 0: assessment_str = \"‚ö†Ô∏è  Minimal\"\n",
    "            else: assessment_str = \"‚ùå Slowdown\"\n",
    "            \n",
    "            print(f\"{result_stats['config_name']:<15} {speedup_val:<8.2f} {improvement_val:<18.1f} {assessment_str:<15}\")\n",
    "    \n",
    "    def _analyze_mode_comparison(self, mode_results_list, baseline_mean_ms_for_comparison): # Renamed\n",
    "        \"\"\"Analyze compilation mode performance\"\"\"\n",
    "        print(f\"\\nüéØ COMPILATION MODE COMPARISON ANALYSIS\")\n",
    "        print(\"-\" * 70) # Adjusted width\n",
    "        print(f\"{'Mode':<18} {'Exec Time (ms)':<18} {'Compile Time (ms)':<20} {'Speedup vs Base':<15}\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "        if not mode_results_list:\n",
    "            print(\"   No mode results to analyze.\")\n",
    "            return\n",
    "\n",
    "        best_mode_by_exec = min(mode_results_list, key=lambda x: x['optimized_mean_ms'])\n",
    "        \n",
    "        for result_stats in mode_results_list: # Renamed\n",
    "            speedup_vs_baseline = baseline_mean_ms_for_comparison / result_stats['optimized_mean_ms'] if result_stats['optimized_mean_ms'] > 0 else float('inf')\n",
    "            print(f\"{result_stats['mode']:<18} {result_stats['optimized_mean_ms']:<18.3f} {result_stats.get('compilation_ms', 'N/A'):<20.1f} {speedup_vs_baseline:<15.2f}x\")\n",
    "            \n",
    "        print(f\"\\nüèÜ Best performing mode (by execution time): {best_mode_by_exec['mode']} ({best_mode_by_exec['optimized_mean_ms']:.3f}ms exec)\")\n",
    "    \n",
    "    def _analyze_scaling_trends(self, scaling_results_list): # Renamed\n",
    "        \"\"\"Analyze input scaling trends\"\"\"\n",
    "        print(f\"\\nüìà INPUT SCALING TRENDS ANALYSIS (Elements/ms)\")\n",
    "        print(\"-\" * 50) # Adjusted width\n",
    "        print(f\"{'Scale Config (B,S,H)':<25} {'Elements/ms (K)':<20}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        if not scaling_results_list:\n",
    "            print(\"   No scaling results to analyze.\")\n",
    "            return\n",
    "\n",
    "        for result_stats in scaling_results_list: # Renamed\n",
    "            elements_per_ms = result_stats['total_elements'] / result_stats['optimized_mean_ms'] if result_stats['optimized_mean_ms'] > 0 else 0\n",
    "            scale_cfg = result_stats['scale_config']\n",
    "            cfg_str = f\"B{scale_cfg['batch']}_S{scale_cfg['seq_len']}_H{scale_cfg['hidden_size']}\"\n",
    "            print(f\"{cfg_str:<25} {elements_per_ms/1000:<20.1f}\")\n",
    "    \n",
    "    # Model factories for different complexities\n",
    "    # These now accept hidden_size as an argument, assuming seq_len is part of input_shape\n",
    "    def _create_simple_model(self, hidden_size, input_seq_len=128): # Default seq_len if not from shape\n",
    "        # Simple model: Linear -> ReLU -> Linear. Input: (Batch, SeqLen, HiddenIn)\n",
    "        # For this example, let's assume hidden_size is the input feature size for the first linear layer.\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size), # Input features = hidden_size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size)  # Output features = hidden_size\n",
    "        )\n",
    "    \n",
    "    def _create_medium_model(self, hidden_size, input_seq_len=256):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size * 2), # Example: expand\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size * 2, hidden_size), # Example: contract\n",
    "            nn.LayerNorm(hidden_size)\n",
    "        )\n",
    "    \n",
    "    def _create_complex_model(self, hidden_size, input_seq_len=512):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size * 4, hidden_size * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size * 4, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.GELU()\n",
    "        )\n",
    "    \n",
    "    def _create_very_complex_model(self, hidden_size, input_seq_len=256): # Example: Transformer-like block\n",
    "        # A more complex model with multiple layers, e.g., a few transformer blocks\n",
    "        # This is a simplified example, not a full transformer block\n",
    "        layers = []\n",
    "        num_layers = 4 # Example: 4 \"blocks\"\n",
    "        current_size = hidden_size\n",
    "        for i in range(num_layers):\n",
    "            intermediate_size = current_size * 4 # Feedforward expansion\n",
    "            layers.extend([\n",
    "                nn.LayerNorm(current_size),\n",
    "                nn.Linear(current_size, intermediate_size),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(intermediate_size, current_size),\n",
    "                nn.Dropout(0.1) # Dropout after FFN\n",
    "            ])\n",
    "            # Add a residual connection concept if this were a real block\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "# Execute comprehensive benchmarking\n",
    "# Ensure 'device' is defined from a previous cell (e.g., setup cell)\n",
    "# global device # If device is from global scope of a previous cell execution\n",
    "if 'device' not in globals():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Warning: 'device' not found in global scope, re-initialized to {device}\")\n",
    "\n",
    "benchmark_suite_instance = AdvancedBenchmarkSuite(device=device) # Renamed\n",
    "\n",
    "print(\"üöÄ LAUNCHING COMPREHENSIVE BENCHMARK SUITE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Run all benchmark categories\n",
    "complexity_results_list = benchmark_suite_instance.benchmark_model_complexity() # Renamed\n",
    "mode_results_list = benchmark_suite_instance.benchmark_compilation_modes() # Renamed\n",
    "scaling_results_list = benchmark_suite_instance.benchmark_input_scaling() # Renamed\n",
    "\n",
    "print(f\"\\nüéì Comprehensive Benchmarking Complete!\")\n",
    "print(f\"   üìä Use these results to guide optimization decisions.\")\n",
    "print(f\"   üéØ Focus compilation efforts on models and configurations showing significant speedup (e.g., >1.5x).\")\n",
    "print(f\"   ‚ö° Consider input scaling behavior when designing production systems, especially for models sensitive to tensor sizes.\")\n",
    "print(f\"   ‚öôÔ∏è  Evaluate different compilation modes; 'max-autotune' might offer better runtime but costs more compile time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89394f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêõ DEBUGGING COMPILATION ISSUES\n",
      "=============================================\n",
      "üîç Issue 1: Graph Breaks from Python control flow dependent on tensor values\n",
      "----------------------------------------------------------------------\n",
      "   Testing function prone to graph breaks (Python if on tensor data):\n",
      "   Expect warnings about graph breaks if TORCH_LOGS includes 'graph_breaks' or similar verbosity.\n",
      "   ‚úÖ Problematic function compiled. Check logs for graph break warnings.\n",
      "   ‚úÖ Problematic function compiled. Check logs for graph break warnings.\n",
      "   ‚úÖ Improved version (with torch.where) compiled. Should have fewer/no graph breaks for this case.\n",
      "\n",
      "üîç Issue 2: Handling Dynamic Shapes in Compiled Functions\n",
      "----------------------------------------------------------------------\n",
      "   Testing with different input shapes (fixed rank, varying dimensions):\n",
      "\n",
      "   Attempt 1: Default compilation (dynamic=False implicitly)\n",
      "      Running with shape torch.Size([10, 20, 5])...\n",
      "   ‚úÖ Improved version (with torch.where) compiled. Should have fewer/no graph breaks for this case.\n",
      "\n",
      "üîç Issue 2: Handling Dynamic Shapes in Compiled Functions\n",
      "----------------------------------------------------------------------\n",
      "   Testing with different input shapes (fixed rank, varying dimensions):\n",
      "\n",
      "   Attempt 1: Default compilation (dynamic=False implicitly)\n",
      "      Running with shape torch.Size([10, 20, 5])...\n",
      "      ‚úÖ Shape torch.Size([10, 20, 5]): Success (may have recompiled if specialization occurred)\n",
      "      Running with shape torch.Size([15, 30, 3])...\n",
      "      ‚úÖ Shape torch.Size([10, 20, 5]): Success (may have recompiled if specialization occurred)\n",
      "      Running with shape torch.Size([15, 30, 3])...\n",
      "      ‚úÖ Shape torch.Size([15, 30, 3]): Success (may have recompiled if specialization occurred)\n",
      "      Running with shape torch.Size([20, 10, 8])...\n",
      "      ‚úÖ Shape torch.Size([20, 10, 8]): Success (may have recompiled if specialization occurred)\n",
      "   ‚úÖ Default compilation handled multiple shapes (possibly via recompilation/specialization).\n",
      "\n",
      "   Attempt 2: Compiling with dynamic=True\n",
      "      Running with shape torch.Size([10, 20, 5]) (dynamic=True)...\n",
      "      ‚úÖ Dynamic (dynamic=True) shape torch.Size([10, 20, 5]): Success\n",
      "      Running with shape torch.Size([15, 30, 3]) (dynamic=True)...\n",
      "      ‚úÖ Dynamic (dynamic=True) shape torch.Size([15, 30, 3]): Success\n",
      "      Running with shape torch.Size([20, 10, 8]) (dynamic=True)...\n",
      "      ‚úÖ Dynamic (dynamic=True) shape torch.Size([20, 10, 8]): Success\n",
      "   ‚úÖ `dynamic=True` compilation successful with varying shapes, likely avoiding recompilations.\n",
      "\n",
      "üîç Issue 3: Performance Regression for Trivial Operations\n",
      "----------------------------------------------------------------------\n",
      "   Measuring baseline for very_simple_operation on shape torch.Size([1000, 1000])...\n",
      "   Measuring compiled version for very_simple_operation...\n",
      "      ‚úÖ Shape torch.Size([15, 30, 3]): Success (may have recompiled if specialization occurred)\n",
      "      Running with shape torch.Size([20, 10, 8])...\n",
      "      ‚úÖ Shape torch.Size([20, 10, 8]): Success (may have recompiled if specialization occurred)\n",
      "   ‚úÖ Default compilation handled multiple shapes (possibly via recompilation/specialization).\n",
      "\n",
      "   Attempt 2: Compiling with dynamic=True\n",
      "      Running with shape torch.Size([10, 20, 5]) (dynamic=True)...\n",
      "      ‚úÖ Dynamic (dynamic=True) shape torch.Size([10, 20, 5]): Success\n",
      "      Running with shape torch.Size([15, 30, 3]) (dynamic=True)...\n",
      "      ‚úÖ Dynamic (dynamic=True) shape torch.Size([15, 30, 3]): Success\n",
      "      Running with shape torch.Size([20, 10, 8]) (dynamic=True)...\n",
      "      ‚úÖ Dynamic (dynamic=True) shape torch.Size([20, 10, 8]): Success\n",
      "   ‚úÖ `dynamic=True` compilation successful with varying shapes, likely avoiding recompilations.\n",
      "\n",
      "üîç Issue 3: Performance Regression for Trivial Operations\n",
      "----------------------------------------------------------------------\n",
      "   Measuring baseline for very_simple_operation on shape torch.Size([1000, 1000])...\n",
      "   Measuring compiled version for very_simple_operation...\n",
      "   Baseline (simple op): 0.7951 ms\n",
      "   Compiled (simple op): 1.0714 ms\n",
      "   ‚ö†Ô∏è  Performance regression detected for trivial operation!\n",
      "      The overhead of compilation and calling the compiled kernel\n",
      "      exceeds the benefit for this very simple operation.\n",
      "   üí° Recommendations:\n",
      "      ‚Ä¢ Avoid compiling extremely simple, non-bottleneck functions.\n",
      "      ‚Ä¢ Profile to identify true bottlenecks before applying torch.compile broadly.\n",
      "\n",
      "üéì Debugging Best Practices Summary:\n",
      "   ‚úÖ Be mindful of Python control flow on tensor data; use `torch.where` or refactor for traceability.\n",
      "   ‚úÖ Use `dynamic=True` or input specialization when dealing with variable input shapes if recompilation is an issue.\n",
      "   ‚úÖ Profile! Not all operations benefit from compilation; overhead can exceed gains for trivial ops.\n",
      "   ‚úÖ Utilize environment variables like `TORCH_LOGS` for detailed insights during debugging.\n",
      "   ‚úÖ Start with simple, isolated examples when debugging, then gradually add complexity.\n",
      "   Baseline (simple op): 0.7951 ms\n",
      "   Compiled (simple op): 1.0714 ms\n",
      "   ‚ö†Ô∏è  Performance regression detected for trivial operation!\n",
      "      The overhead of compilation and calling the compiled kernel\n",
      "      exceeds the benefit for this very simple operation.\n",
      "   üí° Recommendations:\n",
      "      ‚Ä¢ Avoid compiling extremely simple, non-bottleneck functions.\n",
      "      ‚Ä¢ Profile to identify true bottlenecks before applying torch.compile broadly.\n",
      "\n",
      "üéì Debugging Best Practices Summary:\n",
      "   ‚úÖ Be mindful of Python control flow on tensor data; use `torch.where` or refactor for traceability.\n",
      "   ‚úÖ Use `dynamic=True` or input specialization when dealing with variable input shapes if recompilation is an issue.\n",
      "   ‚úÖ Profile! Not all operations benefit from compilation; overhead can exceed gains for trivial ops.\n",
      "   ‚úÖ Utilize environment variables like `TORCH_LOGS` for detailed insights during debugging.\n",
      "   ‚úÖ Start with simple, isolated examples when debugging, then gradually add complexity.\n"
     ]
    }
   ],
   "source": [
    "# üîç Debugging Compilation Issues: Common Problems and Solutions\n",
    "\n",
    "def demonstrate_common_issues():\n",
    "    \"\"\"\n",
    "    Show common compilation issues and how to debug and fix them\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üêõ DEBUGGING COMPILATION ISSUES\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Issue 1: Graph Breaks from Dynamic Control Flow\n",
    "    print(\"üîç Issue 1: Graph Breaks from Python control flow dependent on tensor values\")\n",
    "    print(\"-\" * 70) # Adjusted width\n",
    "    \n",
    "    # Ensure device is defined\n",
    "    if 'device' not in globals():\n",
    "        current_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Warning: 'device' not found, using {current_device}\")\n",
    "    else:\n",
    "        current_device = device\n",
    "\n",
    "\n",
    "    # Problematic function: Python if statement on tensor data\n",
    "    def problematic_function_graph_break(x):\n",
    "        y = torch.relu(x)\n",
    "        # This condition is evaluated at runtime based on tensor data - causes graph break\n",
    "        if x.sum() > 0:  \n",
    "            return y + 1.0\n",
    "        else:\n",
    "            return y - 1.0\n",
    "    \n",
    "    # Improved function: Use torch.where for conditional logic on tensors\n",
    "    def improved_function_no_graph_break(x):\n",
    "        y = torch.relu(x)\n",
    "        # torch.where is traceable and avoids graph break for this pattern\n",
    "        condition = x.sum() > 0 \n",
    "        return torch.where(condition, y + 1.0, y - 1.0)\n",
    "    \n",
    "    test_input_graph_break = torch.randn(100, device=current_device)\n",
    "    \n",
    "    print(\"   Testing function prone to graph breaks (Python if on tensor data):\")\n",
    "    print(\"   Expect warnings about graph breaks if TORCH_LOGS includes 'graph_breaks' or similar verbosity.\")\n",
    "    \n",
    "    try:\n",
    "        # Enable graph break logging if not already on (for this specific test)\n",
    "        original_torch_logs = os.environ.get(\"TORCH_LOGS\")\n",
    "        os.environ[\"TORCH_LOGS\"] = str(original_torch_logs or \"\") + \",graph_breaks\" \n",
    "        torch._dynamo.reset() # Reset to apply new env var\n",
    "\n",
    "        compiled_problematic = torch.compile(problematic_function_graph_break)\n",
    "        result1 = compiled_problematic(test_input_graph_break)\n",
    "        print(\"   ‚úÖ Problematic function compiled. Check logs for graph break warnings.\")\n",
    "        \n",
    "        torch._dynamo.reset() # Reset for the next compilation\n",
    "        compiled_improved = torch.compile(improved_function_no_graph_break)\n",
    "        result2 = compiled_improved(test_input_graph_break)\n",
    "        print(\"   ‚úÖ Improved version (with torch.where) compiled. Should have fewer/no graph breaks for this case.\")\n",
    "\n",
    "        # Restore TORCH_LOGS\n",
    "        if original_torch_logs is None:\n",
    "            del os.environ[\"TORCH_LOGS\"]\n",
    "        else:\n",
    "            os.environ[\"TORCH_LOGS\"] = original_torch_logs\n",
    "        torch._dynamo.reset()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Compilation issue during graph break demo: {e}\")\n",
    "    \n",
    "    # Issue 2: Dynamic Shapes\n",
    "    print(f\"\\nüîç Issue 2: Handling Dynamic Shapes in Compiled Functions\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Function whose behavior might implicitly depend on shape details not just rank/dtype\n",
    "    def shape_sensitive_function_reshape(x):\n",
    "        # Example: reshape that might be problematic if not specialized or dynamic=True\n",
    "        return x.view(x.shape[0], -1).mean(dim=0) # Flatten all but batch\n",
    "    \n",
    "    shapes_to_test_dynamic = [\n",
    "        (10, 20, 5), \n",
    "        (15, 30, 3),   \n",
    "        (20, 10, 8), \n",
    "    ]\n",
    "    \n",
    "    print(\"   Testing with different input shapes (fixed rank, varying dimensions):\")\n",
    "    \n",
    "    print(\"\\n   Attempt 1: Default compilation (dynamic=False implicitly)\")\n",
    "    try:\n",
    "        torch._dynamo.reset()\n",
    "        # Default compilation might lead to recompilations or errors if shapes vary too much\n",
    "        compiled_static_shapes = torch.compile(shape_sensitive_function_reshape) \n",
    "        \n",
    "        for i, shape_dims in enumerate(shapes_to_test_dynamic):\n",
    "            test_tensor_dyn = torch.randn(shape_dims, device=current_device)\n",
    "            print(f\"      Running with shape {test_tensor_dyn.shape}...\")\n",
    "            _ = compiled_static_shapes(test_tensor_dyn)\n",
    "            print(f\"      ‚úÖ Shape {test_tensor_dyn.shape}: Success (may have recompiled if specialization occurred)\")\n",
    "        print(\"   ‚úÖ Default compilation handled multiple shapes (possibly via recompilation/specialization).\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Default compilation issue with varying shapes: {e}\")\n",
    "    \n",
    "    print(\"\\n   Attempt 2: Compiling with dynamic=True\")\n",
    "    try:\n",
    "        torch._dynamo.reset()\n",
    "        compiled_dynamic_shapes = torch.compile(shape_sensitive_function_reshape, dynamic=True)\n",
    "        \n",
    "        for i, shape_dims in enumerate(shapes_to_test_dynamic):\n",
    "            test_tensor_dyn = torch.randn(shape_dims, device=current_device)\n",
    "            print(f\"      Running with shape {test_tensor_dyn.shape} (dynamic=True)...\")\n",
    "            _ = compiled_dynamic_shapes(test_tensor_dyn)\n",
    "            print(f\"      ‚úÖ Dynamic (dynamic=True) shape {test_tensor_dyn.shape}: Success\")\n",
    "        print(\"   ‚úÖ `dynamic=True` compilation successful with varying shapes, likely avoiding recompilations.\")\n",
    "\n",
    "    except Exception as e2:\n",
    "        print(f\"   ‚ùå Still failing with dynamic=True: {e2}\")\n",
    "    \n",
    "    # Issue 3: Performance Regression Detection for very simple operations\n",
    "    print(f\"\\nüîç Issue 3: Performance Regression for Trivial Operations\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    def very_simple_operation(x):\n",
    "        # Extremely simple operation that might not benefit from compilation overhead\n",
    "        return x + 1.0\n",
    "    \n",
    "    test_tensor_simple_op = torch.randn(1000, 1000, device=current_device) # Larger tensor\n",
    "    \n",
    "    print(f\"   Measuring baseline for very_simple_operation on shape {test_tensor_simple_op.shape}...\")\n",
    "    baseline_times_simple = []\n",
    "    for _ in range(10): # Fewer iterations for quick demo\n",
    "        if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "        start_time = time.perf_counter()\n",
    "        _ = very_simple_operation(test_tensor_simple_op)\n",
    "        if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "        baseline_times_simple.append(time.perf_counter() - start_time)\n",
    "    baseline_avg_simple = statistics.mean(baseline_times_simple)\n",
    "    \n",
    "    print(f\"   Measuring compiled version for very_simple_operation...\")\n",
    "    torch._dynamo.reset()\n",
    "    compiled_very_simple = torch.compile(very_simple_operation)\n",
    "    \n",
    "    # Warmup and first run (includes compilation time)\n",
    "    _ = compiled_very_simple(test_tensor_simple_op) \n",
    "    if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "\n",
    "    compiled_times_simple = []\n",
    "    for _ in range(10):\n",
    "        if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "        start_time = time.perf_counter()\n",
    "        _ = compiled_very_simple(test_tensor_simple_op)\n",
    "        if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "        compiled_times_simple.append(time.perf_counter() - start_time)\n",
    "    compiled_avg_simple = statistics.mean(compiled_times_simple)\n",
    "    \n",
    "    print(f\"   Baseline (simple op): {baseline_avg_simple*1000:.4f} ms\")\n",
    "    print(f\"   Compiled (simple op): {compiled_avg_simple*1000:.4f} ms\")\n",
    "    \n",
    "    # Regression if compiled is, e.g., 5% slower (allowing for noise)\n",
    "    if compiled_avg_simple > baseline_avg_simple * 1.05:  \n",
    "        print(\"   ‚ö†Ô∏è  Performance regression detected for trivial operation!\")\n",
    "        print(\"      The overhead of compilation and calling the compiled kernel\")\n",
    "        print(\"      exceeds the benefit for this very simple operation.\")\n",
    "        print(\"   üí° Recommendations:\")\n",
    "        print(\"      ‚Ä¢ Avoid compiling extremely simple, non-bottleneck functions.\")\n",
    "        print(\"      ‚Ä¢ Profile to identify true bottlenecks before applying torch.compile broadly.\")\n",
    "    elif compiled_avg_simple < baseline_avg_simple:\n",
    "        speedup_simple = baseline_avg_simple / compiled_avg_simple\n",
    "        print(f\"   ‚úÖ Performance improved or similar: {speedup_simple:.2f}x speedup for simple op.\")\n",
    "    else:\n",
    "        print(f\"   ‚ÑπÔ∏è  Compiled performance is similar to baseline for this simple op.\")\n",
    "\n",
    "\n",
    "# Run debugging demonstration\n",
    "demonstrate_common_issues()\n",
    "\n",
    "print(f\"\\nüéì Debugging Best Practices Summary:\")\n",
    "print(f\"   ‚úÖ Be mindful of Python control flow on tensor data; use `torch.where` or refactor for traceability.\")\n",
    "print(f\"   ‚úÖ Use `dynamic=True` or input specialization when dealing with variable input shapes if recompilation is an issue.\")  \n",
    "print(f\"   ‚úÖ Profile! Not all operations benefit from compilation; overhead can exceed gains for trivial ops.\")\n",
    "print(f\"   ‚úÖ Utilize environment variables like `TORCH_LOGS` for detailed insights during debugging.\")\n",
    "print(f\"   ‚úÖ Start with simple, isolated examples when debugging, then gradually add complexity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee33eea2",
   "metadata": {},
   "source": [
    "## Debugging Common Compilation Issues\n",
    "\n",
    "Even with PyTorch's sophisticated compilation system, issues can arise. Let's explore common problems and their solutions.\n",
    "\n",
    "### üêõ Common Issues and Solutions\n",
    "\n",
    "#### 1. **Compilation Failures**\n",
    "```python\n",
    "# Common error: Dynamic shapes\n",
    "RuntimeError: Cannot compile with dynamic shapes\n",
    "\n",
    "# Solution: Use torch.compile with dynamic=True or fix shapes\n",
    "compiled_fn = torch.compile(fn, dynamic=True)\n",
    "```\n",
    "\n",
    "#### 2. **Performance Regressions**\n",
    "```python\n",
    "# Issue: Compiled version slower than baseline\n",
    "# Causes: Small models, wrong compilation mode, graph breaks\n",
    "\n",
    "# Solutions:\n",
    "# 1. Try different modes\n",
    "compiled_fn = torch.compile(fn, mode=\"reduce-overhead\")  # vs \"default\"\n",
    "\n",
    "# 2. Check for graph breaks\n",
    "with torch._dynamo.optimize(\"inductor\"):\n",
    "    result = fn(input)  # Will show graph break warnings\n",
    "```\n",
    "\n",
    "#### 3. **Memory Issues**\n",
    "```python\n",
    "# Issue: Out of memory during compilation\n",
    "# Solution: Reduce compilation scope or use checkpointing\n",
    "@torch.compile(mode=\"reduce-overhead\")\n",
    "def smaller_function(x):\n",
    "    # Break large functions into smaller ones\n",
    "    return partial_computation(x)\n",
    "```\n",
    "\n",
    "#### 4. **Unsupported Operations**\n",
    "```python\n",
    "# Issue: Some operations don't support compilation\n",
    "# Solution: Selective compilation or fallbacks\n",
    "\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.compiled_part = torch.compile(self.core_computation)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Compiled part\n",
    "        x = self.compiled_part(x)\n",
    "        \n",
    "        # Unsupported operations run normally\n",
    "        x = unsupported_operation(x)\n",
    "        \n",
    "        return x\n",
    "```\n",
    "\n",
    "### üîß Debugging Toolkit\n",
    "\n",
    "1. **Environment Variables**: Use detailed logging\n",
    "2. **Graph Breaks**: Monitor for optimization barriers\n",
    "3. **Profiling**: Use torch.profiler for detailed analysis\n",
    "4. **Selective Compilation**: Isolate problematic areas\n",
    "\n",
    "## üéØ Recommended Jupyter Debugging Methodology\n",
    "\n",
    "Based on our comprehensive analysis, here's the **optimal debugging workflow** for PyTorch `torch.compile()` in Jupyter environments:\n",
    "\n",
    "### **Primary Debugging Strategy: Two-Method Approach**\n",
    "\n",
    "#### **Method 1: Dynamo Analysis** üìä \n",
    "**Use for:** Quick issue identification and production debugging\n",
    "- ‚úÖ **Native Jupyter operation** - no external processes required\n",
    "- ‚úÖ **Structured output** - programmatic access to compilation data  \n",
    "- ‚úÖ **Fast execution** - immediate insights without compilation overhead\n",
    "- ‚úÖ **Actionable information** - directly identifies graph breaks and optimization barriers\n",
    "\n",
    "```python\n",
    "# Quick debugging workflow\n",
    "explanations = torch._dynamo.explain(your_model)(test_input)\n",
    "# Instantly see graph breaks, operation counts, and optimization potential\n",
    "```\n",
    "\n",
    "#### **Method 2: Subprocess Capture** üîç\n",
    "**Use for:** Deep learning and environment variable exploration\n",
    "- ‚úÖ **Complete visibility** - captures all PyTorch logs that Jupyter normally can't see\n",
    "- ‚úÖ **Environment variable effects** - shows the impact of TORCH_LOGS, debug settings\n",
    "- ‚úÖ **Educational value** - perfect for understanding compilation internals\n",
    "- ‚úÖ **Comprehensive output** - access to detailed compilation pipeline information\n",
    "\n",
    "```python\n",
    "# Deep investigation workflow  \n",
    "debug_success = demonstrate_jupyter_vs_terminal_logging()\n",
    "# Captures external PyTorch logs for complete compilation visibility\n",
    "```\n",
    "\n",
    "### **Why This Two-Method Approach Is Superior**\n",
    "\n",
    "**üöÄ Efficiency**: Start with Dynamo Analysis for 90% of debugging needs\n",
    "**üî¨ Depth**: Use Subprocess Capture when you need complete compilation visibility  \n",
    "**üéØ Practicality**: Both methods work reliably in Jupyter environments\n",
    "**üí° Complementary**: Quick analysis + deep investigation = complete debugging coverage\n",
    "\n",
    "### **When to Use Each Method**\n",
    "\n",
    "| Scenario | Recommended Method | Why |\n",
    "|----------|-------------------|-----|\n",
    "| **Production debugging** | Dynamo Analysis | Fast, programmatic, native Jupyter |\n",
    "| **Learning PyTorch compilation** | Subprocess Capture | Complete visibility into internal processes |\n",
    "| **Graph break troubleshooting** | Dynamo Analysis | Direct identification of breaks |\n",
    "| **Environment variable testing** | Subprocess Capture | Shows actual log output effects |\n",
    "| **Automated analysis** | Dynamo Analysis | Structured, programmable output |\n",
    "| **Understanding kernel generation** | Subprocess Capture | Reveals Triton code generation process |\n",
    "\n",
    "This methodology provides complete debugging coverage while maximizing efficiency and maintaining the interactive benefits of Jupyter development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cf8e15",
   "metadata": {},
   "source": [
    "## Summary: Advanced Debugging & Optimization Mastered\n",
    "\n",
    "Excellent work! You've now mastered advanced debugging techniques and optimization strategies for PyTorch's `torch.compile()` system. Let's recap your newly acquired expert-level skills:\n",
    "\n",
    "### **Advanced Skills Mastered**\n",
    "\n",
    "#### **üîç Expert-Level Debugging**\n",
    "- **Jupyter-Focused Debugging**: Mastered the two most effective debugging methods for Jupyter environments\n",
    "- **Subprocess Capture**: External process execution to capture PyTorch logs that Jupyter can't see\n",
    "- **Dynamo Analysis**: Programmatic analysis of compilation graphs and optimization decisions  \n",
    "- **Artifact Inspection**: Understanding and analyzing generated Triton kernels and debug files\n",
    "\n",
    "#### **‚ö° Performance Engineering**\n",
    "- **Statistical Benchmarking**: Rigorous performance measurement techniques\n",
    "- **Break-even Analysis**: Economic modeling for compilation decisions\n",
    "- **Scaling Analysis**: Understanding performance across different model sizes\n",
    "- **Mode Comparison**: Choosing optimal compilation strategies\n",
    "\n",
    "#### **üéØ Optimization Strategies**\n",
    "- **Systematic Analysis**: Framework for evaluating compilation benefits\n",
    "- **Pattern Recognition**: Identifying operations that benefit from compilation\n",
    "- **Selective Compilation**: Strategic application for maximum benefit\n",
    "- **Production Considerations**: Real-world deployment strategies\n",
    "\n",
    "### **Expert Techniques Acquired**\n",
    "\n",
    "1. **‚úÖ Jupyter-Native Debugging**: Two-method approach optimized for notebook environments\n",
    "2. **‚úÖ Kernel Exploration**: Understanding and analyzing generated Triton code\n",
    "3. **‚úÖ Performance Benchmarking**: Statistical measurement and analysis\n",
    "4. **‚úÖ Issue Resolution**: Common problems and systematic solutions\n",
    "\n",
    "### **Preferred Jupyter Debugging Workflow**\n",
    "\n",
    "**Primary Methods for Jupyter Development:**\n",
    "\n",
    "1. **üîç Subprocess Capture**: Capture PyTorch logs by running compilation externally\n",
    "   - ‚úÖ Shows all PyTorch debug output that Jupyter normally can't display\n",
    "   - ‚úÖ Perfect for understanding environment variable effects\n",
    "   - ‚úÖ Ideal for learning and detailed investigation\n",
    "\n",
    "2. **üìä Dynamo Analysis**: Use `torch._dynamo.explain()` for programmatic insights\n",
    "   - ‚úÖ Always works natively in Jupyter\n",
    "   - ‚úÖ Structured, actionable data about graph breaks and optimization\n",
    "   - ‚úÖ Fast execution without external processes\n",
    "   - ‚úÖ Perfect for automated analysis and production debugging\n",
    "\n",
    "### **What You Can Now Do**\n",
    "\n",
    "- **Debug Complex Compilation Issues**: Two-method systematic approach to troubleshooting\n",
    "- **Analyze Generated Kernels**: Understanding optimization patterns in Triton code\n",
    "- **Measure Performance Scientifically**: Statistical rigor in benchmarking\n",
    "- **Make Informed Decisions**: Data-driven compilation strategies\n",
    "\n",
    "### **What's Next in Part 3?**\n",
    "\n",
    "Now that you're an expert in debugging and optimization, Part 3 will cover:\n",
    "\n",
    "#### **Part 3: Production Deployment & Best Practices** *(Final Part)*\n",
    "- **Enterprise Deployment Patterns**: Production-ready strategies\n",
    "- **Advanced Troubleshooting**: Expert problem-solving techniques  \n",
    "- **Performance Monitoring**: Real-time optimization tracking\n",
    "- **Best Practices**: Professional recommendations and patterns\n",
    "\n",
    "### üí° **Apply Your Advanced Skills**\n",
    "\n",
    "**Expert Challenge**: Take a complex PyTorch model from your work and apply the full debugging and optimization pipeline you've learned. Use the two-method debugging approach and benchmarking framework to make data-driven decisions about compilation strategy!\n",
    "\n",
    "---\n",
    "\n",
    "## üîó **Continue to Final Part**\n",
    "\n",
    "Ready for production deployment? Continue with **Part 3: Production Deployment & Best Practices** where we'll cover:\n",
    "\n",
    "- Enterprise-grade deployment strategies\n",
    "- Advanced troubleshooting techniques\n",
    "- Production monitoring and alerting\n",
    "- Expert best practices and patterns\n",
    "\n",
    "**You're now a torch.compile() optimization expert! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-qat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
