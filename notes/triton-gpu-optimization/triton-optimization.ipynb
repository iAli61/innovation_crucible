{
 "cells": [
  {
   "cell_type": "raw",
   "id": "8113b231",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Triton GPU Optimization and Kernel Analysis\"\n",
    "format: \n",
    "  html:\n",
    "    code-fold: true\n",
    "jupyter: python3\n",
    "page-layout: full\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad703673",
   "metadata": {},
   "source": [
    "# Optimizing PyTorch Kernels with Triton\n",
    "\n",
    "This notebook explores how PyTorch leverages Triton to generate optimized kernels for various operations, enhancing performance on CUDA-enabled GPUs. We will delve into the specifics of how PyTorch compiles and executes custom operations, using LayerNorm + GELU as our primary example. \n",
    "\n",
    "The process will involve:\n",
    "1. Understanding the baseline PyTorch implementation.\n",
    "2. Examining the Triton-generated code.\n",
    "3. Building a pipeline to benchmark different implementations.\n",
    "4. Exploring techniques for further kernel tuning.\n",
    "\n",
    "Finally, we will apply this pipeline and knowledge to optimize other common patterns like:\n",
    "- Softmax + Dropout\n",
    "- RMSNorm\n",
    "- Sigmoid * x (Swish/SiLU)\n",
    "\n",
    "## Kernel Organization Structure\n",
    "\n",
    "Generated kernels will be saved in an organized manner:\n",
    "```\n",
    "./triton_kernels/\n",
    "â”œâ”€â”€ experiment_1/          # Organized by experiment name\n",
    "â”‚   â”œâ”€â”€ kernel_001.py     # Generated kernels\n",
    "â”‚   â””â”€â”€ kernel_002.py\n",
    "â”‚   â””â”€â”€ other artifacts from pytorch compile\n",
    "â”‚   â””â”€â”€ ...\n",
    "â”œâ”€â”€ experiment_2/\n",
    "â”‚   â””â”€â”€ kernel_003.py\n",
    "â”‚   â””â”€â”€ other artifacts from pytorch compile\n",
    "â”‚   â””â”€â”€ ...\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "- PyTorch with CUDA support\n",
    "- Triton GPU compiler\n",
    "- NVIDIA GPU with CUDA capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e470fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Triton Kernel Optimization Setup\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import gc\n",
    "import shutil\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "class ExperimentManager:\n",
    "    \"\"\"Manages experiment directories and kernel organization for advanced optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir=\"./triton_kernels\"):\n",
    "        self.base_dir = Path(base_dir).resolve()\n",
    "        self.base_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.current_experiment = None\n",
    "        self.experiment_counter = 1\n",
    "        \n",
    "    def create_experiment(self, name: str = None) -> Path:\n",
    "        \"\"\"Create a new experiment directory for kernel analysis\"\"\"\n",
    "        if name is None:\n",
    "            name = f\"experiment_{self.experiment_counter}\"\n",
    "            self.experiment_counter += 1\n",
    "        \n",
    "        experiment_path = self.base_dir / name\n",
    "        experiment_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.current_experiment = experiment_path\n",
    "        \n",
    "        metadata = {\n",
    "            \"experiment_name\": name,\n",
    "            \"created_at\": datetime.datetime.now().isoformat(),\n",
    "            \"description\": \"\",\n",
    "            \"kernels\": []\n",
    "        }\n",
    "        \n",
    "        with open(experiment_path / \"metadata.json\", \"w\") as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"ğŸ“ Created experiment: {experiment_path}\")\n",
    "        return experiment_path\n",
    "    \n",
    "    def set_experiment_cache(self, experiment_path: Path):\n",
    "        \"\"\"Set Triton cache to point to experiment directory\"\"\"\n",
    "        os.environ[\"TRITON_CACHE_DIR\"] = str(experiment_path)\n",
    "        print(f\"ğŸ”§ Set cache directory: {experiment_path}\")\n",
    "    \n",
    "    def save_kernel_metadata(self, kernel_info: dict):\n",
    "        \"\"\"Save metadata about generated kernels\"\"\"\n",
    "        if self.current_experiment is None:\n",
    "            return\n",
    "        \n",
    "        metadata_file = self.current_experiment / \"metadata.json\"\n",
    "        if metadata_file.exists():\n",
    "            with open(metadata_file, \"r\") as f:\n",
    "                metadata = json.load(f)\n",
    "        else:\n",
    "            metadata = {\"kernels\": []}\n",
    "        \n",
    "        metadata[\"kernels\"].append(kernel_info)\n",
    "        \n",
    "        with open(metadata_file, \"w\") as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "\n",
    "# Simplified setup for advanced optimization experiments\n",
    "def setup_advanced_optimization():\n",
    "    \"\"\"Configure environment for advanced kernel optimization experiments\"\"\"\n",
    "    \n",
    "    # Enable output code logging for kernel analysis\n",
    "    os.environ[\"TORCH_LOGS\"] = \"output_code\"\n",
    "    os.environ[\"TRITON_PRINT_AUTOTUNING\"] = \"1\"\n",
    "    \n",
    "    print(\"ğŸš€ Advanced Triton Kernel Optimization Environment\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Detect device\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        print(f\"âœ… CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   Compute capability: {torch.cuda.get_device_capability(0)}\")\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        print(\"âš ï¸  Using CPU (CUDA not available)\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "def clear_compilation_cache():\n",
    "    \"\"\"Clear PyTorch compilation cache for fresh experiments\"\"\"\n",
    "    torch._dynamo.reset()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def find_triton_kernels(search_dirs=None):\n",
    "    \"\"\"Search for generated Triton kernel files\"\"\"\n",
    "    \n",
    "    if search_dirs is None:\n",
    "        search_dirs = [\n",
    "            \"./triton_kernels\",\n",
    "            f\"/tmp/torchinductor_{os.getenv('USER', 'user')}/\",\n",
    "            \"/tmp/torchinductor_alibina/\",\n",
    "            \"/tmp/triton/\"\n",
    "        ]\n",
    "    \n",
    "    kernel_files = []\n",
    "    \n",
    "    for cache_dir in search_dirs:\n",
    "        cache_path = Path(cache_dir)\n",
    "        if cache_path.exists():\n",
    "            for file_path in cache_path.rglob(\"*.py\"):\n",
    "                try:\n",
    "                    content = file_path.read_text()\n",
    "                    triton_patterns = [\n",
    "                        '@triton.jit', 'triton_per_fused', 'triton_poi_fused',\n",
    "                        'import triton', 'tl.load', 'tl.store'\n",
    "                    ]\n",
    "                    \n",
    "                    if any(pattern in content for pattern in triton_patterns):\n",
    "                        kernel_files.append((str(file_path), content))\n",
    "                except Exception:\n",
    "                    continue\n",
    "    \n",
    "    return kernel_files\n",
    "\n",
    "# Initialize environment for advanced optimization\n",
    "experiment_manager = ExperimentManager(\"./triton_kernels\")\n",
    "device = setup_advanced_optimization()\n",
    "clear_compilation_cache()\n",
    "\n",
    "print(f\"\\nâœ… Ready for advanced kernel optimization experiments!\")\n",
    "print(f\"ğŸ“‚ Kernels will be organized in: {experiment_manager.base_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626e5a2e",
   "metadata": {},
   "source": [
    "## Advanced Kernel Fusion Experiments\n",
    "\n",
    "This section focuses on advanced kernel optimization patterns using PyTorch + Triton. Each experiment demonstrates different fusion strategies and their performance benefits.\n",
    "\n",
    "### ğŸ¯ Experiment Overview\n",
    "\n",
    "| Experiment | Pattern | Focus | Learning Objective |\n",
    "|------------|---------|-------|-------------------|\n",
    "| **Experiment 1** | LayerNorm + GELU | Sequential fusion | Fundamental fusion concepts |\n",
    "| **Experiment 2** | Softmax + Dropout | Reduction + element-wise | Attention mechanism optimization |\n",
    "| **Experiment 3** | RMSNorm | Modern normalization | Alternative approaches |\n",
    "| **Experiment 4** | SiLU/Swish variants | Implementation comparison | Built-in vs custom optimization |\n",
    "\n",
    "### ğŸ§® Mathematical Background\n",
    "\n",
    "**Layer Normalization:**\n",
    "```\n",
    "LayerNorm(x) = Î³ * (x - Î¼) / Ïƒ + Î²\n",
    "where Î¼ = mean(x), Ïƒ = std(x)\n",
    "```\n",
    "\n",
    "**GELU Activation:**\n",
    "```\n",
    "GELU(x) = x * Î¦(x) = x * 0.5 * (1 + erf(x/âˆš2))\n",
    "â‰ˆ x * 0.5 * (1 + tanh(âˆš(2/Ï€) * (x + 0.044715 * xÂ³)))\n",
    "```\n",
    "\n",
    "### ğŸ”„ Fusion Strategy\n",
    "\n",
    "**Without Fusion (separate kernels):**\n",
    "1. Load input â†’ Compute LayerNorm â†’ Store intermediate\n",
    "2. Load intermediate â†’ Compute GELU â†’ Store final\n",
    "\n",
    "**With Fusion (combined kernel):**\n",
    "1. Load input â†’ Compute LayerNorm + GELU â†’ Store final\n",
    "\n",
    "This eliminates intermediate memory allocation, providing significant speedup on memory-bound operations.\n",
    "\n",
    "### ğŸ“ Test Configuration\n",
    "\n",
    "- **Batch Size**: 32 (typical training batch)\n",
    "- **Sequence Length**: 512 (BERT-base length)  \n",
    "- **Hidden Dimension**: 768 (BERT-base width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f7194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: LayerNorm + GELU Fusion\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class LayerNormGELU(nn.Module):\n",
    "    \"\"\"LayerNorm followed by GELU - a prime candidate for kernel fusion\"\"\"\n",
    "    \n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape, eps=eps)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Step 1: Apply layer normalization (creates intermediate tensor)\n",
    "        normalized = self.layer_norm(x)\n",
    "        # Step 2: Apply GELU activation (creates another intermediate)\n",
    "        output = F.gelu(normalized)\n",
    "        return output\n",
    "\n",
    "def create_test_tensors(batch_size=32, seq_len=512, hidden_dim=768):\n",
    "    \"\"\"Create transformer-typical test tensors\"\"\"\n",
    "    return torch.randn(batch_size, seq_len, hidden_dim, device=device, dtype=torch.float32)\n",
    "\n",
    "# Test the baseline implementation\n",
    "print(\"=== Experiment 1: LayerNorm + GELU Fusion ===\")\n",
    "\n",
    "test_input = create_test_tensors()\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Memory usage: {test_input.element_size() * test_input.numel() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Initialize model\n",
    "model = LayerNormGELU(test_input.shape[-1]).to(device)\n",
    "\n",
    "# Test baseline\n",
    "with torch.no_grad():\n",
    "    baseline_output = model(test_input)\n",
    "    print(f\"Output shape: {baseline_output.shape}\")\n",
    "    print(f\"Output range: [{baseline_output.min():.4f}, {baseline_output.max():.4f}]\")\n",
    "\n",
    "print(\"âœ… Baseline implementation ready for optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfb7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ Compiled Version with Kernel Capture\n",
    "# \n",
    "# This section demonstrates how PyTorch's torch.compile() works with Triton\n",
    "# to automatically optimize our LayerNorm + GELU pattern through kernel fusion.\n",
    "\n",
    "# Advanced Kernel Compilation and Analysis\n",
    "\n",
    "def capture_kernels_for_experiment(model_fn, input_tensor, experiment_name):\n",
    "    \"\"\"\n",
    "    Capture and organize generated Triton kernels for advanced analysis\n",
    "    \n",
    "    This function demonstrates how to:\n",
    "    1. Organize kernel generation experiments\n",
    "    2. Trigger optimal kernel compilation\n",
    "    3. Capture generated artifacts for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create dedicated experiment directory\n",
    "    exp_path = experiment_manager.create_experiment(experiment_name)\n",
    "    experiment_manager.set_experiment_cache(exp_path)\n",
    "    \n",
    "    # Clear compilation cache for fresh kernel generation\n",
    "    clear_compilation_cache()\n",
    "    \n",
    "    # Compile with maximum optimization\n",
    "    print(f\"\\nğŸ”§ Compiling {experiment_name} with max-autotune...\")\n",
    "    compiled_model = torch.compile(model_fn, mode=\"max-autotune\")\n",
    "    \n",
    "    # Trigger kernel generation\n",
    "    print(\"ğŸ”¥ Generating optimized kernels...\")\n",
    "    with torch.no_grad():\n",
    "        _ = compiled_model(input_tensor)  # First call triggers compilation\n",
    "        _ = compiled_model(input_tensor)  # Second call ensures completion\n",
    "    \n",
    "    # Find and catalog generated kernels\n",
    "    kernel_files = find_triton_kernels([str(exp_path)])\n",
    "    \n",
    "    if kernel_files:\n",
    "        print(f\"âœ… Found {len(kernel_files)} kernel files\")\n",
    "        for i, (file_path, content) in enumerate(kernel_files):\n",
    "            print(f\"   {i+1}. {Path(file_path).name}\")\n",
    "            \n",
    "            kernel_info = {\n",
    "                \"kernel_id\": f\"kernel_{i+1:03d}\",\n",
    "                \"file_path\": file_path,\n",
    "                \"size_bytes\": len(content),\n",
    "                \"created_at\": datetime.datetime.now().isoformat(),\n",
    "                \"operations_detected\": analyze_kernel_operations(content)\n",
    "            }\n",
    "            experiment_manager.save_kernel_metadata(kernel_info)\n",
    "    else:\n",
    "        print(\"âš ï¸  No Triton kernels found - checking system cache...\")\n",
    "        system_kernel_files = find_triton_kernels()\n",
    "        if system_kernel_files:\n",
    "            print(f\"ğŸ“¦ Found {len(system_kernel_files)} kernels in system cache\")\n",
    "    \n",
    "    return compiled_model, exp_path\n",
    "\n",
    "def analyze_kernel_operations(content):\n",
    "    \"\"\"Analyze kernel content to identify fused operations\"\"\"\n",
    "    operations = []\n",
    "    content_lower = content.lower()\n",
    "    \n",
    "    if 'layer_norm' in content_lower or 'norm' in content_lower:\n",
    "        operations.append(\"layer_norm\")\n",
    "    if 'gelu' in content_lower:\n",
    "        operations.append(\"gelu\") \n",
    "    if 'softmax' in content_lower:\n",
    "        operations.append(\"softmax\")\n",
    "    if 'dropout' in content_lower:\n",
    "        operations.append(\"dropout\")\n",
    "    if 'sigmoid' in content_lower:\n",
    "        operations.append(\"sigmoid\")\n",
    "    \n",
    "    return operations\n",
    "\n",
    "def find_triton_kernels(search_dirs=None):\n",
    "    \"\"\"\n",
    "    Enhanced kernel finding with better pattern matching\n",
    "    \n",
    "    Searches for Triton kernel files (.py) that contain Triton-specific code patterns.\n",
    "    This helps us identify which files are actually generated kernels vs other Python files.\n",
    "    \"\"\"\n",
    "    \n",
    "    if search_dirs is None:\n",
    "        # Default system cache directories where PyTorch/Triton store generated kernels\n",
    "        search_dirs = [\n",
    "            f\"/tmp/torchinductor_{os.getenv('USER', 'user')}/\",\n",
    "            \"/tmp/torchinductor_alibina/\", \n",
    "            \"/tmp/triton/\",\n",
    "            str(Path.home() / \".triton\" / \"cache\")\n",
    "        ]\n",
    "    \n",
    "    kernel_files = []\n",
    "    \n",
    "    for cache_dir in search_dirs:\n",
    "        cache_path = Path(cache_dir)\n",
    "        if cache_path.exists():\n",
    "            # Search for Python files recursively\n",
    "            for file_path in cache_path.rglob(\"*.py\"):\n",
    "                try:\n",
    "                    content = file_path.read_text()\n",
    "                    # Look for Triton-specific patterns to identify kernel files\n",
    "                    triton_patterns = [\n",
    "                        '@triton.jit',           # Triton JIT decorator\n",
    "                        'triton_per_fused',      # Fused reduction kernels\n",
    "                        'triton_poi_fused',      # Fused pointwise kernels\n",
    "                        'import triton',         # Triton imports\n",
    "                        'tl.load',              # Triton load operations\n",
    "                        'tl.store'              # Triton store operations\n",
    "                    ]\n",
    "                    \n",
    "                    if any(pattern in content for pattern in triton_patterns):\n",
    "                        kernel_files.append((str(file_path), content))\n",
    "                except Exception:\n",
    "                    # Skip files that can't be read\n",
    "                    continue\n",
    "    \n",
    "    return kernel_files\n",
    "\n",
    "# ğŸ§ª Execute Experiment 1: LayerNorm + GELU Fusion\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ§ª EXPERIMENT 1: LayerNorm + GELU Fusion\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“– Learning Objectives:\")\n",
    "print(\"   â€¢ Observe automatic kernel fusion in action\")\n",
    "print(\"   â€¢ Compare fused vs unfused performance\")\n",
    "print(\"   â€¢ Analyze generated Triton kernel code\")\n",
    "print(\"   â€¢ Understand compilation overhead vs runtime benefits\")\n",
    "\n",
    "compiled_model, exp1_path = capture_kernels_for_experiment(\n",
    "    model, test_input, \"layernorm_gelu_fusion\"\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š Experiment results saved to: {exp1_path}\")\n",
    "print(f\"ğŸ” You can examine the generated kernel files in this directory!\")\n",
    "\n",
    "# ğŸ§ª Verify correctness: compiled output should match baseline\n",
    "print(f\"\\nğŸ”¬ Correctness Verification:\")\n",
    "with torch.no_grad():\n",
    "    compiled_output = compiled_model(test_input)\n",
    "    \n",
    "    # Check if outputs are numerically equivalent\n",
    "    if torch.allclose(baseline_output, compiled_output, rtol=1e-5, atol=1e-6):\n",
    "        print(\"âœ… Compiled model output matches baseline perfectly\")\n",
    "        print(f\"   ğŸ“Š Max absolute difference: {(baseline_output - compiled_output).abs().max():.2e}\")\n",
    "    else:\n",
    "        print(\"âŒ Output mismatch detected!\")\n",
    "        print(f\"   ğŸ“Š Max absolute difference: {(baseline_output - compiled_output).abs().max():.2e}\")\n",
    "        print(\"   ğŸ’¡ Small differences are normal due to different computation orders\")\n",
    "\n",
    "print(f\"\\nğŸ“ Key Learning: The compiled model produces identical results\")\n",
    "print(f\"   but will be significantly faster on subsequent runs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6408803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Comprehensive Benchmarking Pipeline\n",
    "#\n",
    "# This section implements a rigorous benchmarking methodology to measure\n",
    "# the true performance impact of kernel fusion and compilation optimizations.\n",
    "\n",
    "import statistics\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    \"\"\"\n",
    "    Container for benchmark results with comprehensive metrics\n",
    "    \n",
    "    This class stores all the important metrics we need to evaluate\n",
    "    kernel performance comprehensively.\n",
    "    \"\"\"\n",
    "    name: str\n",
    "    mean_time: float      # Average execution time\n",
    "    std_time: float       # Standard deviation (shows consistency)\n",
    "    min_time: float       # Best-case performance\n",
    "    max_time: float       # Worst-case performance\n",
    "    throughput: float     # Elements processed per second\n",
    "    speedup: float = 1.0  # Speedup relative to baseline\n",
    "\n",
    "class PerformanceBenchmarker:\n",
    "    \"\"\"\n",
    "    ğŸ¯ Professional-grade benchmarking for GPU kernel performance\n",
    "    \n",
    "    Key principles implemented:\n",
    "    1. Proper warmup to avoid compilation overhead in measurements\n",
    "    2. GPU synchronization to get accurate timings\n",
    "    3. Multiple runs for statistical significance\n",
    "    4. Comprehensive metrics including throughput and speedup\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, warmup_runs=5, benchmark_runs=20):\n",
    "        \"\"\"\n",
    "        Initialize benchmarker with scientific rigor\n",
    "        \n",
    "        Args:\n",
    "            warmup_runs: Number of runs to \"warm up\" before measuring\n",
    "                        (eliminates compilation overhead and cache misses)\n",
    "            benchmark_runs: Number of timed runs for statistical analysis\n",
    "        \"\"\"\n",
    "        self.warmup_runs = warmup_runs\n",
    "        self.benchmark_runs = benchmark_runs\n",
    "        self.baseline_time = None\n",
    "        \n",
    "    def benchmark_function(self, func, input_tensor, name: str) -> BenchmarkResult:\n",
    "        \"\"\"\n",
    "        Benchmark a function with scientific rigor\n",
    "        \n",
    "        This method implements GPU benchmarking best practices:\n",
    "        1. Warmup phase to eliminate one-time costs\n",
    "        2. Proper CUDA synchronization for accurate timing\n",
    "        3. Statistical analysis across multiple runs\n",
    "        4. Comprehensive metrics calculation\n",
    "        \n",
    "        Args:\n",
    "            func: Function to benchmark\n",
    "            input_tensor: Input data for the function\n",
    "            name: Human-readable name for this benchmark\n",
    "            \n",
    "        Returns:\n",
    "            BenchmarkResult with comprehensive performance metrics\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"    ğŸ”¥ Benchmarking: {name}\")\n",
    "        \n",
    "        # ğŸ”¥ Phase 1: Warmup runs\n",
    "        # These runs eliminate compilation overhead and prepare GPU caches\n",
    "        print(f\"       Warmup: {self.warmup_runs} runs...\")\n",
    "        for i in range(self.warmup_runs):\n",
    "            with torch.no_grad():\n",
    "                _ = func(input_tensor)\n",
    "        \n",
    "        # Ensure all warmup operations complete before timing\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        # â±ï¸ Phase 2: Timed benchmark runs\n",
    "        print(f\"       Timing: {self.benchmark_runs} runs...\")\n",
    "        times = []\n",
    "        \n",
    "        for i in range(self.benchmark_runs):\n",
    "            # Synchronize before starting timer\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            # Start timing\n",
    "            start = time.perf_counter()\n",
    "            \n",
    "            # Execute function\n",
    "            with torch.no_grad():\n",
    "                output = func(input_tensor)\n",
    "            \n",
    "            # Synchronize before stopping timer (crucial for GPU timing!)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            # Stop timing\n",
    "            end = time.perf_counter()\n",
    "            times.append(end - start)\n",
    "        \n",
    "        # ğŸ“Š Phase 3: Statistical analysis\n",
    "        mean_time = statistics.mean(times)\n",
    "        std_time = statistics.stdev(times) if len(times) > 1 else 0.0\n",
    "        min_time = min(times)\n",
    "        max_time = max(times)\n",
    "        \n",
    "        # Calculate throughput: how many elements processed per second\n",
    "        num_elements = input_tensor.numel()\n",
    "        throughput = num_elements / mean_time\n",
    "        \n",
    "        # Calculate speedup relative to baseline\n",
    "        speedup = 1.0\n",
    "        if self.baseline_time is not None:\n",
    "            speedup = self.baseline_time / mean_time\n",
    "        elif \"baseline\" in name.lower():\n",
    "            self.baseline_time = mean_time\n",
    "        \n",
    "        print(f\"       âœ… Results: {mean_time*1000:.3f}ms Â± {std_time*1000:.3f}ms\")\n",
    "        \n",
    "        return BenchmarkResult(\n",
    "            name=name,\n",
    "            mean_time=mean_time,\n",
    "            std_time=std_time,\n",
    "            min_time=min_time,\n",
    "            max_time=max_time,\n",
    "            throughput=throughput,\n",
    "            speedup=speedup\n",
    "        )\n",
    "    \n",
    "    def print_results(self, results: List[BenchmarkResult]):\n",
    "        \"\"\"\n",
    "        Print formatted benchmark results in a professional table\n",
    "        \n",
    "        This creates an easy-to-read summary table showing:\n",
    "        - Execution times with standard deviation\n",
    "        - Speedup factors relative to baseline\n",
    "        - Throughput in millions of elements per second\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ğŸƒâ€â™‚ï¸ PERFORMANCE BENCHMARK RESULTS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Table header\n",
    "        print(f\"{'Implementation':<20} {'Time (ms)':<12} {'Â±Std (ms)':<10} {'Speedup':<8} {'Throughput':<15}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Results rows\n",
    "        for result in results:\n",
    "            print(f\"{result.name:<20} \"\n",
    "                  f\"{result.mean_time*1000:<12.3f} \"\n",
    "                  f\"Â±{result.std_time*1000:<9.3f} \"\n",
    "                  f\"{result.speedup:<8.2f}x \"\n",
    "                  f\"{result.throughput/1e6:<15.1f}M elem/s\")\n",
    "        \n",
    "        # Highlight best performer\n",
    "        if len(results) > 1:\n",
    "            best = max(results, key=lambda r: r.speedup)\n",
    "            print(f\"\\nğŸ† Best performer: {best.name} ({best.speedup:.2f}x speedup)\")\n",
    "            \n",
    "            # Calculate performance improvement\n",
    "            if best.speedup > 1.1:\n",
    "                improvement = (best.speedup - 1) * 100\n",
    "                print(f\"ğŸš€ Performance improvement: {improvement:.1f}% faster than baseline\")\n",
    "\n",
    "# ğŸƒâ€â™‚ï¸ Execute Comprehensive Benchmarks\n",
    "print(\"\\nğŸ“Š COMPREHENSIVE PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ¯ Testing multiple tensor sizes to understand scaling behavior\")\n",
    "print(\"ğŸ“– Learning Objectives:\")\n",
    "print(\"   â€¢ Measure fusion benefits across different scales\")\n",
    "print(\"   â€¢ Understand how performance scales with tensor size\")\n",
    "print(\"   â€¢ Observe consistency of performance improvements\")\n",
    "print(\"   â€¢ Analyze throughput characteristics\")\n",
    "\n",
    "benchmarker = PerformanceBenchmarker(warmup_runs=10, benchmark_runs=50)\n",
    "all_results = []\n",
    "\n",
    "# Test configurations: small to large to observe scaling\n",
    "test_configs = [\n",
    "    (16, 128, 768),   # Small: Typical inference batch\n",
    "    (32, 512, 768),   # Medium: Training batch  \n",
    "    (64, 1024, 768),  # Large: Large batch training\n",
    "]\n",
    "\n",
    "for i, (batch_size, seq_len, hidden_dim) in enumerate(test_configs, 1):\n",
    "    print(f\"\\nğŸ“Š Configuration {i}/3: Batch={batch_size}, Seq={seq_len}, Hidden={hidden_dim}\")\n",
    "    \n",
    "    # Calculate total elements and memory usage\n",
    "    test_input = create_test_tensors(batch_size, seq_len, hidden_dim)\n",
    "    total_elements = test_input.numel()\n",
    "    memory_mb = test_input.element_size() * total_elements / (1024**2)\n",
    "    \n",
    "    print(f\"    ğŸ“ Tensor shape: {test_input.shape}\")\n",
    "    print(f\"    ğŸ”¢ Total elements: {total_elements:,}\")\n",
    "    print(f\"    ğŸ’¾ Memory usage: {memory_mb:.1f} MB\")\n",
    "    \n",
    "    # Create fresh model instances to avoid compilation caching between sizes\n",
    "    baseline_model = LayerNormGELU(test_input.shape[-1]).to(device)\n",
    "    compiled_model_fresh = torch.compile(baseline_model, mode=\"max-autotune\")\n",
    "    \n",
    "    # Benchmark baseline implementation\n",
    "    baseline_result = benchmarker.benchmark_function(\n",
    "        baseline_model, test_input, f\"Baseline-{batch_size}x{seq_len}\"\n",
    "    )\n",
    "    all_results.append(baseline_result)\n",
    "    \n",
    "    # Benchmark compiled version\n",
    "    compiled_result = benchmarker.benchmark_function(\n",
    "        compiled_model_fresh, test_input, f\"Compiled-{batch_size}x{seq_len}\"\n",
    "    )\n",
    "    all_results.append(compiled_result)\n",
    "    \n",
    "    # Print results for this configuration\n",
    "    benchmarker.print_results([baseline_result, compiled_result])\n",
    "    \n",
    "    # Reset baseline for next configuration\n",
    "    benchmarker.baseline_time = None\n",
    "\n",
    "print(f\"\\nğŸ“ Key Insights from Comprehensive Benchmarking:\")\n",
    "print(f\"   â€¢ Kernel fusion provides consistent speedups\")\n",
    "print(f\"   â€¢ Performance benefits scale with tensor size\")\n",
    "print(f\"   â€¢ Compilation overhead is one-time cost\")\n",
    "print(f\"   â€¢ Larger tensors show more dramatic improvements\")\n",
    "print(f\"\\nâœ… Benchmarking complete! Results saved in experiment directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fbd37a",
   "metadata": {},
   "source": [
    "## Exploring More Fusion Patterns\n",
    "\n",
    "### ğŸ¯ Why Study Multiple Patterns?\n",
    "\n",
    "Now that we understand the fundamentals with LayerNorm + GELU, let's explore other common patterns. Each pattern teaches us different aspects of GPU optimization:\n",
    "\n",
    "| Pattern | Primary Learning | Common Use Case |\n",
    "|---------|------------------|-----------------|\n",
    "| **Softmax + Dropout** | Attention mechanism optimization | Transformer attention layers |\n",
    "| **RMSNorm** | Alternative normalization schemes | Modern LLMs (LLaMA, PaLM) |\n",
    "| **SiLU/Swish** | Activation function variants | MLP layers, ConvNeXt |\n",
    "\n",
    "### ğŸ§  Fusion Strategy Patterns\n",
    "\n",
    "Different operations benefit from fusion in different ways:\n",
    "\n",
    "1. **Sequential Fusion**: Operations applied one after another (LayerNorm â†’ GELU)\n",
    "2. **Parallel Fusion**: Multiple operations on same data (computing mean + variance)\n",
    "3. **Reduction Fusion**: Operations that reduce dimensionality (Softmax across sequence)\n",
    "\n",
    "### ğŸ“š Learning Progression\n",
    "\n",
    "We'll progress through increasingly complex patterns:\n",
    "- **Experiment 2**: Softmax + Dropout (attention patterns)\n",
    "- **Experiment 3**: RMSNorm (modern normalization)\n",
    "- **Experiment 4**: SiLU variants (activation comparison)\n",
    "\n",
    "Each experiment will be organized in its own directory with:\n",
    "- Generated Triton kernels\n",
    "- Performance benchmarks\n",
    "- Metadata and analysis\n",
    "- Compilation artifacts\n",
    "\n",
    "Let's dive into each pattern and see how PyTorch + Triton optimizes them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4986896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Softmax + Dropout Fusion\n",
    "#\n",
    "# ğŸ“– Educational Focus: Attention Mechanism Optimization\n",
    "#\n",
    "# This pattern is found in every transformer attention layer:\n",
    "# 1. Compute attention scores (Q @ K^T / âˆšd)\n",
    "# 2. Apply softmax to get attention weights  \n",
    "# 3. Apply dropout for regularization\n",
    "# 4. Use weights to attend to values (Attention @ V)\n",
    "#\n",
    "# Mathematical Background:\n",
    "# Softmax(x_i) = exp(x_i) / Î£(exp(x_j))\n",
    "# Dropout(x_i) = x_i / p with probability p, else 0\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ§ª EXPERIMENT 2: Softmax + Dropout Fusion\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“– Focus: Optimizing Transformer Attention Mechanisms\")\n",
    "print(\"ğŸ¯ Key Learning: How reduction operations (softmax) fuse with element-wise ops\")\n",
    "\n",
    "class SoftmaxDropout(nn.Module):\n",
    "    \"\"\"\n",
    "    Softmax followed by Dropout - the heart of attention mechanisms\n",
    "    \n",
    "    This pattern appears in every transformer attention layer and is\n",
    "    a perfect candidate for fusion because:\n",
    "    1. Softmax is a reduction operation (needs to see all elements)\n",
    "    2. Dropout is element-wise (can be applied during softmax computation)\n",
    "    3. Both are memory-bound operations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout_prob = dropout_prob\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Step 1: Apply softmax along last dimension\n",
    "        # This computes: softmax(x_i) = exp(x_i) / sum(exp(x_j))\n",
    "        softmax_out = F.softmax(x, dim=-1)\n",
    "        \n",
    "        # Step 2: Apply dropout for regularization\n",
    "        # During training: randomly zero elements with probability dropout_prob\n",
    "        # During inference: scale by (1 - dropout_prob)\n",
    "        dropped_out = F.dropout(softmax_out, p=self.dropout_prob, training=self.training)\n",
    "        \n",
    "        return dropped_out\n",
    "\n",
    "# ğŸ¯ Create attention-like test data\n",
    "# Shape: [batch_size, num_heads, seq_len, seq_len]\n",
    "# This represents attention scores before softmax in multi-head attention\n",
    "batch_size, num_heads, seq_len = 32, 8, 512\n",
    "\n",
    "attention_input = torch.randn(batch_size, num_heads, seq_len, seq_len, device=device)\n",
    "print(f\"ğŸ“Š Attention input shape: {attention_input.shape}\")\n",
    "print(f\"ğŸ’¾ Memory usage: {attention_input.element_size() * attention_input.numel() / 1024**2:.1f} MB\")\n",
    "print(f\"ğŸ”¢ Total elements: {attention_input.numel():,}\")\n",
    "\n",
    "# Initialize model in training mode to enable dropout\n",
    "softmax_dropout_model = SoftmaxDropout(dropout_prob=0.1).to(device)\n",
    "softmax_dropout_model.train()  # Enable dropout for this experiment\n",
    "\n",
    "print(f\"\\nğŸ“ Model configuration:\")\n",
    "print(f\"   Dropout probability: {softmax_dropout_model.dropout_prob}\")\n",
    "print(f\"   Training mode: {softmax_dropout_model.training}\")\n",
    "\n",
    "# ğŸ”§ Capture kernels for this attention pattern\n",
    "print(f\"\\nğŸ”§ Compiling and capturing attention mechanism kernels...\")\n",
    "compiled_softmax_dropout, exp2_path = capture_kernels_for_experiment(\n",
    "    softmax_dropout_model, attention_input, \"softmax_dropout_fusion\"\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š Softmax + Dropout experiment saved to: {exp2_path}\")\n",
    "\n",
    "# ğŸƒâ€â™‚ï¸ Quick performance benchmark\n",
    "print(f\"\\nğŸƒâ€â™‚ï¸ Performance Analysis:\")\n",
    "print(f\"   ğŸ¯ This pattern tests reduction + element-wise fusion\")\n",
    "print(f\"   ğŸ“ˆ Expected benefit: Reduced memory bandwidth for attention\")\n",
    "\n",
    "benchmarker_exp2 = PerformanceBenchmarker(warmup_runs=5, benchmark_runs=20)\n",
    "\n",
    "baseline_result = benchmarker_exp2.benchmark_function(\n",
    "    softmax_dropout_model, attention_input, \"Softmax+Dropout Baseline\"\n",
    ")\n",
    "\n",
    "compiled_result = benchmarker_exp2.benchmark_function(\n",
    "    compiled_softmax_dropout, attention_input, \"Softmax+Dropout Compiled\"\n",
    ")\n",
    "\n",
    "benchmarker_exp2.print_results([baseline_result, compiled_result])\n",
    "\n",
    "# ğŸ”¬ Correctness verification for stochastic operations\n",
    "print(f\"\\nğŸ”¬ Correctness Note:\")\n",
    "print(f\"   âš ï¸  Dropout is stochastic - outputs won't match exactly\")\n",
    "print(f\"   âœ… We verify the statistical properties instead\")\n",
    "\n",
    "# Test in eval mode for deterministic comparison\n",
    "softmax_dropout_model.eval()\n",
    "compiled_softmax_dropout.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    baseline_eval = softmax_dropout_model(attention_input)\n",
    "    compiled_eval = compiled_softmax_dropout(attention_input)\n",
    "    \n",
    "    if torch.allclose(baseline_eval, compiled_eval, rtol=1e-5, atol=1e-6):\n",
    "        print(f\"   âœ… Eval mode outputs match perfectly\")\n",
    "    else:\n",
    "        print(f\"   ğŸ“Š Max difference: {(baseline_eval - compiled_eval).abs().max():.2e}\")\n",
    "\n",
    "print(f\"\\nğŸ“ Key Insights:\")\n",
    "print(f\"   â€¢ Softmax + Dropout fusion reduces memory traffic\")\n",
    "print(f\"   â€¢ Attention mechanisms benefit significantly from this optimization\")\n",
    "print(f\"   â€¢ Stochastic operations require careful correctness verification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f713ca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª Experiment 3: RMSNorm (Root Mean Square Normalization)\n",
    "#\n",
    "# ğŸ“– Educational Focus: Modern Normalization Schemes\n",
    "#\n",
    "# RMSNorm is used in modern large language models like:\n",
    "# - LLaMA (Meta)\n",
    "# - PaLM (Google) \n",
    "# - GPT-NeoX\n",
    "#\n",
    "# Mathematical Comparison:\n",
    "# LayerNorm: (x - Î¼) / Ïƒ * Î³ + Î²  (requires mean AND variance)\n",
    "# RMSNorm:   x / RMS(x) * Î³       (only requires RMS, simpler!)\n",
    "#\n",
    "# Where RMS(x) = âˆš(mean(xÂ²) + Îµ)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ§ª EXPERIMENT 3: RMSNorm Optimization\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“– Focus: Modern Alternative to LayerNorm\")\n",
    "print(\"ğŸ¯ Key Learning: Simpler normalization can be more efficient\")\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Root Mean Square Normalization - A simpler alternative to LayerNorm\n",
    "    \n",
    "    Benefits of RMSNorm vs LayerNorm:\n",
    "    1. ğŸ¯ Simpler: Only requires RMS, not mean AND variance\n",
    "    2. âš¡ Faster: Fewer operations per element\n",
    "    3. ğŸ”¢ Numerically stable: Avoids mean subtraction\n",
    "    4. ğŸ“ Equivalent performance: Similar results to LayerNorm in practice\n",
    "    \n",
    "    Used in: LLaMA, PaLM, GPT-NeoX, and other modern LLMs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        # Learnable scaling parameter (like Î³ in LayerNorm)\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.eps = eps  # Small constant for numerical stability\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Step 1: Compute Root Mean Square\n",
    "        # RMS = âˆš(mean(xÂ²) + Îµ)\n",
    "        variance = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Step 2: Normalize by RMS \n",
    "        # rsqrt is more efficient than 1/sqrt\n",
    "        x = x * torch.rsqrt(variance + self.eps)\n",
    "        \n",
    "        # Step 3: Apply learned scaling\n",
    "        return self.weight * x\n",
    "\n",
    "# ğŸ¯ Create LLaMA-style test data\n",
    "# Modern LLMs use much larger hidden dimensions\n",
    "batch_size, seq_len, hidden_dim = 32, 512, 4096  # LLaMA-7B dimensions\n",
    "\n",
    "rms_input = torch.randn(batch_size, seq_len, hidden_dim, device=device)\n",
    "print(f\"ğŸ“Š RMSNorm input shape: {rms_input.shape}\")\n",
    "print(f\"ğŸ’¾ Memory usage: {rms_input.element_size() * rms_input.numel() / 1024**2:.1f} MB\")\n",
    "print(f\"ğŸ”¢ Total elements: {rms_input.numel():,}\")\n",
    "print(f\"ğŸ“ Hidden dimension: {hidden_dim} (LLaMA-7B scale)\")\n",
    "\n",
    "# Initialize RMSNorm model\n",
    "rmsnorm_model = RMSNorm(hidden_dim).to(device)\n",
    "print(f\"\\nğŸ“ Model configuration:\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in rmsnorm_model.parameters()):,}\")\n",
    "print(f\"   Epsilon: {rmsnorm_model.eps}\")\n",
    "\n",
    "# Compare with equivalent LayerNorm for educational purposes\n",
    "layernorm_model = nn.LayerNorm(hidden_dim).to(device)\n",
    "print(f\"   LayerNorm parameters: {sum(p.numel() for p in layernorm_model.parameters()):,}\")\n",
    "\n",
    "# ğŸ”§ Capture kernels for RMSNorm optimization\n",
    "print(f\"\\nğŸ”§ Compiling and capturing RMSNorm kernels...\")\n",
    "print(f\"   ğŸ¯ Expected optimization: Fused power + mean + rsqrt + multiply\")\n",
    "\n",
    "compiled_rmsnorm, exp3_path = capture_kernels_for_experiment(\n",
    "    rmsnorm_model, rms_input, \"rmsnorm_optimization\"\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š RMSNorm experiment saved to: {exp3_path}\")\n",
    "\n",
    "# ğŸƒâ€â™‚ï¸ Comprehensive benchmark: RMSNorm vs LayerNorm\n",
    "print(f\"\\nğŸƒâ€â™‚ï¸ Comprehensive Performance Analysis:\")\n",
    "print(f\"   ğŸ“Š Comparing RMSNorm vs LayerNorm vs Compiled RMSNorm\")\n",
    "\n",
    "benchmarker_exp3 = PerformanceBenchmarker(warmup_runs=5, benchmark_runs=20)\n",
    "\n",
    "# Benchmark all three variants\n",
    "rmsnorm_baseline = benchmarker_exp3.benchmark_function(\n",
    "    rmsnorm_model, rms_input, \"RMSNorm Baseline\"\n",
    ")\n",
    "\n",
    "rmsnorm_compiled = benchmarker_exp3.benchmark_function(\n",
    "    compiled_rmsnorm, rms_input, \"RMSNorm Compiled\"\n",
    ")\n",
    "\n",
    "layernorm_baseline = benchmarker_exp3.benchmark_function(\n",
    "    layernorm_model, rms_input, \"LayerNorm Baseline\"\n",
    ")\n",
    "\n",
    "# Compare all results\n",
    "all_norm_results = [rmsnorm_baseline, rmsnorm_compiled, layernorm_baseline]\n",
    "benchmarker_exp3.print_results(all_norm_results)\n",
    "\n",
    "# ğŸ”¬ Mathematical correctness verification\n",
    "print(f\"\\nğŸ”¬ Mathematical Verification:\")\n",
    "with torch.no_grad():\n",
    "    rms_output = rmsnorm_model(rms_input)\n",
    "    compiled_output = compiled_rmsnorm(rms_input)\n",
    "    \n",
    "    # Verify outputs match\n",
    "    if torch.allclose(rms_output, compiled_output, rtol=1e-5, atol=1e-6):\n",
    "        print(f\"   âœ… Compiled RMSNorm matches baseline perfectly\")\n",
    "    else:\n",
    "        print(f\"   ğŸ“Š Max difference: {(rms_output - compiled_output).abs().max():.2e}\")\n",
    "    \n",
    "    # Compare RMS vs LayerNorm properties\n",
    "    layernorm_output = layernorm_model(rms_input)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Normalization Comparison:\")\n",
    "    print(f\"   RMSNorm output std: {rms_output.std():.6f}\")\n",
    "    print(f\"   LayerNorm output std: {layernorm_output.std():.6f}\")\n",
    "    print(f\"   RMSNorm output mean: {rms_output.mean():.6f}\")\n",
    "    print(f\"   LayerNorm output mean: {layernorm_output.mean():.6f}\")\n",
    "\n",
    "print(f\"\\nğŸ“ Key Insights:\")\n",
    "print(f\"   â€¢ RMSNorm is computationally simpler than LayerNorm\")\n",
    "print(f\"   â€¢ Modern LLMs prefer RMSNorm for efficiency\")\n",
    "print(f\"   â€¢ Fusion makes normalization operations very fast\")\n",
    "print(f\"   â€¢ Both normalization schemes achieve similar statistical properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570d1c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4: SiLU/Swish Activation (x * sigmoid(x))\n",
    "# \n",
    "# ğŸ“– Educational Focus: Activation Function Variants and Implementation Comparison\n",
    "# \n",
    "# SiLU (Sigmoid Linear Unit) = Swish = x * sigmoid(x)\n",
    "# \n",
    "# Mathematical properties:\n",
    "# - Smooth and differentiable everywhere\n",
    "# - Non-monotonic (has a small \"dip\" near x = -1.25)\n",
    "# - Self-gated: sigmoid(x) acts as a learned gate\n",
    "# - Used in: EfficientNet, MobilenetV3, some transformer variants\n",
    "# \n",
    "# Comparison with other activations:\n",
    "# ReLU(x) = max(0, x)           # Simple but not smooth\n",
    "# GELU(x) = x * Î¦(x)            # Gaussian-based, smooth\n",
    "# SiLU(x) = x * Ïƒ(x)            # Sigmoid-based, smooth\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ§ª EXPERIMENT 4: SiLU/Swish Activation Optimization\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“– Focus: Comparing Custom vs Built-in Implementations\")\n",
    "print(\"ğŸ¯ Key Learning: How PyTorch optimizes different implementation styles\")\n",
    "\n",
    "class SiLUActivation(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom SiLU implementation: x * sigmoid(x)\n",
    "    \n",
    "    This implementation explicitly computes:\n",
    "    1. sigmoid(x) = 1 / (1 + exp(-x))\n",
    "    2. x * sigmoid(x)\n",
    "    \n",
    "    Educational purpose: See how explicit operations get fused\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Explicit computation: x * sigmoid(x)\n",
    "        # This creates an intermediate sigmoid result\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "class SiLUBuiltin(nn.Module):\n",
    "    \"\"\"\n",
    "    Built-in SiLU implementation using PyTorch's optimized version\n",
    "    \n",
    "    PyTorch's nn.SiLU() may have hand-optimized kernels\n",
    "    or special handling in the compilation pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.silu = nn.SiLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.silu(x)\n",
    "\n",
    "# ğŸ¯ Create MLP-style test data\n",
    "# SiLU is commonly used in MLP layers of modern architectures\n",
    "batch_size, seq_len, hidden_dim = 64, 512, 2048  # Typical MLP dimensions\n",
    "\n",
    "silu_input = torch.randn(batch_size, seq_len, hidden_dim, device=device)\n",
    "print(f\"ğŸ“Š SiLU input shape: {silu_input.shape}\")\n",
    "print(f\"ğŸ’¾ Memory usage: {silu_input.element_size() * silu_input.numel() / 1024**2:.1f} MB\")\n",
    "print(f\"ğŸ”¢ Total elements: {silu_input.numel():,}\")\n",
    "\n",
    "# ğŸ”§ Test both implementations and capture their kernels\n",
    "print(f\"\\nğŸ”§ Comparing Implementation Strategies:\")\n",
    "\n",
    "# Strategy 1: Custom explicit implementation\n",
    "print(f\"\\n1ï¸âƒ£ Custom Implementation (x * sigmoid(x)):\")\n",
    "silu_custom_model = SiLUActivation().to(device)\n",
    "compiled_silu_custom, exp4a_path = capture_kernels_for_experiment(\n",
    "    silu_custom_model, silu_input, \"silu_custom_implementation\"\n",
    ")\n",
    "\n",
    "# Strategy 2: Built-in PyTorch implementation\n",
    "print(f\"\\n2ï¸âƒ£ Built-in Implementation (nn.SiLU):\")\n",
    "silu_builtin_model = SiLUBuiltin().to(device)\n",
    "compiled_silu_builtin, exp4b_path = capture_kernels_for_experiment(\n",
    "    silu_builtin_model, silu_input, \"silu_builtin_implementation\"\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š Custom SiLU experiment saved to: {exp4a_path}\")\n",
    "print(f\"ğŸ“Š Built-in SiLU experiment saved to: {exp4b_path}\")\n",
    "\n",
    "# ğŸƒâ€â™‚ï¸ Comprehensive benchmark of all variants\n",
    "print(f\"\\nğŸƒâ€â™‚ï¸ Comprehensive SiLU Performance Analysis:\")\n",
    "print(f\"   ğŸ“Š Testing: Custom vs Built-in vs Compiled versions\")\n",
    "print(f\"   ğŸ¯ Learning: How different implementations affect performance\")\n",
    "\n",
    "benchmarker_exp4 = PerformanceBenchmarker(warmup_runs=5, benchmark_runs=20)\n",
    "\n",
    "# Benchmark all SiLU variants\n",
    "silu_results = []\n",
    "\n",
    "# Custom implementations\n",
    "silu_results.append(benchmarker_exp4.benchmark_function(\n",
    "    silu_custom_model, silu_input, \"SiLU Custom\"\n",
    "))\n",
    "silu_results.append(benchmarker_exp4.benchmark_function(\n",
    "    compiled_silu_custom, silu_input, \"SiLU Custom Compiled\"\n",
    "))\n",
    "\n",
    "# Built-in implementations  \n",
    "silu_results.append(benchmarker_exp4.benchmark_function(\n",
    "    silu_builtin_model, silu_input, \"SiLU Built-in\"\n",
    "))\n",
    "silu_results.append(benchmarker_exp4.benchmark_function(\n",
    "    compiled_silu_builtin, silu_input, \"SiLU Built-in Compiled\"\n",
    "))\n",
    "\n",
    "# Display comprehensive results\n",
    "benchmarker_exp4.print_results(silu_results)\n",
    "\n",
    "# ğŸ”¬ Correctness verification across implementations\n",
    "print(f\"\\nğŸ”¬ Correctness Verification:\")\n",
    "with torch.no_grad():\n",
    "    custom_output = silu_custom_model(silu_input)\n",
    "    builtin_output = silu_builtin_model(silu_input)\n",
    "    compiled_custom = compiled_silu_custom(silu_input)\n",
    "    compiled_builtin = compiled_silu_builtin(silu_input)\n",
    "    \n",
    "    # Check all implementations produce same results\n",
    "    implementations = [\n",
    "        (\"Custom\", custom_output),\n",
    "        (\"Built-in\", builtin_output), \n",
    "        (\"Compiled Custom\", compiled_custom),\n",
    "        (\"Compiled Built-in\", compiled_builtin)\n",
    "    ]\n",
    "    \n",
    "    print(f\"   ğŸ“Š Cross-implementation comparison:\")\n",
    "    for i, (name1, output1) in enumerate(implementations):\n",
    "        for j, (name2, output2) in enumerate(implementations[i+1:], i+1):\n",
    "            max_diff = (output1 - output2).abs().max().item()\n",
    "            if max_diff < 1e-6:\n",
    "                print(f\"   âœ… {name1} â‰ˆ {name2} (max diff: {max_diff:.2e})\")\n",
    "            else:\n",
    "                print(f\"   âš ï¸  {name1} vs {name2} (max diff: {max_diff:.2e})\")\n",
    "\n",
    "# ğŸ“Š Mathematical properties demonstration\n",
    "print(f\"\\nğŸ“Š SiLU Mathematical Properties:\")\n",
    "test_range = torch.linspace(-3, 3, 7, device=device)\n",
    "silu_values = test_range * torch.sigmoid(test_range)\n",
    "\n",
    "print(f\"   Input:  {test_range.cpu().numpy()}\")\n",
    "print(f\"   SiLU:   {silu_values.cpu().numpy()}\")\n",
    "print(f\"   ğŸ“ Note the smooth curve and small dip around x = -1.25\")\n",
    "\n",
    "print(f\"\\nğŸ“ Key Insights:\")\n",
    "print(f\"   â€¢ Built-in implementations may have optimized kernels\")\n",
    "print(f\"   â€¢ Compilation can make custom implementations competitive\")\n",
    "print(f\"   â€¢ Different implementation styles lead to different fusion opportunities\")\n",
    "print(f\"   â€¢ All variants produce mathematically identical results\")\n",
    "print(f\"   â€¢ SiLU provides smooth, self-gated activation behavior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82161a1",
   "metadata": {},
   "source": [
    "## ğŸ” Kernel Analysis and Deep Learning\n",
    "\n",
    "Now comes the exciting part - analyzing what PyTorch and Triton actually generated! This is where we transition from using tools to understanding the underlying optimizations.\n",
    "\n",
    "### ğŸ“ What We're Looking For\n",
    "\n",
    "When analyzing generated kernels, we want to understand:\n",
    "\n",
    "1. **ğŸ”— Fusion Patterns**: Which operations got combined into single kernels?\n",
    "2. **ğŸ§® Memory Access**: How efficiently does the kernel access memory?\n",
    "3. **âš¡ Parallelization**: How work is distributed across GPU cores?\n",
    "4. **ğŸ¯ Optimization Techniques**: What clever optimizations did Triton apply?\n",
    "\n",
    "### ğŸ” Kernel Naming Conventions\n",
    "\n",
    "Triton kernels follow predictable naming patterns:\n",
    "\n",
    "| Pattern | Meaning | Example |\n",
    "|---------|---------|---------|\n",
    "| `triton_poi_fused_*` | Pointwise fused operations | Element-wise operations |\n",
    "| `triton_per_fused_*` | Per-tensor reduction fused | Softmax, LayerNorm |\n",
    "| `triton_red_fused_*` | Reduction operations | Sum, mean across dimensions |\n",
    "\n",
    "### ğŸ§  Understanding Kernel Structure\n",
    "\n",
    "A typical Triton kernel has these components:\n",
    "\n",
    "```python\n",
    "@triton.jit\n",
    "def kernel_name(\n",
    "    input_ptr, output_ptr,    # Memory pointers\n",
    "    n_elements,               # Problem size\n",
    "    BLOCK_SIZE: tl.constexpr  # Compile-time constant\n",
    "):\n",
    "    # 1. Calculate thread/block indices\n",
    "    pid = tl.program_id(0)\n",
    "    \n",
    "    # 2. Load data from memory\n",
    "    data = tl.load(input_ptr + offsets)\n",
    "    \n",
    "    # 3. Perform computations (fused operations!)\n",
    "    result = complex_computation(data)\n",
    "    \n",
    "    # 4. Store results back to memory\n",
    "    tl.store(output_ptr + offsets, result)\n",
    "```\n",
    "\n",
    "### ğŸ“Š Performance Analysis Techniques\n",
    "\n",
    "For each experiment, we'll analyze:\n",
    "- **Kernel Count**: How many kernels were generated?\n",
    "- **Fusion Success**: Which operations got fused together?\n",
    "- **Memory Patterns**: Coalesced vs scattered memory access\n",
    "- **Block Sizes**: How work is partitioned across GPU cores\n",
    "\n",
    "Let's dive into the analysis of our experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0ebfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel Analysis Tools\n",
    "def analyze_experiment_kernels(experiment_path: Path):\n",
    "    \"\"\"Analyze generated kernels in an experiment directory\"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸ” Analyzing kernels in: {experiment_path.name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Read metadata\n",
    "    metadata_file = experiment_path / \"metadata.json\"\n",
    "    if metadata_file.exists():\n",
    "        with open(metadata_file, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        print(f\"ğŸ“‹ Experiment: {metadata.get('experiment_name', 'Unknown')}\")\n",
    "        print(f\"ğŸ“… Created: {metadata.get('created_at', 'Unknown')}\")\n",
    "        print(f\"ğŸ”¢ Kernels found: {len(metadata.get('kernels', []))}\")\n",
    "    \n",
    "    # Find and analyze kernel files\n",
    "    kernel_files = list(experiment_path.glob(\"*.py\"))\n",
    "    \n",
    "    if not kernel_files:\n",
    "        print(\"âŒ No kernel files found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nğŸ“„ Kernel Files ({len(kernel_files)}):\")\n",
    "    for i, kernel_file in enumerate(kernel_files, 1):\n",
    "        content = kernel_file.read_text()\n",
    "        lines = len(content.split('\\n'))\n",
    "        size_kb = len(content.encode('utf-8')) / 1024\n",
    "        \n",
    "        print(f\"   {i}. {kernel_file.name} ({lines} lines, {size_kb:.1f} KB)\")\n",
    "        \n",
    "        # Extract key information\n",
    "        if '@triton.jit' in content:\n",
    "            print(f\"      âœ… Triton JIT kernel detected\")\n",
    "        \n",
    "        if 'tl.load' in content and 'tl.store' in content:\n",
    "            print(f\"      ğŸ”„ Memory operations: load/store patterns found\")\n",
    "        \n",
    "        if 'BLOCK_SIZE' in content or 'block_size' in content:\n",
    "            print(f\"      ğŸ“¦ Block-based processing detected\")\n",
    "        \n",
    "        # Look for fusion patterns\n",
    "        fusion_indicators = []\n",
    "        if 'layer_norm' in content.lower():\n",
    "            fusion_indicators.append(\"LayerNorm\")\n",
    "        if 'gelu' in content.lower():\n",
    "            fusion_indicators.append(\"GELU\")\n",
    "        if 'softmax' in content.lower():\n",
    "            fusion_indicators.append(\"Softmax\")\n",
    "        if 'dropout' in content.lower():\n",
    "            fusion_indicators.append(\"Dropout\")\n",
    "        if 'sigmoid' in content.lower():\n",
    "            fusion_indicators.append(\"Sigmoid\")\n",
    "        \n",
    "        if fusion_indicators:\n",
    "            print(f\"      ğŸ”— Fusion detected: {' + '.join(fusion_indicators)}\")\n",
    "\n",
    "def create_experiment_summary():\n",
    "    \"\"\"Create a summary of all experiments\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ“Š EXPERIMENT SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    base_path = Path(\"./triton_kernels\")\n",
    "    if not base_path.exists():\n",
    "        print(\"âŒ No experiments found\")\n",
    "        return\n",
    "    \n",
    "    experiments = [d for d in base_path.iterdir() if d.is_dir()]\n",
    "    \n",
    "    if not experiments:\n",
    "        print(\"âŒ No experiment directories found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ğŸ§ª Total experiments: {len(experiments)}\\n\")\n",
    "    \n",
    "    for exp_dir in sorted(experiments):\n",
    "        analyze_experiment_kernels(exp_dir)\n",
    "        print()\n",
    "\n",
    "# Analyze all experiments\n",
    "create_experiment_summary()\n",
    "\n",
    "# Show directory structure\n",
    "print(\"\\nğŸ“ Final Directory Structure:\")\n",
    "def show_tree(path: Path, prefix=\"\", max_depth=3, current_depth=0):\n",
    "    \"\"\"Show directory tree structure\"\"\"\n",
    "    if current_depth >= max_depth:\n",
    "        return\n",
    "    \n",
    "    if path.is_dir():\n",
    "        items = sorted(list(path.iterdir()))\n",
    "        for i, item in enumerate(items):\n",
    "            is_last = i == len(items) - 1\n",
    "            current_prefix = \"â””â”€â”€ \" if is_last else \"â”œâ”€â”€ \"\n",
    "            print(f\"{prefix}{current_prefix}{item.name}\")\n",
    "            \n",
    "            if item.is_dir() and current_depth < max_depth - 1:\n",
    "                next_prefix = prefix + (\"    \" if is_last else \"â”‚   \")\n",
    "                show_tree(item, next_prefix, max_depth, current_depth + 1)\n",
    "\n",
    "triton_kernels_path = Path(\"./triton_kernels\")\n",
    "if triton_kernels_path.exists():\n",
    "    print(f\"{triton_kernels_path}/\")\n",
    "    show_tree(triton_kernels_path)\n",
    "else:\n",
    "    print(\"âŒ Triton kernels directory not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c6507c",
   "metadata": {},
   "source": [
    "## ğŸ“ Conclusions and Mastery Path\n",
    "\n",
    "### ğŸ”‘ Key Insights from Our Journey\n",
    "\n",
    "Through our systematic exploration, we've uncovered fundamental principles of GPU kernel optimization:\n",
    "\n",
    "#### 1. **Kernel Fusion is Transformative** ğŸ”—\n",
    "- **Memory Bandwidth**: The primary bottleneck for most ML operations\n",
    "- **Fusion Benefits**: Combining operations eliminates intermediate memory transfers\n",
    "- **Automatic Optimization**: PyTorch + Triton handles this complexity for us\n",
    "\n",
    "#### 2. **Compilation Has Two Phases** âš¡\n",
    "- **First Run**: 10-100x slower due to kernel generation and autotuning\n",
    "- **Subsequent Runs**: Near-optimal performance using cached kernels\n",
    "- **Production Tip**: Pre-compile in development, cache in production\n",
    "\n",
    "#### 3. **Different Patterns, Different Optimizations** ğŸ¯\n",
    "- **Sequential Operations** (LayerNorm + GELU): Straightforward fusion\n",
    "- **Reduction Operations** (Softmax + Dropout): Complex memory patterns\n",
    "- **Alternative Implementations** (RMSNorm): Simpler can be faster\n",
    "- **Built-in vs Custom**: Multiple paths to optimization\n",
    "\n",
    "### ğŸ› ï¸ Practical Optimization Strategies\n",
    "\n",
    "#### Memory-First Thinking ğŸ’¾\n",
    "```python\n",
    "# Bad: Multiple memory roundtrips\n",
    "x = layer_norm(x)      # Memory: Load x, store normalized\n",
    "x = gelu(x)           # Memory: Load normalized, store activated\n",
    "\n",
    "# Good: Single memory roundtrip  \n",
    "x = compiled_layer_norm_gelu(x)  # Memory: Load x, store final result\n",
    "```\n",
    "\n",
    "#### Leverage Autotuning ğŸ¯\n",
    "- Let Triton find optimal block sizes for your hardware\n",
    "- Use `mode=\"max-autotune\"` for best performance\n",
    "- Cache compiled kernels across runs\n",
    "\n",
    "#### Profile Before Optimizing ğŸ“Š\n",
    "- Measure baseline performance first\n",
    "- Identify memory-bound vs compute-bound operations\n",
    "- Focus optimization efforts where they matter most\n",
    "\n",
    "### ğŸš€ Advanced Optimization Techniques\n",
    "\n",
    "#### 1. **Custom Triton Kernels** âœï¸\n",
    "When PyTorch's automatic fusion isn't enough:\n",
    "- Write hand-optimized Triton kernels for critical paths\n",
    "- Implement novel algorithms not available in PyTorch\n",
    "- Optimize for specific hardware characteristics\n",
    "\n",
    "#### 2. **Mixed Precision Optimization** ğŸ¨\n",
    "```python\n",
    "# Combine kernel fusion with mixed precision\n",
    "@torch.compile(mode=\"max-autotune\")\n",
    "def optimized_attention(q, k, v):\n",
    "    # Automatically uses appropriate precision\n",
    "    scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.matmul(weights, v)\n",
    "```\n",
    "\n",
    "#### 3. **Memory Layout Optimization** ğŸ“\n",
    "- Experiment with different tensor layouts (NCHW vs NHWC)\n",
    "- Use tensor cores when available (requires specific layouts)\n",
    "- Consider padding strategies for optimal memory alignment\n",
    "\n",
    "### ğŸ”¬ Experiment Organization Benefits\n",
    "\n",
    "Our structured approach provides:\n",
    "\n",
    "#### **Reproducibility** ğŸ”„\n",
    "- Each experiment is self-contained with metadata\n",
    "- Easy to reproduce results across different hardware\n",
    "- Clear documentation of what was tested\n",
    "\n",
    "#### **Comparison** âš–ï¸\n",
    "- Side-by-side performance analysis\n",
    "- Clear identification of best-performing approaches\n",
    "- Understanding of trade-offs between different methods\n",
    "\n",
    "#### **Learning** ğŸ“\n",
    "- Generated kernels serve as learning materials\n",
    "- Progression from simple to complex patterns\n",
    "- Foundation for advanced optimization work\n",
    "\n",
    "### ğŸ›¤ï¸ Your Next Steps\n",
    "\n",
    "#### Immediate Actions (This Week) ğŸ“…\n",
    "1. **Apply to Your Models**: Use `@torch.compile()` on your existing PyTorch models\n",
    "2. **Measure Impact**: Benchmark before/after compilation on your workloads  \n",
    "3. **Experiment**: Try different fusion patterns from this notebook\n",
    "\n",
    "#### Intermediate Exploration (This Month) ğŸ“ˆ\n",
    "1. **Custom Patterns**: Implement fusion for operations specific to your domain\n",
    "2. **Hardware Tuning**: Experiment with different GPUs and configurations\n",
    "3. **Production Integration**: Deploy compiled models in your applications\n",
    "\n",
    "#### Advanced Mastery (Ongoing) ğŸš€\n",
    "1. **Custom Triton Kernels**: Write hand-optimized kernels for critical operations\n",
    "2. **Multi-GPU Scaling**: Extend optimizations to distributed settings\n",
    "3. **Novel Algorithms**: Implement cutting-edge research with optimal GPU utilization\n",
    "\n",
    "### ğŸ¯ Final Thoughts\n",
    "\n",
    "GPU kernel optimization is both an art and a science. The tools (PyTorch + Triton) handle much of the complexity, but understanding the principles helps you:\n",
    "\n",
    "- **Debug Performance Issues**: Know where to look when things are slow\n",
    "- **Design Better Architectures**: Choose patterns that optimize well\n",
    "- **Push Boundaries**: Implement novel ideas with optimal performance\n",
    "\n",
    "The organized experimental approach we've developed here serves as a foundation for continued exploration and optimization of your specific workloads.\n",
    "\n",
    "**ğŸ‰ Congratulations! You've mastered the fundamentals of PyTorch kernel optimization with Triton!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9182090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ Quick Start Guide\n",
    "print(\"ğŸš€ TRITON OPTIMIZATION NOTEBOOK - QUICK START\")\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ“š To use this notebook:\")\n",
    "print(\"   1. Run cells sequentially from top to bottom\")\n",
    "print(\"   2. Each experiment creates its own organized directory\")\n",
    "print(\"   3. Check ./triton_kernels/ for generated kernels and analysis\")\n",
    "print(\"   4. Modify patterns to test your own operations\")\n",
    "print(\"\")\n",
    "print(\"ğŸ“ Learning Path:\")\n",
    "print(\"   Experiment 1: LayerNorm + GELU (fundamentals)\")\n",
    "print(\"   Experiment 2: Softmax + Dropout (attention)\")  \n",
    "print(\"   Experiment 3: RMSNorm (modern normalization)\")\n",
    "print(\"   Experiment 4: SiLU variants (implementation comparison)\")\n",
    "print(\"\")\n",
    "print(\"ğŸ”¬ Each experiment includes:\")\n",
    "print(\"   â€¢ Educational explanations and mathematical background\")\n",
    "print(\"   â€¢ Generated Triton kernels with organized storage\")\n",
    "print(\"   â€¢ Performance benchmarks and analysis\")\n",
    "print(\"   â€¢ Correctness verification and insights\")\n",
    "print(\"\")\n",
    "print(\"âœ¨ Ready to explore GPU kernel optimization!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-qat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
