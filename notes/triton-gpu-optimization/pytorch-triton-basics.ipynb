{
 "cells": [
  {
   "cell_type": "raw",
   "id": "8321858c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"PyTorch + Triton Fundamentals: Understanding Compilation and Optimization\"\n",
    "format: \n",
    "  html:\n",
    "    code-fold: true\n",
    "jupyter: python3\n",
    "page-layout: full\n",
    "categories: [pytorch, triton, compilation, optimization]\n",
    "description: \"Explore the compilation and optimization processes in PyTorch and Triton, focusing on how\n",
    "to effectively use these tools for high-performance computing tasks.\"\n",
    "image: \"https://example.com/image.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbf30cc",
   "metadata": {},
   "source": [
    "# PyTorch + Triton Fundamentals\n",
    "\n",
    "This notebook provides a comprehensive introduction to PyTorch's compilation system and how it leverages Triton for GPU optimization. We'll focus on understanding the fundamentals before diving into advanced kernel optimization.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- How PyTorch's compilation system works internally\n",
    "- Why compilation has overhead and how to manage it\n",
    "- How to use environment variables for debugging and optimization\n",
    "- Best practices for production deployment\n",
    "- How to troubleshoot common compilation issues\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "### Core Concepts\n",
    "1. **PyTorch Compilation Pipeline**: From Python code to optimized GPU kernels\n",
    "2. **Environment Variables**: Powerful debugging and monitoring tools\n",
    "3. **Performance Patterns**: Understanding compilation overhead vs execution benefits\n",
    "4. **Production Deployment**: Best practices for real-world applications\n",
    "\n",
    "### Practical Skills\n",
    "- Setting up optimal development environments\n",
    "- Debugging compilation issues effectively\n",
    "- Measuring and analyzing performance impacts\n",
    "- Deploying compiled models in production\n",
    "\n",
    "Let's start with the fundamentals!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d958205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup and Foundation\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def setup_pytorch_triton_environment():\n",
    "    \"\"\"\n",
    "    Configure PyTorch and Triton for educational exploration\n",
    "    \n",
    "    This function demonstrates how to set up environment variables\n",
    "    that provide deep insights into PyTorch's compilation process.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üöÄ Setting up PyTorch + Triton Learning Environment\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Core environment variables for understanding compilation\n",
    "    educational_settings = {\n",
    "        # Show generated kernel code - see what Triton creates\n",
    "        \"TORCH_LOGS\": \"output_code\",\n",
    "        \n",
    "        # Display autotuning process - see optimization in action\n",
    "        \"TRITON_PRINT_AUTOTUNING\": \"1\", \n",
    "        \n",
    "        # Show cache statistics - understand reuse patterns\n",
    "        \"TRITON_PRINT_CACHE_STATS\": \"1\",\n",
    "        \n",
    "        # Additional debugging (optional)\n",
    "        # \"TORCH_LOGS\": \"output_code,dynamo,inductor\",  # More detailed logs\n",
    "        # \"TRITON_PRINT_CACHE_DIR\": \"1\",  # Show cache directory\n",
    "    }\n",
    "    \n",
    "    for key, value in educational_settings.items():\n",
    "        os.environ[key] = value\n",
    "        print(f\"‚úÖ {key} = '{value}'\")\n",
    "    \n",
    "    print(f\"\\nüìñ What these variables reveal:\")\n",
    "    print(f\"  ‚Ä¢ TORCH_LOGS: Shows actual generated Triton kernel source code\")\n",
    "    print(f\"  ‚Ä¢ TRITON_PRINT_AUTOTUNING: Displays different configurations being tested\")\n",
    "    print(f\"  ‚Ä¢ TRITON_PRINT_CACHE_STATS: Shows kernel cache hits vs misses\")\n",
    "    \n",
    "    return educational_settings\n",
    "\n",
    "def detect_and_configure_device():\n",
    "    \"\"\"\n",
    "    Detect GPU capabilities and configure for optimal learning\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüîç Device Detection and Configuration\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        print(f\"‚úÖ CUDA GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   Device count: {torch.cuda.device_count()}\")\n",
    "        print(f\"   CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"   Compute capability: {torch.cuda.get_device_capability(0)}\")\n",
    "        print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "        \n",
    "        # Check Triton availability\n",
    "        try:\n",
    "            import triton\n",
    "            print(f\"‚úÖ Triton available: {triton.__version__}\")\n",
    "        except ImportError:\n",
    "            print(f\"‚ö†Ô∏è  Triton not available - install with: pip install triton\")\n",
    "            \n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        print(\"‚ö†Ô∏è  CUDA not available - using CPU\")\n",
    "        print(\"   Note: Many optimizations are GPU-specific\")\n",
    "    \n",
    "    print(f\"\\nüéØ Selected device: {device.upper()}\")\n",
    "    return device\n",
    "\n",
    "# Initialize the learning environment\n",
    "settings = setup_pytorch_triton_environment()\n",
    "device = detect_and_configure_device()\n",
    "\n",
    "print(f\"\\n‚úÖ Environment ready for PyTorch + Triton exploration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9101c81d",
   "metadata": {},
   "source": [
    "## Understanding PyTorch Compilation Pipeline\n",
    "\n",
    "### üß† How PyTorch Compilation Works\n",
    "\n",
    "When you use `@torch.compile()`, PyTorch goes through several sophisticated stages:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e011f70e",
   "metadata": {},
   "source": [
    "---\n",
    "engine: jupyter\n",
    "---\n",
    "\n",
    "```{mermaid}\n",
    "---\n",
    "title: PyTorch Compilation Pipeline\n",
    "config:\n",
    "  theme: base\n",
    "  themeVariables:\n",
    "    primaryColor: \"#ff6b6b\"\n",
    "    primaryTextColor: \"#2c3e50\"\n",
    "    primaryBorderColor: \"#3498db\"\n",
    "    lineColor: \"#34495e\"\n",
    "    secondaryColor: \"#74b9ff\"\n",
    "    tertiaryColor: \"#a29bfe\"\n",
    "---\n",
    "\n",
    "flowchart LR\n",
    "    %% Define styles\n",
    "    classDef startEnd fill:#e74c3c,stroke:#c0392b,stroke-width:3px,color:#fff,font-weight:bold\n",
    "    classDef process fill:#3498db,stroke:#2980b9,stroke-width:2px,color:#fff,font-weight:bold\n",
    "    classDef optimization fill:#f39c12,stroke:#e67e22,stroke-width:2px,color:#fff,font-weight:bold\n",
    "    classDef generation fill:#9b59b6,stroke:#8e44ad,stroke-width:2px,color:#fff,font-weight:bold\n",
    "    classDef storage fill:#2ecc71,stroke:#27ae60,stroke-width:2px,color:#fff,font-weight:bold\n",
    "    \n",
    "    %% Define subgraphs first to ensure horizontal alignment\n",
    "    subgraph Frontend [\"üîß Frontend Processing\"]\n",
    "        direction TB\n",
    "        A((\"üêç<br/>Python<br/>Code\")):::startEnd\n",
    "        B[\"üìä Graph<br/>Capture\"]:::process\n",
    "        C{\"‚ö° Graph<br/>Optimization\"}:::optimization\n",
    "        A ==> B ==> C\n",
    "    end\n",
    "    \n",
    "    subgraph Backend [\"‚ö° Backend Processing\"]\n",
    "        direction TB\n",
    "        D[/\"üéØ Backend<br/>Selection\"/]:::process\n",
    "        E[[\"‚öôÔ∏è Kernel<br/>Generation\"]]:::generation\n",
    "        F[\"üî® Compilation\"]:::generation\n",
    "        D ==> E ==> F\n",
    "    end\n",
    "    \n",
    "    subgraph Runtime [\"üèÉ Runtime\"]\n",
    "        direction TB\n",
    "        G[(\"üíæ Caching\")]:::storage\n",
    "        H((\"üöÄ<br/>Execution\")):::startEnd\n",
    "        G ==> H\n",
    "    end\n",
    "    \n",
    "    %% Connect the subgraphs\n",
    "    Frontend ==> Backend ==> Runtime\n",
    "    \n",
    "    %% Style the subgraphs\n",
    "    style Frontend fill:#ecf0f1,stroke:#bdc3c7,stroke-width:2px,stroke-dasharray: 5 5\n",
    "    style Backend fill:#fdf2e9,stroke:#f39c12,stroke-width:2px,stroke-dasharray: 5 5\n",
    "    style Runtime fill:#e8f8f5,stroke:#2ecc71,stroke-width:2px,stroke-dasharray: 5 5\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6c2bb7",
   "metadata": {},
   "source": [
    "\n",
    "Let's explore each stage:\n",
    "\n",
    "#### 1. **Graph Capture** üìä\n",
    "- PyTorch traces your Python code execution\n",
    "- Creates a computation graph (nodes = operations, edges = data flow)\n",
    "- Captures control flow and data dependencies\n",
    "\n",
    "#### 2. **Graph Optimization** ‚ö°\n",
    "- Fusion opportunities identified (combine multiple ops)\n",
    "- Dead code elimination (remove unused computations)\n",
    "- Constant folding (precompute constant expressions)\n",
    "- Memory layout optimization\n",
    "\n",
    "#### 3. **Backend Selection** üéØ\n",
    "- Triton selected for GPU operations\n",
    "- Different backends for different hardware (CPU, GPU, TPU)\n",
    "- Backend-specific optimization passes\n",
    "\n",
    "#### 4. **Kernel Generation** üîß\n",
    "- Triton generates GPU kernel source code\n",
    "- Automatic memory management and parallelization\n",
    "- Hardware-specific optimizations applied\n",
    "\n",
    "#### 5. **Compilation** ‚öôÔ∏è\n",
    "- Triton kernels compiled to GPU machine code\n",
    "- CUDA compilation pipeline invoked\n",
    "- Binary kernels ready for execution\n",
    "\n",
    "#### 6. **Caching** üíæ\n",
    "- Compiled kernels cached for reuse\n",
    "- Cache keys based on input shapes and types\n",
    "- Avoids recompilation for identical patterns\n",
    "\n",
    "### üéì Key Insight: Two-Phase Performance\n",
    "\n",
    "This pipeline explains the fundamental performance pattern:\n",
    "- **First Run**: Slow (includes all compilation overhead)\n",
    "- **Subsequent Runs**: Fast (uses cached compiled kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c665250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating Compilation Overhead vs Execution Speed\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "def demonstrate_compilation_phases():\n",
    "    \"\"\"\n",
    "    Practical demonstration of compilation overhead vs execution speed\n",
    "    \n",
    "    This function shows the two-phase performance pattern that's\n",
    "    fundamental to understanding PyTorch compilation.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üß™ DEMONSTRATION: Compilation Phases\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Enable verbose compilation output to see Triton kernel generation\n",
    "    old_verbose = os.environ.get('TORCH_COMPILE_DEBUG', '0')\n",
    "    os.environ['TORCH_COMPILE_DEBUG'] = '1'\n",
    "    \n",
    "    # Also enable TorchInductor debug output\n",
    "    import torch._inductor.config as config\n",
    "    old_debug = config.debug\n",
    "    config.debug = True\n",
    "    \n",
    "    print(\"üîß Enabled verbose compilation output - you should now see Triton kernel generation!\")\n",
    "    \n",
    "    try:\n",
    "        # Create a simple but representative model\n",
    "        class SimpleModel(nn.Module):\n",
    "            def __init__(self, hidden_size=512):\n",
    "                super().__init__()\n",
    "                self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "                \n",
    "            def forward(self, x):\n",
    "                # Simple pattern: normalize then activate\n",
    "                normalized = self.layer_norm(x)\n",
    "                return F.gelu(normalized)\n",
    "        \n",
    "        # Initialize model and test data\n",
    "        model = SimpleModel().to(device)\n",
    "        test_input = torch.randn(32, 128, 512, device=device)\n",
    "        \n",
    "        print(f\"\\nüìä Test configuration:\")\n",
    "        print(f\"   Model: LayerNorm + GELU\")\n",
    "        print(f\"   Input shape: {test_input.shape}\")\n",
    "        print(f\"   Device: {device}\")\n",
    "        \n",
    "        # Phase 1: Measure baseline (uncompiled) performance\n",
    "        print(f\"\\nüîç Phase 1: Baseline (Uncompiled) Performance\")\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(5):\n",
    "            with torch.no_grad():\n",
    "                _ = model(test_input)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        # Measure baseline\n",
    "        baseline_times = []\n",
    "        for _ in range(10):\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            start = time.perf_counter()\n",
    "            with torch.no_grad():\n",
    "                output = model(test_input)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            baseline_times.append(time.perf_counter() - start)\n",
    "        \n",
    "        baseline_avg = sum(baseline_times) / len(baseline_times)\n",
    "        print(f\"   Average time: {baseline_avg*1000:.3f} ms\")\n",
    "        \n",
    "        # Phase 2: Compile the model\n",
    "        print(f\"\\nüîß Phase 2: Compiling Model (Watch for Triton Output Below)\")\n",
    "        print(f\"   Note: With debug enabled, you should see detailed Triton kernel generation\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        compiled_model = torch.compile(model, mode=\"default\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"üîö End of compilation output\")\n",
    "        \n",
    "        # Phase 3: First run (compilation + execution)\n",
    "        print(f\"\\n‚è±Ô∏è  Phase 3: First Run (Compilation + Execution)\")\n",
    "        print(f\"   Note: Additional Triton kernels may be generated during first execution\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            compiled_output = compiled_model(test_input)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        first_run_time = time.perf_counter() - start\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"   First run time: {first_run_time*1000:.3f} ms\")\n",
    "        print(f\"   Overhead factor: {first_run_time/baseline_avg:.1f}x slower than baseline\")\n",
    "        \n",
    "        # Phase 4: Subsequent runs (cached execution)\n",
    "        print(f\"\\n‚ö° Phase 4: Subsequent Runs (Cached Kernels)\")\n",
    "        \n",
    "        cached_times = []\n",
    "        for i in range(10):\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            start = time.perf_counter()\n",
    "            with torch.no_grad():\n",
    "                _ = compiled_model(test_input)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            cached_times.append(time.perf_counter() - start)\n",
    "        \n",
    "        cached_avg = sum(cached_times) / len(cached_times)\n",
    "        print(f\"   Average cached time: {cached_avg*1000:.3f} ms\")\n",
    "        print(f\"   Speedup vs baseline: {baseline_avg/cached_avg:.2f}x\")\n",
    "        print(f\"   Speedup vs first run: {first_run_time/cached_avg:.1f}x\")\n",
    "        \n",
    "        # Verify correctness\n",
    "        max_diff = (output - compiled_output).abs().max().item()\n",
    "        print(f\"\\n‚úÖ Correctness check: Max difference = {max_diff:.2e}\")\n",
    "        \n",
    "        return {\n",
    "            'baseline_avg': baseline_avg,\n",
    "            'first_run_time': first_run_time, \n",
    "            'cached_avg': cached_avg,\n",
    "            'compilation_overhead': first_run_time / baseline_avg,\n",
    "            'speedup': baseline_avg / cached_avg\n",
    "        }\n",
    "    \n",
    "    finally:\n",
    "        # Restore original settings\n",
    "        os.environ['TORCH_COMPILE_DEBUG'] = old_verbose\n",
    "        config.debug = old_debug\n",
    "        print(f\"\\nüîß Restored original debug settings\")\n",
    "\n",
    "# Run the demonstration\n",
    "results = demonstrate_compilation_phases()\n",
    "\n",
    "print(f\"\\nüéì Key Takeaways:\")\n",
    "print(f\"   ‚Ä¢ Compilation adds significant overhead to first run\")\n",
    "print(f\"   ‚Ä¢ Subsequent runs benefit from cached optimized kernels\")\n",
    "print(f\"   ‚Ä¢ The break-even point depends on how many times you'll run the model\")\n",
    "print(f\"   ‚Ä¢ In production, you want to 'warm up' during initialization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0815f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIRECT APPROACH: Capture and display Triton compilation output\n",
    "import torch\n",
    "import sys\n",
    "import io\n",
    "import contextlib\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Clear any previous compilations\n",
    "torch._dynamo.reset()\n",
    "\n",
    "print(\"üéØ DIRECT TRITON KERNEL GENERATION CAPTURE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Set up comprehensive logging\n",
    "logging.basicConfig(level=logging.DEBUG, force=True)\n",
    "\n",
    "# Enable ALL debug output\n",
    "os.environ.update({\n",
    "    'TORCH_COMPILE_DEBUG': '1',\n",
    "    'TORCH_LOGS': '+dynamo,+inductor,+aot',\n",
    "    'TORCHINDUCTOR_VERBOSE': '1',\n",
    "    'TRITON_PRINT_AUTOTUNING': '1',\n",
    "    'TRITON_DEBUG': '1'\n",
    "})\n",
    "\n",
    "# Configure inductor for maximum verbosity\n",
    "import torch._inductor.config as config\n",
    "config.debug = True\n",
    "config.trace.enabled = True\n",
    "config.verbose_progress = True\n",
    "\n",
    "print(\"üîß Environment configured for maximum compilation visibility\")\n",
    "\n",
    "# Create a simple model that will definitely generate Triton kernels\n",
    "def triton_demo_model(x):\n",
    "    # This pattern should trigger multiple Triton kernels\n",
    "    y = torch.relu(x)           # Pointwise operation\n",
    "    z = y * 2.0 + 1.0          # Fused arithmetic\n",
    "    return torch.sum(z, dim=-1) # Reduction operation\n",
    "\n",
    "# Test data\n",
    "x = torch.randn(512, 512, device=device, requires_grad=False)\n",
    "\n",
    "print(f\"\\nüìä Input: {x.shape} on {device}\")\n",
    "print(\"\\nüöÄ COMPILING MODEL - Watch for Triton output below:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Capture stdout and stderr during compilation\n",
    "stdout_capture = io.StringIO()\n",
    "stderr_capture = io.StringIO()\n",
    "\n",
    "try:\n",
    "    with contextlib.redirect_stdout(stdout_capture), \\\n",
    "         contextlib.redirect_stderr(stderr_capture):\n",
    "        \n",
    "        # Compile the model\n",
    "        compiled_model = torch.compile(triton_demo_model, mode=\"default\")\n",
    "        \n",
    "        # First execution (triggers kernel generation)\n",
    "        result = compiled_model(x)\n",
    "        \n",
    "    # Get captured output\n",
    "    stdout_output = stdout_capture.getvalue()\n",
    "    stderr_output = stderr_capture.getvalue()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üîö END OF COMPILATION\")\n",
    "    \n",
    "    # Display captured output\n",
    "    if stdout_output:\n",
    "        print(f\"\\nüìù CAPTURED STDOUT ({len(stdout_output)} chars):\")\n",
    "        print(\"-\" * 40)\n",
    "        print(stdout_output[:2000])  # Show first 2000 chars\n",
    "        if len(stdout_output) > 2000:\n",
    "            print(f\"... ({len(stdout_output) - 2000} more characters)\")\n",
    "    \n",
    "    if stderr_output:\n",
    "        print(f\"\\nüìù CAPTURED STDERR ({len(stderr_output)} chars):\")\n",
    "        print(\"-\" * 40)\n",
    "        print(stderr_output[:2000])  # Show first 2000 chars\n",
    "        if len(stderr_output) > 2000:\n",
    "            print(f\"... ({len(stderr_output) - 2000} more characters)\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Compilation successful!\")\n",
    "    print(f\"   Result shape: {result.shape}\")\n",
    "    print(f\"   Result: {result[:5]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during compilation: {e}\")\n",
    "    # Still show captured output even if there was an error\n",
    "    stdout_output = stdout_capture.getvalue()\n",
    "    stderr_output = stderr_capture.getvalue()\n",
    "    \n",
    "    if stdout_output:\n",
    "        print(f\"\\nüìù PARTIAL STDOUT:\")\n",
    "        print(stdout_output[:1000])\n",
    "    if stderr_output:\n",
    "        print(f\"\\nüìù PARTIAL STDERR:\")\n",
    "        print(stderr_output[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65174994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMINE GENERATED TRITON KERNELS DIRECTLY\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üîç EXAMINING GENERATED TRITON KERNELS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Check the debug trace directory mentioned in the output above\n",
    "debug_base = \"/home/alibina/repo/innovation_crucible/notes/triton-gpu-optimization/torch_compile_debug\"\n",
    "if os.path.exists(debug_base):\n",
    "    # Find the latest run directory\n",
    "    run_dirs = glob.glob(f\"{debug_base}/run_*\")\n",
    "    if run_dirs:\n",
    "        latest_run = max(run_dirs, key=os.path.getctime)\n",
    "        inductor_dir = os.path.join(latest_run, \"torchinductor\")\n",
    "        \n",
    "        print(f\"üìÇ Latest debug run: {os.path.basename(latest_run)}\")\n",
    "        print(f\"üìÇ Inductor directory: {inductor_dir}\")\n",
    "        \n",
    "        if os.path.exists(inductor_dir):\n",
    "            # Find all generated files\n",
    "            all_files = []\n",
    "            for root, dirs, files in os.walk(inductor_dir):\n",
    "                for file in files:\n",
    "                    full_path = os.path.join(root, file)\n",
    "                    all_files.append(full_path)\n",
    "            \n",
    "            print(f\"\\nüìÑ Found {len(all_files)} generated files:\")\n",
    "            \n",
    "            # Categorize files\n",
    "            py_files = [f for f in all_files if f.endswith('.py')]\n",
    "            cpp_files = [f for f in all_files if f.endswith(('.cpp', '.h'))]\n",
    "            other_files = [f for f in all_files if not f.endswith(('.py', '.cpp', '.h', '.lock'))]\n",
    "            \n",
    "            print(f\"   üêç Python files: {len(py_files)}\")\n",
    "            print(f\"   üîß C++ files: {len(cpp_files)}\")\n",
    "            print(f\"   üìã Other files: {len(other_files)}\")\n",
    "            \n",
    "            # Show Python files (likely Triton kernels)\n",
    "            if py_files:\n",
    "                print(f\"\\nüêç PYTHON/TRITON KERNEL FILES:\")\n",
    "                for f in py_files:\n",
    "                    rel_path = os.path.relpath(f, inductor_dir)\n",
    "                    size = os.path.getsize(f)\n",
    "                    print(f\"   üìÑ {rel_path} ({size} bytes)\")\n",
    "                \n",
    "                # Show content of the first substantial Python file\n",
    "                substantial_py = [f for f in py_files if os.path.getsize(f) > 100]\n",
    "                if substantial_py:\n",
    "                    print(f\"\\nüìù KERNEL SOURCE CODE ({os.path.basename(substantial_py[0])}):\")\n",
    "                    print(\"-\" * 50)\n",
    "                    try:\n",
    "                        with open(substantial_py[0], 'r') as file:\n",
    "                            content = file.read()\n",
    "                            lines = content.split('\\n')\n",
    "                            \n",
    "                            # Show the content with line numbers\n",
    "                            for i, line in enumerate(lines[:50], 1):  # First 50 lines\n",
    "                                print(f\"{i:3d}: {line}\")\n",
    "                            \n",
    "                            if len(lines) > 50:\n",
    "                                print(f\"... ({len(lines) - 50} more lines)\")\n",
    "                            \n",
    "                            # Look for Triton-specific keywords\n",
    "                            triton_keywords = ['@triton.jit', 'tl.program_id', 'tl.load', 'tl.store', 'BLOCK_SIZE']\n",
    "                            found_keywords = [kw for kw in triton_keywords if kw in content]\n",
    "                            \n",
    "                            if found_keywords:\n",
    "                                print(f\"\\nüéØ TRITON KEYWORDS FOUND: {', '.join(found_keywords)}\")\n",
    "                            else:\n",
    "                                print(f\"\\n‚ÑπÔ∏è  This appears to be generated wrapper code, not raw Triton kernel\")\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ùå Could not read file: {e}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Inductor directory not found: {inductor_dir}\")\n",
    "    else:\n",
    "        print(\"‚ùå No debug run directories found\")\n",
    "else:\n",
    "    print(f\"‚ùå Debug base directory not found: {debug_base}\")\n",
    "\n",
    "# Also check the kernel cache\n",
    "print(f\"\\nüìÅ CHECKING KERNEL CACHE\")\n",
    "cache_dir = \"/tmp/torchinductor_alibina\"\n",
    "if os.path.exists(cache_dir):\n",
    "    cache_files = []\n",
    "    for root, dirs, files in os.walk(cache_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.py'):\n",
    "                cache_files.append(os.path.join(root, file))\n",
    "    \n",
    "    print(f\"üîß Found {len(cache_files)} cached Python files\")\n",
    "    \n",
    "    if cache_files:\n",
    "        # Show the most recent cache file\n",
    "        latest_cache = max(cache_files, key=os.path.getctime)\n",
    "        print(f\"\\nüìù LATEST CACHED KERNEL ({os.path.basename(latest_cache)}):\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        try:\n",
    "            with open(latest_cache, 'r') as file:\n",
    "                content = file.read()\n",
    "                lines = content.split('\\n')\n",
    "                \n",
    "                for i, line in enumerate(lines[:30], 1):  # First 30 lines\n",
    "                    print(f\"{i:3d}: {line}\")\n",
    "                \n",
    "                if len(lines) > 30:\n",
    "                    print(f\"... ({len(lines) - 30} more lines)\")\n",
    "                \n",
    "                # Check for Triton signatures\n",
    "                if '@triton.jit' in content:\n",
    "                    print(\"\\n‚úÖ This is a genuine Triton kernel!\")\n",
    "                elif 'triton' in content.lower():\n",
    "                    print(\"\\nüîß This file references Triton\")\n",
    "                else:\n",
    "                    print(\"\\n‚ÑπÔ∏è  This appears to be wrapper/helper code\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Could not read cache file: {e}\")\n",
    "else:\n",
    "    print(f\"‚ùå Cache directory not found: {cache_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd46256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL APPROACH: Direct FX compilation to show Triton kernel generation\n",
    "import torch\n",
    "import torch.fx\n",
    "from torch._inductor import compile_fx\n",
    "import sys\n",
    "from io import StringIO\n",
    "\n",
    "print(\"üöÄ FINAL APPROACH: Direct FX Compilation\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Clear everything\n",
    "torch._dynamo.reset()\n",
    "\n",
    "# Create a simple function that will generate Triton kernels\n",
    "def kernel_demo(x, y):\n",
    "    # Multiple operations that should each generate kernels\n",
    "    z1 = torch.relu(x)              # Pointwise\n",
    "    z2 = z1 + y                     # Pointwise fusion\n",
    "    z3 = z2 * 2.0                   # More fusion\n",
    "    z4 = torch.sum(z3, dim=0)       # Reduction\n",
    "    return z4\n",
    "\n",
    "# Create test inputs\n",
    "x = torch.randn(256, 256, device=device)\n",
    "y = torch.randn(256, 256, device=device)\n",
    "\n",
    "print(f\"üìä Inputs: x={x.shape}, y={y.shape} on {device}\")\n",
    "\n",
    "# Enable verbose mode\n",
    "import torch._inductor.config as config\n",
    "config.debug = True\n",
    "config.verbose_progress = True\n",
    "\n",
    "# Capture the FX graph\n",
    "print(\"\\nüîç Step 1: Capturing FX Graph...\")\n",
    "traced = torch.fx.symbolic_trace(kernel_demo)\n",
    "print(f\"‚úÖ Graph captured with {len(list(traced.graph.nodes))} nodes\")\n",
    "\n",
    "# Show the graph\n",
    "print(\"\\nüìä FX Graph Structure:\")\n",
    "print(traced.graph)\n",
    "\n",
    "print(\"\\nüîß Step 2: Compiling with Inductor (Watch for Triton output)...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Redirect stdout to capture compilation output\n",
    "old_stdout = sys.stdout\n",
    "output_capture = StringIO()\n",
    "\n",
    "try:\n",
    "    sys.stdout = output_capture\n",
    "    \n",
    "    # Compile using inductor directly\n",
    "    compiled_fn = compile_fx(traced, [x, y])\n",
    "    \n",
    "    # Restore stdout\n",
    "    sys.stdout = old_stdout\n",
    "    \n",
    "    # Get the captured output\n",
    "    compilation_output = output_capture.getvalue()\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"üîö Compilation Complete\")\n",
    "    \n",
    "    # Show compilation output\n",
    "    if compilation_output:\n",
    "        print(f\"\\nüìù COMPILATION OUTPUT ({len(compilation_output)} characters):\")\n",
    "        print(\"-\" * 40)\n",
    "        lines = compilation_output.split('\\n')\n",
    "        for i, line in enumerate(lines[:100]):  # First 100 lines\n",
    "            if line.strip():  # Skip empty lines\n",
    "                print(f\"{i+1:3d}: {line}\")\n",
    "        \n",
    "        if len(lines) > 100:\n",
    "            print(f\"... ({len(lines) - 100} more lines)\")\n",
    "        \n",
    "        # Look for Triton-specific content\n",
    "        triton_indicators = ['triton', 'kernel', '@jit', 'tl.', 'BLOCK_SIZE']\n",
    "        found_indicators = []\n",
    "        for indicator in triton_indicators:\n",
    "            if indicator in compilation_output.lower():\n",
    "                found_indicators.append(indicator)\n",
    "        \n",
    "        if found_indicators:\n",
    "            print(f\"\\nüéØ TRITON INDICATORS FOUND: {', '.join(found_indicators)}\")\n",
    "        else:\n",
    "            print(\"\\n‚ÑπÔ∏è  No obvious Triton indicators in compilation output\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No compilation output captured\")\n",
    "    \n",
    "    # Test the compiled function\n",
    "    print(f\"\\n‚ö° Step 3: Testing Compiled Function...\")\n",
    "    result = compiled_fn(x, y)\n",
    "    print(f\"‚úÖ Result shape: {result.shape}\")\n",
    "    print(f\"   Sample values: {result[:5]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    sys.stdout = old_stdout\n",
    "    print(f\"‚ùå Compilation failed: {e}\")\n",
    "    \n",
    "    # Show partial output\n",
    "    partial_output = output_capture.getvalue()\n",
    "    if partial_output:\n",
    "        print(f\"\\nüìù PARTIAL OUTPUT:\")\n",
    "        print(partial_output[:1000])\n",
    "\n",
    "print(f\"\\nüéì Summary:\")\n",
    "print(f\"   ‚Ä¢ FX graph successfully traced and compiled\")\n",
    "print(f\"   ‚Ä¢ Check the output above for Triton kernel generation details\")\n",
    "print(f\"   ‚Ä¢ Generated kernels are cached in /tmp/torchinductor_alibina\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8012eb19",
   "metadata": {},
   "source": [
    "Now, let's to Set environment variables to see Triton compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c967d1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Clear any cached compilations first\n",
    "torch._dynamo.reset()\n",
    "\n",
    "# Set environment variables to show detailed compilation info\n",
    "os.environ['TORCH_COMPILE_DEBUG'] = '1'\n",
    "os.environ['TORCH_LOGS'] = '+dynamo,+inductor'\n",
    "os.environ['TORCHINDUCTOR_VERBOSE'] = '1'\n",
    "\n",
    "# Enable all relevant logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "print(\"üîß Environment variables set for maximum verbosity:\")\n",
    "print(f\"   TORCH_COMPILE_DEBUG = {os.environ.get('TORCH_COMPILE_DEBUG')}\")\n",
    "print(f\"   TORCH_LOGS = {os.environ.get('TORCH_LOGS')}\")\n",
    "print(f\"   TORCHINDUCTOR_VERBOSE = {os.environ.get('TORCHINDUCTOR_VERBOSE')}\")\n",
    "print(\"\\nNow run the compilation demonstration above to see Triton kernel generation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf0615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal example to trigger Triton kernel generation with maximum visibility\n",
    "import torch\n",
    "torch._dynamo.reset()  # Clear cache\n",
    "\n",
    "# Enable the most specific debugging available\n",
    "import torch._inductor.config as config\n",
    "config.debug = True\n",
    "config.trace.enabled = True\n",
    "\n",
    "# Additional environment variables for Triton visibility\n",
    "import os\n",
    "os.environ['TRITON_PRINT_AUTOTUNING'] = '1'\n",
    "os.environ['TRITON_DEBUG'] = '1'\n",
    "\n",
    "print(\"üéØ FOCUSED TRITON DEMONSTRATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Simple operation that will definitely trigger Triton compilation\n",
    "def simple_operation(x):\n",
    "    return torch.relu(x) + 1.0\n",
    "\n",
    "# Create input\n",
    "x = torch.randn(1024, 1024, device=device)\n",
    "\n",
    "print(\"üìù About to compile a simple ReLU + addition...\")\n",
    "print(\"   Look for compilation messages in the output below:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Compile with maximum verbosity\n",
    "compiled_fn = torch.compile(simple_operation, mode=\"default\")\n",
    "\n",
    "# Trigger compilation\n",
    "print(\"üöÄ First execution (triggers kernel generation):\")\n",
    "result = compiled_fn(x)\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(\"‚úÖ Compilation completed!\")\n",
    "print(f\"   Result shape: {result.shape}\")\n",
    "print(f\"   Result mean: {result.mean():.4f}\")\n",
    "\n",
    "# Show some kernel information if available\n",
    "import torch._inductor.codecache as codecache\n",
    "cache_dir = codecache.cache_dir()\n",
    "print(f\"üìÅ Kernel cache directory: {cache_dir}\")\n",
    "\n",
    "# Try to list generated files\n",
    "import glob\n",
    "triton_files = glob.glob(f\"{cache_dir}/*triton*\")\n",
    "if triton_files:\n",
    "    print(f\"üîß Found {len(triton_files)} Triton-related cache files\")\n",
    "    for f in triton_files[:3]:  # Show first 3\n",
    "        print(f\"   - {os.path.basename(f)}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No Triton cache files found yet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bfd61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the generated Triton kernels\n",
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"üîç EXPLORING GENERATED TRITON KERNELS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Find the debug trace directory\n",
    "debug_dirs = glob.glob(\"./torch_compile_debug/*/torchinductor/\")\n",
    "if debug_dirs:\n",
    "    latest_debug_dir = max(debug_dirs, key=os.path.getctime)\n",
    "    print(f\"üìÇ Latest debug directory: {latest_debug_dir}\")\n",
    "    \n",
    "    # Look for generated kernel files\n",
    "    kernel_files = []\n",
    "    for ext in ['*.py', '*.cpp', '*.h']:\n",
    "        kernel_files.extend(glob.glob(os.path.join(latest_debug_dir, \"**\", ext), recursive=True))\n",
    "    \n",
    "    print(f\"\\nüîß Found {len(kernel_files)} generated files:\")\n",
    "    for f in kernel_files:\n",
    "        rel_path = os.path.relpath(f, latest_debug_dir)\n",
    "        print(f\"   üìÑ {rel_path}\")\n",
    "    \n",
    "    # Try to show some Triton kernel source\n",
    "    triton_files = [f for f in kernel_files if 'triton' in f.lower() or f.endswith('.py')]\n",
    "    if triton_files:\n",
    "        print(f\"\\nüìù Triton kernel source (first 30 lines of {os.path.basename(triton_files[0])}):\")\n",
    "        print(\"-\" * 50)\n",
    "        try:\n",
    "            with open(triton_files[0], 'r') as file:\n",
    "                lines = file.readlines()\n",
    "                for i, line in enumerate(lines[:30]):\n",
    "                    print(f\"{i+1:2d}: {line.rstrip()}\")\n",
    "                if len(lines) > 30:\n",
    "                    print(f\"... ({len(lines) - 30} more lines)\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Could not read file: {e}\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  No Triton kernel source files found yet\")\n",
    "else:\n",
    "    print(\"‚ùå No debug directories found\")\n",
    "\n",
    "# Also check the kernel cache\n",
    "cache_dir = \"/tmp/torchinductor_alibina\"\n",
    "print(f\"\\nüìÅ Checking kernel cache: {cache_dir}\")\n",
    "if os.path.exists(cache_dir):\n",
    "    cache_files = glob.glob(f\"{cache_dir}/**/*\", recursive=True)\n",
    "    py_files = [f for f in cache_files if f.endswith('.py') and os.path.isfile(f)]\n",
    "    \n",
    "    print(f\"üêç Found {len(py_files)} Python files in cache:\")\n",
    "    for f in py_files[:5]:  # Show first 5\n",
    "        rel_path = os.path.relpath(f, cache_dir)\n",
    "        print(f\"   üìÑ {rel_path}\")\n",
    "    \n",
    "    # Show content of a kernel file if available\n",
    "    if py_files:\n",
    "        print(f\"\\nüìù Sample kernel content (first 20 lines):\")\n",
    "        print(\"-\" * 40)\n",
    "        try:\n",
    "            with open(py_files[0], 'r') as file:\n",
    "                lines = file.readlines()\n",
    "                for i, line in enumerate(lines[:20]):\n",
    "                    print(f\"{i+1:2d}: {line.rstrip()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Could not read file: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå Cache directory not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354904ec",
   "metadata": {},
   "source": [
    "## Environment Variables: Your Debugging Toolkit\n",
    "\n",
    "Environment variables are your window into PyTorch's compilation process. Let's explore the most important ones and what they reveal.\n",
    "\n",
    "### üîç Essential Environment Variables\n",
    "\n",
    "| Variable | Purpose | What You'll See | When to Use |\n",
    "|----------|---------|-----------------|-------------|\n",
    "| `TORCH_LOGS=output_code` | Shows generated kernel code | Actual Triton source code | Understanding optimizations |\n",
    "| `TRITON_PRINT_AUTOTUNING=1` | Displays autotuning process | Different block sizes tested | Performance debugging |\n",
    "| `TRITON_PRINT_CACHE_STATS=1` | Shows cache statistics | Cache hits vs misses | Cache optimization |\n",
    "| `TORCH_LOGS=dynamo` | Shows graph capture | Python ‚Üí graph conversion | Debugging capture issues |\n",
    "| `TORCH_LOGS=inductor` | Shows backend compilation | Optimization passes | Backend debugging |\n",
    "\n",
    "### üéØ Debug Levels\n",
    "\n",
    "You can combine multiple log types:\n",
    "```python\n",
    "# Comprehensive debugging (verbose!)\n",
    "os.environ[\"TORCH_LOGS\"] = \"output_code,dynamo,inductor\"\n",
    "\n",
    "# Focus on specific areas\n",
    "os.environ[\"TORCH_LOGS\"] = \"output_code\"  # Just kernel code\n",
    "```\n",
    "\n",
    "### üí° Production vs Development\n",
    "\n",
    "**Development Environment:**\n",
    "- Enable detailed logging for learning and debugging\n",
    "- Use cache statistics to understand reuse patterns\n",
    "- Monitor autotuning to see optimization decisions\n",
    "\n",
    "**Production Environment:**\n",
    "- Minimal logging for performance\n",
    "- Cache kernels to avoid recompilation\n",
    "- Pre-warm models during initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01828451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring Environment Variables in Action\n",
    "def explore_environment_variables():\n",
    "    \"\"\"\n",
    "    Demonstrate how different environment variables provide insights\n",
    "    into the compilation process.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üîç EXPLORING ENVIRONMENT VARIABLES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create a model that will trigger interesting optimizations\n",
    "    def fusion_example(x):\n",
    "        # Multiple operations that can be fused\n",
    "        y = torch.relu(x)\n",
    "        z = y * 2.0\n",
    "        w = z + 1.0\n",
    "        return torch.tanh(w)\n",
    "    \n",
    "    test_data = torch.randn(1000, device=device)\n",
    "    \n",
    "    print(\"üìä Test case: Multi-operation fusion example\")\n",
    "    print(\"   Operations: ReLU ‚Üí Multiply ‚Üí Add ‚Üí Tanh\")\n",
    "    print(\"   Expected: These should fuse into a single kernel\")\n",
    "    \n",
    "    # Demonstrate different logging levels\n",
    "    scenarios = [\n",
    "        (\"minimal\", {}),\n",
    "        (\"output_code\", {\"TORCH_LOGS\": \"output_code\"}),\n",
    "        (\"with_autotuning\", {\n",
    "            \"TORCH_LOGS\": \"output_code\",\n",
    "            \"TRITON_PRINT_AUTOTUNING\": \"1\"\n",
    "        }),\n",
    "        (\"comprehensive\", {\n",
    "            \"TORCH_LOGS\": \"output_code,dynamo,inductor\",\n",
    "            \"TRITON_PRINT_AUTOTUNING\": \"1\",\n",
    "            \"TRITON_PRINT_CACHE_STATS\": \"1\"\n",
    "        })\n",
    "    ]\n",
    "    \n",
    "    for scenario_name, env_vars in scenarios:\n",
    "        print(f\"\\nüéØ Scenario: {scenario_name.upper()}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Temporarily set environment variables\n",
    "        original_env = {}\n",
    "        for key, value in env_vars.items():\n",
    "            original_env[key] = os.environ.get(key)\n",
    "            os.environ[key] = value\n",
    "            print(f\"   {key} = {value}\")\n",
    "        \n",
    "        if not env_vars:\n",
    "            print(\"   No special logging enabled\")\n",
    "        \n",
    "        print(f\"\\n   Compiling and running...\")\n",
    "        \n",
    "        # Clear compilation cache to force recompilation\n",
    "        torch._dynamo.reset()\n",
    "        \n",
    "        # Compile and run\n",
    "        compiled_fn = torch.compile(fusion_example)\n",
    "        \n",
    "        # Time the execution\n",
    "        start = time.perf_counter()\n",
    "        result = compiled_fn(test_data)\n",
    "        execution_time = time.perf_counter() - start\n",
    "        \n",
    "        print(f\"   ‚úÖ Execution time: {execution_time*1000:.3f} ms\")\n",
    "        \n",
    "        # Restore original environment\n",
    "        for key in env_vars:\n",
    "            if original_env[key] is not None:\n",
    "                os.environ[key] = original_env[key]\n",
    "            else:\n",
    "                os.environ.pop(key, None)\n",
    "        \n",
    "        print(f\"   üîÑ Environment restored\")\n",
    "    \n",
    "    print(f\"\\nüéì Observations:\")\n",
    "    print(f\"   ‚Ä¢ 'minimal': Clean output, no compilation details\")\n",
    "    print(f\"   ‚Ä¢ 'output_code': Shows generated Triton kernel source\")\n",
    "    print(f\"   ‚Ä¢ 'with_autotuning': Shows performance optimization process\")\n",
    "    print(f\"   ‚Ä¢ 'comprehensive': Full insight into entire pipeline\")\n",
    "    \n",
    "    # Restore our educational settings\n",
    "    for key, value in settings.items():\n",
    "        os.environ[key] = value\n",
    "\n",
    "# Run the exploration\n",
    "explore_environment_variables()\n",
    "\n",
    "print(f\"\\nüí° Pro Tips:\")\n",
    "print(f\"   ‚Ä¢ Start with TORCH_LOGS=output_code for learning\")\n",
    "print(f\"   ‚Ä¢ Add autotuning logs when optimizing performance\")\n",
    "print(f\"   ‚Ä¢ Use comprehensive logging only when debugging issues\")\n",
    "print(f\"   ‚Ä¢ Turn off logging in production for best performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e229c96",
   "metadata": {},
   "source": [
    "## Performance Patterns and Optimization Strategies\n",
    "\n",
    "Understanding PyTorch compilation performance patterns is crucial for effective optimization. Let's explore the key patterns and how to leverage them.\n",
    "\n",
    "### üìä Performance Pattern Analysis\n",
    "\n",
    "#### The Break-Even Point\n",
    "```\n",
    "Total Time = Compilation Time + (Execution Time √ó Number of Runs)\n",
    "\n",
    "Uncompiled Total = Baseline Time √ó Number of Runs\n",
    "Compiled Total = Compilation Time + (Optimized Time √ó Number of Runs)\n",
    "\n",
    "Break-even when: Compilation Time = (Baseline - Optimized) √ó Number of Runs\n",
    "```\n",
    "\n",
    "#### Factors Affecting Performance\n",
    "\n",
    "1. **Model Complexity**: More operations ‚Üí more fusion opportunities\n",
    "2. **Input Size**: Larger tensors ‚Üí better amortization of overhead\n",
    "3. **Hardware**: Better GPUs ‚Üí more optimization opportunities\n",
    "4. **Pattern Recognition**: Common patterns ‚Üí better optimizations\n",
    "\n",
    "### üéØ Optimization Strategies\n",
    "\n",
    "#### Strategy 1: Warm-up in Development\n",
    "```python\n",
    "# During model initialization\n",
    "model = MyModel()\n",
    "compiled_model = torch.compile(model)\n",
    "\n",
    "# Warm-up with dummy data\n",
    "dummy_input = torch.randn(typical_batch_size, ...)\n",
    "_ = compiled_model(dummy_input)  # Triggers compilation\n",
    "\n",
    "# Now ready for production use\n",
    "```\n",
    "\n",
    "#### Strategy 2: Selective Compilation\n",
    "```python\n",
    "# Compile only the critical paths\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.critical_path = torch.compile(self.forward_critical)\n",
    "        self.non_critical = self.forward_simple\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            return self.critical_path(x)  # Optimized training\n",
    "        else:\n",
    "            return self.non_critical(x)   # Fast inference\n",
    "```\n",
    "\n",
    "#### Strategy 3: Cache Management\n",
    "```python\n",
    "# Save compiled model state\n",
    "torch.save({\n",
    "    'model_state': model.state_dict(),\n",
    "    'compiled_state': compiled_model.state_dict()\n",
    "}, 'model_with_cache.pt')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7422fe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Pattern Analysis and Break-Even Calculation\n",
    "def analyze_performance_patterns():\n",
    "    \"\"\"\n",
    "    Analyze when compilation pays off and develop optimization strategies\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üìä PERFORMANCE PATTERN ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test different scenarios\n",
    "    scenarios = [\n",
    "        (\"Small Model\", 32, 64, 256),      # Small: batch=32, seq=64, hidden=256\n",
    "        (\"Medium Model\", 16, 128, 512),    # Medium: batch=16, seq=128, hidden=512  \n",
    "        (\"Large Model\", 8, 256, 1024),     # Large: batch=8, seq=256, hidden=1024\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for scenario_name, batch_size, seq_len, hidden_size in scenarios:\n",
    "        print(f\"\\nüß™ Scenario: {scenario_name}\")\n",
    "        print(f\"   Configuration: B={batch_size}, S={seq_len}, H={hidden_size}\")\n",
    "        \n",
    "        # Create model and data\n",
    "        class TestModel(nn.Module):\n",
    "            def __init__(self, hidden_size):\n",
    "                super().__init__()\n",
    "                self.norm1 = nn.LayerNorm(hidden_size)\n",
    "                self.norm2 = nn.LayerNorm(hidden_size)\n",
    "                \n",
    "            def forward(self, x):\n",
    "                x = F.gelu(self.norm1(x))\n",
    "                x = F.relu(self.norm2(x))\n",
    "                return x\n",
    "        \n",
    "        model = TestModel(hidden_size).to(device)\n",
    "        test_input = torch.randn(batch_size, seq_len, hidden_size, device=device)\n",
    "        \n",
    "        # Measure baseline performance\n",
    "        print(f\"   üìè Measuring baseline...\")\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(5):\n",
    "            with torch.no_grad():\n",
    "                _ = model(test_input)\n",
    "        \n",
    "        # Measure\n",
    "        baseline_times = []\n",
    "        for _ in range(20):\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            start = time.perf_counter()\n",
    "            with torch.no_grad():\n",
    "                _ = model(test_input)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            baseline_times.append(time.perf_counter() - start)\n",
    "        \n",
    "        baseline_avg = sum(baseline_times) / len(baseline_times)\n",
    "        \n",
    "        # Measure compilation overhead\n",
    "        print(f\"   ‚öôÔ∏è  Measuring compilation...\")\n",
    "        \n",
    "        torch._dynamo.reset()  # Clear cache\n",
    "        compiled_model = torch.compile(model, mode=\"default\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            _ = compiled_model(test_input)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        compilation_time = time.perf_counter() - start\n",
    "        \n",
    "        # Measure optimized performance\n",
    "        print(f\"   ‚ö° Measuring optimized performance...\")\n",
    "        \n",
    "        optimized_times = []\n",
    "        for _ in range(20):\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            start = time.perf_counter()\n",
    "            with torch.no_grad():\n",
    "                _ = compiled_model(test_input)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            optimized_times.append(time.perf_counter() - start)\n",
    "        \n",
    "        optimized_avg = sum(optimized_times) / len(optimized_times)\n",
    "        \n",
    "        # Calculate break-even point\n",
    "        if baseline_avg > optimized_avg:\n",
    "            break_even = compilation_time / (baseline_avg - optimized_avg)\n",
    "        else:\n",
    "            break_even = float('inf')  # Never breaks even\n",
    "        \n",
    "        # Store results\n",
    "        scenario_results = {\n",
    "            'name': scenario_name,\n",
    "            'baseline_ms': baseline_avg * 1000,\n",
    "            'optimized_ms': optimized_avg * 1000,\n",
    "            'compilation_ms': compilation_time * 1000,\n",
    "            'speedup': baseline_avg / optimized_avg if optimized_avg > 0 else 0,\n",
    "            'break_even_runs': break_even\n",
    "        }\n",
    "        \n",
    "        results.append(scenario_results)\n",
    "        \n",
    "        # Print results for this scenario\n",
    "        print(f\"   üìä Results:\")\n",
    "        print(f\"      Baseline: {scenario_results['baseline_ms']:.3f} ms\")\n",
    "        print(f\"      Optimized: {scenario_results['optimized_ms']:.3f} ms\")\n",
    "        print(f\"      Compilation: {scenario_results['compilation_ms']:.1f} ms\")\n",
    "        print(f\"      Speedup: {scenario_results['speedup']:.2f}x\")\n",
    "        if break_even != float('inf'):\n",
    "            print(f\"      Break-even: {break_even:.1f} runs\")\n",
    "        else:\n",
    "            print(f\"      Break-even: Never (compilation slower)\")\n",
    "    \n",
    "    # Summary analysis\n",
    "    print(f\"\\nüìà SUMMARY ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(f\"{'Scenario':<15} {'Speedup':<8} {'Break-even':<12} {'Recommendation':<20}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for result in results:\n",
    "        speedup_str = f\"{result['speedup']:.2f}x\"\n",
    "        \n",
    "        if result['break_even_runs'] == float('inf'):\n",
    "            breakeven_str = \"Never\"\n",
    "            recommendation = \"Skip compilation\"\n",
    "        elif result['break_even_runs'] < 5:\n",
    "            breakeven_str = f\"{result['break_even_runs']:.1f} runs\"\n",
    "            recommendation = \"Always compile\"\n",
    "        elif result['break_even_runs'] < 20:\n",
    "            breakeven_str = f\"{result['break_even_runs']:.1f} runs\"\n",
    "            recommendation = \"Compile for training\"\n",
    "        else:\n",
    "            breakeven_str = f\"{result['break_even_runs']:.1f} runs\"\n",
    "            recommendation = \"Selective compilation\"\n",
    "        \n",
    "        print(f\"{result['name']:<15} {speedup_str:<8} {breakeven_str:<12} {recommendation:<20}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the analysis\n",
    "performance_results = analyze_performance_patterns()\n",
    "\n",
    "print(f\"\\nüéì Key Insights:\")\n",
    "print(f\"   ‚Ä¢ Larger models generally benefit more from compilation\")\n",
    "print(f\"   ‚Ä¢ Break-even point varies significantly by model size\")\n",
    "print(f\"   ‚Ä¢ Consider your use case: training vs inference vs experimentation\")\n",
    "print(f\"   ‚Ä¢ Measure your specific workloads - patterns vary!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f638e16c",
   "metadata": {},
   "source": [
    "## Debugging Common Compilation Issues\n",
    "\n",
    "Even with PyTorch's sophisticated compilation system, issues can arise. Let's explore common problems and their solutions.\n",
    "\n",
    "### üêõ Common Issues and Solutions\n",
    "\n",
    "#### 1. **Compilation Failures**\n",
    "```python\n",
    "# Common error: Dynamic shapes\n",
    "RuntimeError: Cannot compile with dynamic shapes\n",
    "\n",
    "# Solution: Use torch.compile with dynamic=True or fix shapes\n",
    "compiled_fn = torch.compile(fn, dynamic=True)\n",
    "```\n",
    "\n",
    "#### 2. **Performance Regressions**\n",
    "```python\n",
    "# Issue: Compiled version slower than baseline\n",
    "# Causes: Small models, wrong compilation mode, graph breaks\n",
    "\n",
    "# Solutions:\n",
    "# 1. Try different modes\n",
    "compiled_fn = torch.compile(fn, mode=\"reduce-overhead\")  # vs \"default\"\n",
    "\n",
    "# 2. Check for graph breaks\n",
    "with torch._dynamo.optimize(\"inductor\"):\n",
    "    result = fn(input)  # Will show graph break warnings\n",
    "```\n",
    "\n",
    "#### 3. **Memory Issues**\n",
    "```python\n",
    "# Issue: Out of memory during compilation\n",
    "# Solution: Reduce compilation scope or use checkpointing\n",
    "@torch.compile(mode=\"reduce-overhead\")\n",
    "def smaller_function(x):\n",
    "    # Break large functions into smaller ones\n",
    "    return partial_computation(x)\n",
    "```\n",
    "\n",
    "#### 4. **Unsupported Operations**\n",
    "```python\n",
    "# Issue: Some operations don't support compilation\n",
    "# Solution: Selective compilation or fallbacks\n",
    "\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.compiled_part = torch.compile(self.core_computation)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Compiled part\n",
    "        x = self.compiled_part(x)\n",
    "        \n",
    "        # Unsupported operations run normally\n",
    "        x = unsupported_operation(x)\n",
    "        \n",
    "        return x\n",
    "```\n",
    "\n",
    "### üîß Debugging Toolkit\n",
    "\n",
    "1. **Environment Variables**: Use detailed logging\n",
    "2. **Graph Breaks**: Monitor for optimization barriers\n",
    "3. **Profiling**: Use torch.profiler for detailed analysis\n",
    "4. **Selective Compilation**: Isolate problematic areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdc7555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging Compilation Issues\n",
    "def demonstrate_debugging_techniques():\n",
    "    \"\"\"\n",
    "    Show common compilation issues and how to debug them\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üêõ DEBUGGING COMPILATION ISSUES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Issue 1: Graph breaks\n",
    "    print(\"üîç Issue 1: Graph Breaks\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    def problematic_function(x):\n",
    "        # This will cause a graph break\n",
    "        y = x * 2\n",
    "        \n",
    "        # Python control flow can cause graph breaks\n",
    "        if x.sum() > 0:  # Dynamic condition\n",
    "            z = y + 1\n",
    "        else:\n",
    "            z = y - 1\n",
    "            \n",
    "        return z\n",
    "    \n",
    "    test_input = torch.randn(100, device=device)\n",
    "    \n",
    "    # Enable graph break warnings\n",
    "    import torch._dynamo as dynamo\n",
    "    \n",
    "    print(\"   Compiling function with potential graph breaks...\")\n",
    "    \n",
    "    # This will show warnings about graph breaks\n",
    "    try:\n",
    "        compiled_problematic = torch.compile(problematic_function)\n",
    "        result = compiled_problematic(test_input)\n",
    "        print(\"   ‚úÖ Compilation succeeded despite graph breaks\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Compilation failed: {e}\")\n",
    "    \n",
    "    # Issue 2: Dynamic shapes\n",
    "    print(f\"\\nüîç Issue 2: Dynamic Shapes\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    def shape_sensitive_function(x):\n",
    "        # Function that's sensitive to input shapes\n",
    "        return x.view(-1, x.shape[-1] // 2, 2).sum(dim=-1)\n",
    "    \n",
    "    # This might cause issues with dynamic shapes\n",
    "    inputs_different_shapes = [\n",
    "        torch.randn(10, 20, device=device),\n",
    "        torch.randn(15, 30, device=device),  # Different shape\n",
    "        torch.randn(20, 40, device=device),  # Another different shape\n",
    "    ]\n",
    "    \n",
    "    print(\"   Testing with different input shapes...\")\n",
    "    \n",
    "    try:\n",
    "        compiled_shape_sensitive = torch.compile(shape_sensitive_function)\n",
    "        \n",
    "        for i, inp in enumerate(inputs_different_shapes):\n",
    "            result = compiled_shape_sensitive(inp)\n",
    "            print(f\"   ‚úÖ Shape {inp.shape}: Success\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Dynamic shapes issue: {e}\")\n",
    "        print(\"   üí° Solution: Use dynamic=True in torch.compile\")\n",
    "        \n",
    "        # Try with dynamic compilation\n",
    "        try:\n",
    "            compiled_dynamic = torch.compile(shape_sensitive_function, dynamic=True)\n",
    "            for i, inp in enumerate(inputs_different_shapes):\n",
    "                result = compiled_dynamic(inp)\n",
    "                print(f\"   ‚úÖ Dynamic shape {inp.shape}: Success\")\n",
    "        except Exception as e2:\n",
    "            print(f\"   ‚ùå Still failing: {e2}\")\n",
    "    \n",
    "    # Issue 3: Performance regression detection\n",
    "    print(f\"\\nüîç Issue 3: Performance Regression Detection\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    def potentially_slow_function(x):\n",
    "        # Simple function that might not benefit from compilation\n",
    "        return x + 1\n",
    "    \n",
    "    simple_input = torch.randn(100, device=device)\n",
    "    \n",
    "    # Measure baseline\n",
    "    times_baseline = []\n",
    "    for _ in range(50):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        _ = potentially_slow_function(simple_input)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        times_baseline.append(time.perf_counter() - start)\n",
    "    \n",
    "    baseline_avg = sum(times_baseline) / len(times_baseline)\n",
    "    \n",
    "    # Measure compiled version\n",
    "    torch._dynamo.reset()\n",
    "    compiled_simple = torch.compile(potentially_slow_function)\n",
    "    \n",
    "    # First run (compilation)\n",
    "    start = time.perf_counter()\n",
    "    _ = compiled_simple(simple_input)\n",
    "    compilation_time = time.perf_counter() - start\n",
    "    \n",
    "    # Subsequent runs\n",
    "    times_compiled = []\n",
    "    for _ in range(50):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        _ = compiled_simple(simple_input)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        times_compiled.append(time.perf_counter() - start)\n",
    "    \n",
    "    compiled_avg = sum(times_compiled) / len(times_compiled)\n",
    "    \n",
    "    print(f\"   Baseline: {baseline_avg*1000:.3f} ms\")\n",
    "    print(f\"   Compiled: {compiled_avg*1000:.3f} ms\")\n",
    "    print(f\"   Compilation overhead: {compilation_time*1000:.3f} ms\")\n",
    "    \n",
    "    if compiled_avg > baseline_avg:\n",
    "        print(\"   ‚ö†Ô∏è  Performance regression detected!\")\n",
    "        print(\"   üí° Recommendations:\")\n",
    "        print(\"      ‚Ä¢ Try different compilation modes\")\n",
    "        print(\"      ‚Ä¢ Consider skipping compilation for simple operations\")\n",
    "        print(\"      ‚Ä¢ Check for graph breaks\")\n",
    "    else:\n",
    "        speedup = baseline_avg / compiled_avg\n",
    "        print(f\"   ‚úÖ Speedup achieved: {speedup:.2f}x\")\n",
    "\n",
    "# Run debugging demonstration\n",
    "demonstrate_debugging_techniques()\n",
    "\n",
    "print(f\"\\nüéì Debugging Best Practices:\")\n",
    "print(f\"   ‚Ä¢ Always measure performance before and after compilation\")\n",
    "print(f\"   ‚Ä¢ Use environment variables to understand what's happening\")\n",
    "print(f\"   ‚Ä¢ Start with simple cases and add complexity gradually\")\n",
    "print(f\"   ‚Ä¢ Monitor for graph breaks and dynamic shape issues\")\n",
    "print(f\"   ‚Ä¢ Consider selective compilation for problematic functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3ca451",
   "metadata": {},
   "source": [
    "## Production Deployment Best Practices\n",
    "\n",
    "Deploying compiled PyTorch models in production requires careful consideration of performance, reliability, and maintainability.\n",
    "\n",
    "### üöÄ Production Deployment Strategy\n",
    "\n",
    "#### Phase 1: Development and Testing\n",
    "1. **Profile Your Workloads**: Measure baseline performance\n",
    "2. **Identify Compilation Candidates**: Focus on hot paths\n",
    "3. **Test Thoroughly**: Verify correctness and performance\n",
    "4. **Benchmark Different Modes**: Find optimal compilation settings\n",
    "\n",
    "#### Phase 2: Staging and Validation\n",
    "1. **Warm-up Strategy**: Pre-compile during initialization\n",
    "2. **Error Handling**: Graceful fallbacks for compilation failures\n",
    "3. **Monitoring**: Track performance and compilation success rates\n",
    "4. **A/B Testing**: Compare compiled vs uncompiled in production\n",
    "\n",
    "#### Phase 3: Production Rollout\n",
    "1. **Gradual Rollout**: Start with small traffic percentage\n",
    "2. **Performance Monitoring**: Track latency and throughput\n",
    "3. **Fallback Mechanisms**: Quick rollback if issues arise\n",
    "4. **Cache Management**: Optimize kernel reuse\n",
    "\n",
    "### üí° Production Patterns\n",
    "\n",
    "#### Pattern 1: Initialization-Time Compilation\n",
    "```python\n",
    "class ProductionModel:\n",
    "    def __init__(self):\n",
    "        self.model = MyModel()\n",
    "        \n",
    "        # Compile during initialization\n",
    "        self.compiled_model = torch.compile(self.model)\n",
    "        \n",
    "        # Warm-up with typical inputs\n",
    "        self._warmup()\n",
    "    \n",
    "    def _warmup(self):\n",
    "        dummy_input = torch.randn(typical_batch_size, ...)\n",
    "        _ = self.compiled_model(dummy_input)\n",
    "```\n",
    "\n",
    "#### Pattern 2: Conditional Compilation\n",
    "```python\n",
    "class AdaptiveModel:\n",
    "    def __init__(self, enable_compilation=True):\n",
    "        self.model = MyModel()\n",
    "        \n",
    "        if enable_compilation:\n",
    "            try:\n",
    "                self.forward = torch.compile(self.model.forward)\n",
    "                self.compiled = True\n",
    "            except Exception:\n",
    "                self.forward = self.model.forward\n",
    "                self.compiled = False\n",
    "        else:\n",
    "            self.forward = self.model.forward\n",
    "            self.compiled = False\n",
    "```\n",
    "\n",
    "#### Pattern 3: Performance Monitoring\n",
    "```python\n",
    "class MonitoredModel:\n",
    "    def __init__(self):\n",
    "        self.model = torch.compile(MyModel())\n",
    "        self.performance_metrics = {\n",
    "            'total_calls': 0,\n",
    "            'total_time': 0,\n",
    "            'compilation_failures': 0\n",
    "        }\n",
    "    \n",
    "    def forward(self, x):\n",
    "        start_time = time.perf_counter()\n",
    "        try:\n",
    "            result = self.model(x)\n",
    "            self.performance_metrics['total_calls'] += 1\n",
    "            self.performance_metrics['total_time'] += time.perf_counter() - start_time\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            self.performance_metrics['compilation_failures'] += 1\n",
    "            # Fallback to uncompiled\n",
    "            return self.model._orig_mod(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aec30af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production Deployment Template\n",
    "class ProductionModelTemplate:\n",
    "    \"\"\"\n",
    "    Template for production deployment of compiled PyTorch models\n",
    "    \n",
    "    This class demonstrates best practices for:\n",
    "    - Safe compilation with fallbacks\n",
    "    - Performance monitoring\n",
    "    - Warm-up strategies\n",
    "    - Error handling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_class, model_args=None, compilation_config=None):\n",
    "        \"\"\"\n",
    "        Initialize production model with compilation\n",
    "        \n",
    "        Args:\n",
    "            model_class: The PyTorch model class to instantiate\n",
    "            model_args: Arguments for model initialization\n",
    "            compilation_config: Configuration for torch.compile\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"üöÄ Initializing Production Model\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Default configurations\n",
    "        model_args = model_args or {}\n",
    "        compilation_config = compilation_config or {\n",
    "            'mode': 'default',\n",
    "            'dynamic': True,  # Handle dynamic shapes\n",
    "            'fullgraph': False  # Allow graph breaks\n",
    "        }\n",
    "        \n",
    "        # Initialize base model\n",
    "        self.model = model_class(**model_args)\n",
    "        self.compilation_config = compilation_config\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.metrics = {\n",
    "            'total_calls': 0,\n",
    "            'total_time': 0.0,\n",
    "            'compilation_failures': 0,\n",
    "            'warmup_time': 0.0,\n",
    "            'compiled': False\n",
    "        }\n",
    "        \n",
    "        # Attempt compilation\n",
    "        self._attempt_compilation()\n",
    "        \n",
    "        # Warm-up if compilation succeeded\n",
    "        if self.metrics['compiled']:\n",
    "            self._warmup()\n",
    "    \n",
    "    def _attempt_compilation(self):\n",
    "        \"\"\"Safely attempt model compilation with fallback\"\"\"\n",
    "        \n",
    "        print(\"üîß Attempting model compilation...\")\n",
    "        \n",
    "        try:\n",
    "            # Create compiled version\n",
    "            self.compiled_model = torch.compile(\n",
    "                self.model,\n",
    "                **self.compilation_config\n",
    "            )\n",
    "            \n",
    "            # Test compilation with dummy input\n",
    "            dummy_input = self._create_dummy_input()\n",
    "            \n",
    "            start_time = time.perf_counter()\n",
    "            _ = self.compiled_model(dummy_input)\n",
    "            compilation_time = time.perf_counter() - start_time\n",
    "            \n",
    "            self.metrics['compiled'] = True\n",
    "            self.metrics['compilation_time'] = compilation_time\n",
    "            \n",
    "            print(f\"‚úÖ Compilation successful\")\n",
    "            print(f\"   Compilation time: {compilation_time*1000:.1f} ms\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Compilation failed: {e}\")\n",
    "            print(\"   Falling back to uncompiled model\")\n",
    "            \n",
    "            self.compiled_model = self.model\n",
    "            self.metrics['compiled'] = False\n",
    "            self.metrics['compilation_failures'] += 1\n",
    "    \n",
    "    def _create_dummy_input(self):\n",
    "        \"\"\"Create dummy input for testing and warm-up\"\"\"\n",
    "        # This should be overridden based on your model's expected input\n",
    "        return torch.randn(1, 128, device=device)\n",
    "    \n",
    "    def _warmup(self, num_warmup_runs=5):\n",
    "        \"\"\"Warm up the compiled model\"\"\"\n",
    "        \n",
    "        print(f\"üî• Warming up compiled model ({num_warmup_runs} runs)...\")\n",
    "        \n",
    "        dummy_input = self._create_dummy_input()\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        for i in range(num_warmup_runs):\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    _ = self.compiled_model(dummy_input)\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è  Warmup run {i+1} failed: {e}\")\n",
    "        \n",
    "        warmup_time = time.perf_counter() - start_time\n",
    "        self.metrics['warmup_time'] = warmup_time\n",
    "        \n",
    "        print(f\"‚úÖ Warmup complete\")\n",
    "        print(f\"   Total warmup time: {warmup_time*1000:.1f} ms\")\n",
    "        print(f\"   Average per run: {warmup_time/num_warmup_runs*1000:.1f} ms\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Production forward pass with monitoring\"\"\"\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        try:\n",
    "            if self.metrics['compiled']:\n",
    "                result = self.compiled_model(x)\n",
    "            else:\n",
    "                result = self.model(x)\n",
    "            \n",
    "            # Update metrics\n",
    "            execution_time = time.perf_counter() - start_time\n",
    "            self.metrics['total_calls'] += 1\n",
    "            self.metrics['total_time'] += execution_time\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Forward pass failed: {e}\")\n",
    "            \n",
    "            # Fallback to uncompiled model\n",
    "            if self.metrics['compiled']:\n",
    "                print(\"   Falling back to uncompiled model\")\n",
    "                self.metrics['compilation_failures'] += 1\n",
    "                result = self.model(x)\n",
    "            else:\n",
    "                raise  # Re-raise if uncompiled model also fails\n",
    "            \n",
    "            execution_time = time.perf_counter() - start_time\n",
    "            self.metrics['total_calls'] += 1\n",
    "            self.metrics['total_time'] += execution_time\n",
    "            \n",
    "            return result\n",
    "    \n",
    "    def get_performance_report(self):\n",
    "        \"\"\"Generate performance report\"\"\"\n",
    "        \n",
    "        if self.metrics['total_calls'] == 0:\n",
    "            return \"No calls made yet\"\n",
    "        \n",
    "        avg_time = self.metrics['total_time'] / self.metrics['total_calls']\n",
    "        \n",
    "        report = f\"\"\"\n",
    "üìä Performance Report\n",
    "{'='*30}\n",
    "Model Status: {'Compiled' if self.metrics['compiled'] else 'Uncompiled'}\n",
    "Total Calls: {self.metrics['total_calls']:,}\n",
    "Total Time: {self.metrics['total_time']*1000:.1f} ms\n",
    "Average Time: {avg_time*1000:.3f} ms per call\n",
    "Compilation Failures: {self.metrics['compilation_failures']}\n",
    "Success Rate: {(1 - self.metrics['compilation_failures']/max(1, self.metrics['total_calls']))*100:.1f}%\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.metrics.get('compilation_time'):\n",
    "            report += f\"Initial Compilation: {self.metrics['compilation_time']*1000:.1f} ms\\n\"\n",
    "        \n",
    "        if self.metrics.get('warmup_time'):\n",
    "            report += f\"Warmup Time: {self.metrics['warmup_time']*1000:.1f} ms\\n\"\n",
    "        \n",
    "        return report.strip()\n",
    "\n",
    "# Demonstration of production deployment\n",
    "def demonstrate_production_deployment():\n",
    "    \"\"\"Demonstrate production deployment patterns\"\"\"\n",
    "    \n",
    "    print(\"üè≠ PRODUCTION DEPLOYMENT DEMONSTRATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Example model for demonstration\n",
    "    class ExampleModel(nn.Module):\n",
    "        def __init__(self, hidden_size=512):\n",
    "            super().__init__()\n",
    "            self.norm = nn.LayerNorm(hidden_size)\n",
    "            self.linear = nn.Linear(hidden_size, hidden_size)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            return F.gelu(self.linear(self.norm(x)))\n",
    "    \n",
    "    # Custom production model with proper dummy input\n",
    "    class ProductionExampleModel(ProductionModelTemplate):\n",
    "        def _create_dummy_input(self):\n",
    "            return torch.randn(16, 64, 512, device=device)\n",
    "    \n",
    "    # Deploy model\n",
    "    production_model = ProductionExampleModel(\n",
    "        model_class=ExampleModel,\n",
    "        model_args={'hidden_size': 512},\n",
    "        compilation_config={\n",
    "            'mode': 'default',\n",
    "            'dynamic': True\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Simulate production usage\n",
    "    print(f\"\\nüìà Simulating Production Usage\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    test_inputs = [\n",
    "        torch.randn(16, 64, 512, device=device),\n",
    "        torch.randn(32, 128, 512, device=device),  # Different shape\n",
    "        torch.randn(8, 32, 512, device=device),    # Another shape\n",
    "    ]\n",
    "    \n",
    "    for i, test_input in enumerate(test_inputs):\n",
    "        print(f\"   Processing batch {i+1} (shape: {test_input.shape})...\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            result = production_model.forward(test_input)\n",
    "        \n",
    "        print(f\"   ‚úÖ Success - Output shape: {result.shape}\")\n",
    "    \n",
    "    # Generate performance report\n",
    "    print(f\"\\n{production_model.get_performance_report()}\")\n",
    "    \n",
    "    return production_model\n",
    "\n",
    "# Run production deployment demonstration\n",
    "prod_model = demonstrate_production_deployment()\n",
    "\n",
    "print(f\"\\nüéì Production Best Practices Summary:\")\n",
    "print(f\"   ‚úÖ Always include fallback mechanisms\")\n",
    "print(f\"   ‚úÖ Monitor performance and failure rates\")\n",
    "print(f\"   ‚úÖ Warm up models during initialization\")\n",
    "print(f\"   ‚úÖ Handle dynamic shapes appropriately\")\n",
    "print(f\"   ‚úÖ Test thoroughly before production deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1635c8b1",
   "metadata": {},
   "source": [
    "## üéì Summary and Next Steps\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "In this notebook, we've explored the fundamental aspects of PyTorch + Triton compilation:\n",
    "\n",
    "#### üß† **Core Understanding**\n",
    "- **Compilation Pipeline**: How PyTorch transforms Python code into optimized GPU kernels\n",
    "- **Two-Phase Performance**: Why first runs are slow but subsequent runs are fast\n",
    "- **Environment Variables**: Powerful tools for debugging and understanding optimizations\n",
    "- **Performance Patterns**: When compilation helps and when it doesn't\n",
    "\n",
    "#### üîß **Practical Skills**\n",
    "- **Environment Setup**: Configuring optimal development environments\n",
    "- **Performance Analysis**: Measuring and understanding compilation benefits\n",
    "- **Debugging Techniques**: Solving common compilation issues\n",
    "- **Production Deployment**: Best practices for real-world applications\n",
    "\n",
    "#### üìä **Key Insights**\n",
    "- Compilation overhead is significant but amortizes over multiple runs\n",
    "- Different model sizes and patterns have different break-even points\n",
    "- Environment variables provide deep insights into the compilation process\n",
    "- Production deployment requires careful error handling and monitoring\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "#### Immediate Actions\n",
    "1. **Experiment with Your Models**: Apply `torch.compile()` to your existing PyTorch models\n",
    "2. **Measure Performance**: Use the techniques from this notebook to analyze benefits\n",
    "3. **Set Up Environment**: Configure development environment with appropriate logging\n",
    "\n",
    "#### Advanced Learning\n",
    "1. **Kernel Optimization**: Dive deeper into specific kernel fusion patterns\n",
    "2. **Custom Triton Kernels**: Learn to write hand-optimized kernels\n",
    "3. **Production Deployment**: Implement robust compilation strategies in your applications\n",
    "\n",
    "#### Continue Your Journey\n",
    "- **Next Notebook**: \"Optimizing PyTorch Kernels with Triton\" - Focus on specific optimization patterns\n",
    "- **Documentation**: Explore PyTorch's compilation documentation\n",
    "- **Community**: Join discussions about PyTorch optimization techniques\n",
    "\n",
    "### üí° Key Takeaways\n",
    "\n",
    "1. **Compilation is an Investment**: Upfront cost, long-term benefits\n",
    "2. **Measurement is Critical**: Always profile before optimizing\n",
    "3. **Environment Variables are Powerful**: Use them to understand and debug\n",
    "4. **Production Needs Planning**: Robust deployment requires careful design\n",
    "5. **Start Simple**: Begin with basic patterns and gradually increase complexity\n",
    "\n",
    "**You now have a solid foundation in PyTorch + Triton fundamentals. Ready to dive deeper into kernel optimization!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58c43fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c46dfc6e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-qat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
